\documentclass{vldb}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{algorithmic}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}



\def\punto{$\hspace*{\fill}\Box$}
\newcommand{\nop}[1]{}
\newcommand{\tuple}[1]{{\langle#1\rangle}}
\def\lBrack{\lbrack\!\lbrack}
\def\rBrack{\rbrack\!\rbrack}
\newcommand{\Bracks}[1]{\lBrack#1\rBrack}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{todo}[theorem]{ToDo}
\newtheorem{algorithm}[theorem]{Algorithm}

\numberofauthors{3}
\title{Cumulus: Combining Periodic and Eventual Consistency (in the cloud)}
\author{
\alignauthor
Oliver Kennedy\\
  \affaddr{Cornell University}
  \affaddr{okennedy@cs.cornell.edu}
\alignauthor
Yanif Ahmad\\
  \affaddr{Cornell University}
  \affaddr{yanif@cs.cornell.edu}
\alignauthor
Christoph Koch\\
  \affaddr{Cornell University}
  \affaddr{koch@cs.cornell.edu}
}
\date{}
\toappear{}

\begin{document}
\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

Recent years have seen the beginning of a paradigm shift in data management research from incrementally improving decades-old database technology to questioning established architectures  and creating fundamentally different, more lightweight systems that are often domain-specific (e.g.,
%\cite{DBLP:conf/cidr/KerstenM05,DBLP:conf/vldb/StonebrakerMAHHH07}
\cite{DBLP:conf/vldb/StonebrakerMAHHH07,DBLP:journals/pvldb/KallmanKNPRZJMSZHA08}).

Part of the impetus for this change was given by potential users such as scientists and the builders of large-scale Web sites and services such as Google, Amazon, and Ebay, who have a need for data management systems but have found current databases not to scale to their needs. One can observe a trend to disregard database contributions in these communities \cite{dbcolumn, DBLP:conf/sigmod/PavloPRADMS09}, and to build lightweight systems based on robust technologies mostly pioneered by the operating systems and distributed systems communities, such as large scale file systems, key-value stores, and map-reduce \cite{DBLP:journals/cacm/DeanG08, DBLP:journals/tocs/ChangDGHWBCFG08}. Further impetus has resulted from the current need to develop data management technology for multicore and cloud computing.

There is a recent tendency among pundits outside the database community to contest the need for powerful queries, and to think of key-value stores -- with only the power to look up data by keys -- as (much more efficient) database query engines.

However, expressive query languages such as SQL do not cease to have important applications and a substantial user base. Alas, we do not know how to process SQL queries on updateable data using a system as lightweight as a key-value store.

This paper contributes a fundamental and versatile building block for enabling new, more lightweight and nimble data processing systems based on SQL aggregation queries.  Cumulus is a thin execution layer that lives above generic key-value stores and can efficiently process a range of incremental view queries.  The query results are stored in the key-value store itself, as ordinary data. 

Cumulus is built around a novel hybrid consistency execution framework.  Like most cloud data management systems, Cumulus achieve low latencies by retreating to the realm of eventual consistency.  However, unlike most eventual consistency systems, Cumulus' runtime periodically generates consistent snapshots of query results as a side effect of its normal operation.  The result is a system that can be employed side-by-side by applications that require low latency and applications that require strong consistency.

We believe that our contribution constitutes an important step towards achieving the apparent contradiction in terms of executing complex aggregation queries on updateable data using little more than a key-value store.

At the heart of our approach is M3, a trigger-based vector processing language.  M3 can either be written directly or produced using DBToaster\cite{1687592}, an aggressive recursive incremental view maintenence compiler.  In most traditional database query processors, the basic building blocks of queries are large-grained operators such as joins.  Conversely, a large class of SQL aggregation queries can be compiled down to very simple message passing programs that incrementally maintain materialized views of the queries.

These message passing programs keep a hierarchy of map data structures (which may be served out of a key-value store) up to date. They can share computation in the case that multiple aggregation queries (e.g., a data cube \cite{datacube}) need to be maintained.  Most importantly, though, these message passing programs can be massively parallelized to the degree that the updating of each single result aggregate value can be done in constant time on normal off-the-shelf computers\footnote{Note that since there are
usually many more aggregate values to maintain than there are processors,
this does not mean that each update is processed in constant time.}.

Our main technical contributions are as follows:
\begin{itemize}
\item We present M3, a massively parallizable language for message passing programs that can be used to incrementally maintain SQL aggregation queries.
\item We present Cumulus, a lightweight realtime exact online aggregation system that integrates seamlessly with generic key-value stores. 
\item We describe three different approaches to ensuring the consistency of the data: Centralized Target Aggregation, Centralized Source Aggregation, and Eventually and Periodically Consistent Source Aggregation.  We deomonstrate how the last of these techniques simultaneously produces low-latency eventually consistent results as well as periodic consistent snapshots.
\item We show evidence for the scalability of our approach by examining the performance of Cumulus on examples drawn from the TPC-H benchmark\cite{tpch2008}
\end{itemize}


\section{The M3 Update Calculus}
Cumulus is built around M3, a trigger-based language closely related to Relational Calculus.  The basic datatype in M3 is the map, a mapping from vectors of keys to numeric values, or entries.  An M3 program consists of a set of triggers of the form
\[
\mbox{{\tt on <R>($\vec x \vec y$) \{ $s_1$; $\dots$; $s_k$ \}}}
\]
where {\tt <R>} is an event parametrized by a set of \textit{event variables} $\vec x \vec y$.  For example, for an M3 program that incrementally computes the results of a SQL query, an event would be a user inserting a row into a relation.  Each event triggers the evaluation of a list of statements $s_i$ each of which updates zero or more entries in a single map.
\[
\mbox{$m[\vec{x}\vec{z}]$ {\tt +=} $t(\vec x \vec y \vec z)$}
\]
Statements are comprised of a reference to a map entry on the left-hand side, and a right-hand side consisting of a term: a monomial algebraic expression over constants, variables, and map entry references.  The simplified version of M3 used in this paper is presented in Figure \ref{fig:m3_defn}

Two types of variables appear in M3 statements: event variables as defined above, and \textit{loop variables}.  Loop variables draw their domains from the right-hand side maps that they parametrize.  For example, consider the trigger:
\[
\mbox{\texttt{ON R(A) \{ m1[B] += m2[A,B] \}}}
\]
When Event \texttt{R(A)} occurs, every value \texttt{m1[B]} will be incremented by the corresponding \texttt{m2[A,B]}\footnote{Because all operations in M3 are increments, default values are required for each map entry.  In the simplified version of M3 that we describe here, the default value is always 0.}.  In effect, this is a multidimensional vector operation where the \textit{slice} of map \texttt{m2} with first parameter equal to \texttt{A} is extracted and used to increment \texttt{m1}.  Because the right hand side must be a monomial, only values of \texttt{B} for which \texttt{m2[A,B]} is nonzero can affect \texttt{m1}.  These values form the \textit{domain} of loop variable \texttt{B}.  Note that the domains of two loop variables may be linked if they appear as parameters to the same map.

%The domains of two variables may be linked.  For example, consider the trigger:
%\[
%\mbox{\texttt{ON R() \{ m1[B,C] += B \cdot m2[B,C] \}}}
%\]
%The domain of \texttt{C} is dependent on the value of \texttt{B} and visa versa.

M3 provides two consistency guarantees for programs:
\begin{itemize}
\item Triggers execute atomically with respect to each other.
\item Statements within a trigger execute in order.
\end{itemize}
These guarantees are equivalent to serializable consistency, where each event constitutes a transaction.


\subsection{Restricted M3}

For the purposes of this paper, we restrict ourselves to a limited, fully incremental version of M3.  
\begin{itemize}
\item \textit{All loop variables appear on both sides of a statement.}  The natural implication of a loop variable appearing only on the right-hand side is as a sum aggregate over the variable.  In the restricted M3, this aggregate can be computed incrementally by maintaining an additional map with one less key.
\item \textit{All loop variables appear as a parameter to exactly one right-hand side map.}  A loop variable that appears in two right-hand side maps is effectively a join between the maps.  In the restricted M3, joins are computed incrementally using event variables.
\item \textit{Terms do not contain conditions.}  Unrestricted M3 supports conditioned non-incremental aggregates.
\end{itemize}

Distributing unrestricted M3 is beyond the scope of this paper.  As shown in \cite{dbtoaster-pods}, this restricted form of M3 remains powerful enough to express any SQL count or sum aggregate query without inequalities.  

\begin{example}
Consider the following query over a TPC-H-like schema.
\begin{verbatim}
SELECT   c.nation, SUM(o.discount * l.price)
FROM     CUSTOMERS c, ORDERS o, LINEITEMS l
WHERE    c.cid = o.cid AND o.oid = l.oid
GROUP BY c.nation
\end{verbatim}

This query can be implemented in streaming form as the following M3 program
\begin{verbatim}
ON Insert_LINEITEMS(OID, PRICE) DO {
 q[NATION] += m1[OID, NATION] * PRICE;
 m2[CID]   += m3[CID, OID] * PRICE;
 m4[OID]   += PRICE }
ON Insert_ORDERS(CID,OID,DISCOUNT) DO {
 q[NATION]      += DISCOUNT * m4[OID] * m5[CID,NATION];
 m1[OID,NATION] += DISCOUNT * m5[CID,NATION];
 m2[CID]        += DISCOUNT * m4[OID];
 m3[CID,OID]    += DISCOUNT }
ON Insert_CUSTOMERS(CID,NATION) DO {
 q[NATION]      += m2[CID];
 m1[OID,NATION] += m3[CID,OID];
 m5[CID,NATION] += 1}
\end{verbatim}
%\begin{verbatim}
%ON Insert_T(C,D) DO {
% q[]   += m1[C] * D;
% m2[B] += D * m3[B, C];
% m4[C] += D }
%ON Insert_S(B,C) DO {
% q[]     += m5[B] * m4[C];
% m1[C]   += m5[B];
% m2[B]   += m4[C];
% m3[B,C] += 1 }
%ON Insert_R(A,B) DO {
% q[]   += A * m2[B];
% m1[C] += A * m3[B,C];
% m5[B] += A }
%\end{verbatim}
with deletion triggers that are identical except for a multiplier of -1.  Here, the map \texttt{q} represents the query result, while other maps represent intermediate subqueries.  For example, \texttt{m2} represents 
\begin{verbatim}
SELECT   o.cid, SUM(o.discount * l.price) 
FROM     ORDERS o, LINEITEMS l
WHERE    o.oid = l.oid
GROUP BY o.cid
\end{verbatim}
To see how loop variables are not required for an incremental join, note the three lines that update the result map \texttt{q}.  Each line incrementally updates the full join result by joining only with the tuple being inserted.
\end{example}

\begin{figure}
\begin{center}
{\small
\begin{eqnarray*}
PROGRAM & := & TRIGGER; TRIGGER; \ldots\\
TRIGGER & := & \textbf{ON}\ \left <EVENT\right>\ ( evt\_var_1, evt\_var_2, \ldots )\\ & & \textbf{DO}\ \{ STMT; STMT; \ldots\}\\
STMT & := & map_{target}[VAL_{1}, \ldots]\ \textbf{+=}\ EXPR\\
EXPR & := & map_{k}[VAL_{a}, \ldots]\ |\ VAL\ |\\&& EXPR\ *\ EXPR\ |\\&& \textbf{IF}\ EXPR\ CMP\ EXPR\\&& \textbf{THEN}\ EXPR\ \textbf{ELSE 0}\\
VAL & := & evt\_var_{n}\ |\ loop\_var_{m}\ |\ constant\\
CMP & := & =\ |\ <\ |\ <=
\end{eqnarray*}}
\caption{A restricted subset of M3}
\label{fig:m3_defn}
\end{center}
\end{figure}



\section{Centralized Consistency}

As all statements in M3 are effectively vector operations over maps, the language is extremely amenable to distribution.  Cumulus distributes both data and processing over a cluster of \textit{nodes}, each using a local BerkleyDB(TODO: Cite??) as a shared-nothing key-value store that is used to store map entries.  Though Cumulus uses the secondary indexing and iteration capabilities of BDB, both features can be built into or atop any key-value store.

All data in Cumulus is stored as part of a map - either as the value of map entries, or as one of the entry's keys.  A map is horizontally partitioned into one or more \textit{map segments} along any or all of its keys, and each segment is stored on one or more nodes in the cluster.

Nodes interact with the outside world via one or more \textit{coordinators}.  Event sources (eg., ticker feeds, sensors, static files, relational databases, or even other M3 programs) send events to the coordinator(s).  The coordinator optionally appends the event to a redo log, tags it with a timestamp, and dispatches it to the cluster of nodes for processing.  The overall design of Cumulus is illustrated in Figure \ref{fig:architecture}.

\begin{figure}
\includegraphics[width=3in]{graphics/Architecture.pdf}
\caption{The Cumulus Architecture}
\label{fig:architecture}
\end{figure}

External applications access the map nodes directly using a the mapping of map segments to nodes, or \textit{layout} obtained from the coordinator.  With the layout, the application fetches desired map values directly from nodes storing the values.  Cumulus provides library methods for doing so, described in Figure \ref{fig:architecture}.  The API allows the client to indicate that a sum, min, or max aggregate is required, so that nodes can pre-aggregate matching values locally to reduce messaging overhead.

\begin{figure}
\begin{algorithmic}
\STATE \textbf{READ\_SLICE}(map, $key_1$, $key_2$, \ldots) 
\end{algorithmic}
\hspace*{0.2in}\parbox[b]{3in}{Read the current version of a slice of a map; Null keys are treated as wildcards. All matching nonzero entries are returned.}
\begin{algorithmic}
\STATE \textbf{READ\_SUM}(map, $key_1$, $key_2$, \ldots)
\STATE \textbf{READ\_MIN}(map, $key_1$, $key_2$, \ldots)
\STATE \textbf{READ\_MAX}(map, $key_1$, $key_2$, \ldots)
\end{algorithmic}
\hspace*{0.2in}\parbox[b]{3in}{As READ\_SLICE but preaggregate values at the node if possible.  Null keys are treated as wildcards. The sum, min, max, etc\ldots of nonzero matching entries is returned.}
\begin{algorithmic}
\STATE \textbf{READ\_STABLE}(map, agg\_type, $key_1$, $key_2$, \ldots)
\end{algorithmic}
\hspace*{0.2in}\parbox[b]{3in}{As READ\_SLICE, but guarantees a self-consistent result when using eventual consistency (See Section \ref{sec:eventualconsistency}).  Returns the most recent consistent snapshot of all matching nonzero entries with optional pre-aggregation.}
\caption{Cumulus' Query API}
\label{fig:queryapi}
\end{figure}

\subsection{Distributing M3}
Cumulus distributes both data processing and data storage across nodes in the cluster.  Two approaches to minimizing network communication overhead immediately present themselves.  
\begin{enumerate}
\item Co-locate the processing of an M3 statement with the node storing the left-hand side map - that is, the map being updated. We refer to this as \textit{target aggregation}.
\item Co-locate the right-hand side maps of an M3 statement on the same node that processes the statement.  We refer to this as \textit{source aggregation}.
\end{enumerate}
As we will show, these questions are also intricately linked to the promary challenge of distributing M3: ensuring serializably consistent execution. 

Throughout the following sections, we assume that each node is fully aware of the data layout.  The generation and dynamic management of layouts is beyond the scope of this paper.

\subsection{Target Aggregation}
In target aggregation, M3 statements are evaluated at the node that will store the results.  Data appearing on the right-hand side of the statement is sent to this node for delta computation.  

\begin{example}
Consider the following M3 statement with maps m1 and m2, each partitioned only on attribute B.
\begin{verbatim}
on R(A) : m1[A,B] += m2[B]
\end{verbatim}
When event R(A) occurs, each node storing a segment of m2 will send each value in its segment to the node storing the corresponding segment of m1.  If the partition boundaries of m2 and m1 are identical, each m2 node will send its entire segment to exactly one m1 node.  This process is illustrated in Figure \ref{fig:messagepassing}a.  Note that If the same node stores each pair of corresponding m1 and m2 partitions, no actual data transmission occurs.  
\end{example}

Even if m1 is also partitioned over A, each event R(A) updates only one map segment per $B$ value; A node storing a segment of m2 is still only required to send the segment to one m1 node.  However, there are situations where multicast is required.  

\begin{example}
Consider the following statement:
\begin{verbatim}
on R(A) : m1[A,B,C] += m2[B]*m3[C]
\end{verbatim}
Let m1 have 10 partitions in a grid pattern, with the domain of B split in half and C split into 5 parts.  Let m2 and m3 use the same partitioning scheme (with 2 and 5 partitions, respectively).  In this instance, when an event R(A) occurs, each m2 node sends its segment to each of the corresponding 5 m1 nodes, while each m3 node will send its segment to each of the corresponding 2 m1 nodes.  This process is illustrated in Figure \ref{fig:messagepassing}b. 

If a right-hand side map contains an event variable, only a subset, or slice of the map is sent.  For example:
\begin{verbatim}
on R(A) : m1[B] += m2[A,B]
\end{verbatim}
In this instance, only the slice \texttt{m2'[B] = m2[A,B]} is sent.
\end{example}

\begin{figure}
\begin{center}
\includegraphics[width=3in]{graphics/message_passing}
\caption{Messages sent during evaluation of a single M3 statement in target aggregation, assuming a uniform partitioning scheme.  (a) The statement \texttt{on~R(A)~:~m1[A,B]~+=~m2[B]} (b) The statement \texttt{on~R(A)~:~m1[A,B,C]~+=~m2[B]*m3[C]} }
\label{fig:messagepassing}
\end{center}
\end{figure}

\subsection{One Coordinator}
The simplest way to provide M3's serializable execution guarantees is to route all events through a centralized coordinator.  Though centralizing creates a scalability bottleneck, the central coordinator's task is straightforward event serialization and dissemination.  The task of dissemination is further simplified by building a distribution tree over the nodes themselves.

As a query is loaded, each node precomputes a table of directives from the M3 program and the data layout.  This table indicates for each class of event (where class is defined by the partitioning scheme), which of the node's map segments will be modified (ie, those that appear on the left-hand side of a triggered statement), and which segments will be read from (right-hand side).  The construction algorithm is shown in Figure \ref{fig:directiveconstruction}.

\begin{figure}
\begin{algorithmic}[1]
\FORALL{Event Variable $V$ in Statement $S$}
  \STATE $partitions[V]$ = 1;
  \FORALL{Map $M \in S$ with $V$ as a key}
    \STATE \COMMENT {Run once for each occurrence of V in S as a key}
    \STATE $C =$ \# of partitions of $M$ on the $V$ axis.
    \STATE $partitions[V] =$ compute\_lcm($partitions[V]$, $C$)
  \ENDFOR
\ENDFOR
\FOR{$P \in \mathbb N^{|partitions|}$ with $0 \leq P_V < partitions[V]$}
  \STATE $Directive[P] = \{\}$
  \FORALL{Map $M \in$ right\_hand\_side($S$)}
    \FORALL{Event Variable $V$ in $M$}
      \STATE $C =$ \# of partitions of $M$ on the $V$ axis.
      \STATE $slice[V] = C\cdot\frac{P}{partitions[V]}$
    \ENDFOR
    \STATE \COMMENT{Note that the slice may have multiple owners}
    \STATE $Dir[P] +=$ \{\textbf{read $M$ from} owner($M, slice$)\}
  \ENDFOR
  \STATE $M = target\_map(S)$
  \FORALL{Event Variable $V$ in $M$}
    \STATE $C =$ \# of partitions of $M$ on the $V$ axis.
    \STATE $slice[V] = C\cdot\frac{P}{partitions[V]}$
  \ENDFOR
  \STATE $Dir[P] +=$ \{\textbf{write $M$ to} owner($M, slice$)\}
\ENDFOR
\STATE \textbf{return} $Dir$
\end{algorithmic}
\caption{The directive table construction algorithm.}
\label{fig:directiveconstruction}
\end{figure}

The coordinator first assigns a version number to each event.  Then, when a node receives an event from the distribution tree, it consults the directive table for the appropriate read and write operations to perform.  

Write operations are deferred until all prior writes on the map have been completed, and all slices appearing on the right hand side of the update statement have been received.  

Similarly, read operations block until all write operations on the map pending at the time of the event are complete.  Upon completion of the last prior write, the source node gathers the required slices, and sends them to the corresponding destination nodes.  Note that the even empty slices must be sent; the destination node needs to hear from all source nodes in order to proceed with its write.

\subsection{Source Aggregation}
Instead of processing updates at the node where the result will be stored, precompilation allows Cumulus to ensure that all data necessary to process a statement is co-located. In this scheme, each map segment is stilly processed at a specific node.  Rather than storing the results locally, a replica of the map segment is created at each node that reads from the segment.  

Consistency is maintained in the same way as in Target Aggregation.  When an event occurs, each node identifies which local maps might be updated as a consequence.  As before, writes are deferred until all input maps have been updated.  Also, as before, writes are always sent across the wire, even if no updates actually occur.  

When discussing source aggregation, we assume without loss of generality that each node processes updates for exactly one map, and refer to maps and nodes interchangeably.  Where the distinction is relevant, maps are indicated with \texttt{fixed width font}.

\section{Decentralized Consistency}
Both target and source aggregation attempt to enforce serializable consistency.  This introduce delays into update processing, and also requires the use of ``empty'' messages.  Worse still, the centralized coordinator creates a single point of failure and a scalability bottleneck.  

The M3 language affords Cumulus a critical benefit: map data is changed only by offset.  Thus, the messaging overhead of an undo operation is equivalent to that of a write.  Cumulus exploits the low undo overhead in order to achieve a processing model that simultaneously generates both low-latency eventual consistency outputs and periodic consistent snapshots.  

\subsection{Eventual Consistency}
\label{sec:eventualconsistency}
In the eventual consistency approach, there are multiple coordinators, each with a numeric identifier.  Coordinators maintain a loosely synchronized clock between themselves, and use it to define epoch boundaries.  Synchronized epoch transitions are not required.  However, tighter inter-coordinator epoch transitions result in a more efficient system.  This is discussed further below.

By decentralizing, each coordinator is freed to do additional work to minimize network usage.  Each coordinator maintains a dispatch table, similar to the directive table used by the nodes under centralized consistency.  The table maps  partitions of each event's parameter space to the set of nodes required to process the partition.  When an event occurs, only these nodes are notified.

Each event is sent to a coordinator, where it is assigned a version identifier consisting of the three-tuple \texttt{\{VersionID, CoordinatorID, EpochID\}}.  Here, the version identifier is a unique identifier, monotonically increasing at the coordinator within a given epoch.  This three-tuple is enough to create a total ordering over all version identifiers by sorting by epoch, coordinator, and then version.  An ordering over map updates can be created by extending the three-tuple with the ordering of the M3 statements.  

Nodes follow the Source Aggregation approach, but instead of deferring writes, send them as soon as possible.  For improved efficiency, this includes a small buffer period.  Writes are recorded in a short-term log, along with the event and statement that produced them.  The value of the delta is computed immediately based on the source maps' current values, excluding deltas tagged with later versions.  The resultant delta is sent immediately.  

When a node receives a map delta, it identifies all earlier updates in its history.  It then determines which of these updates are affected by the incoming delta, computes the new delta, and sends the difference between the deltas.  Pseudocode for this process is presented in Figure \ref{fig:eventualconsistency}.

\begin{figure}
ON Event $R(\vec X)$ with version $V$
\begin{algorithmic}[1]
\FORALL{Statement $S_i$ Triggered By $R(\vec X)$}
	\STATE $target =$ target\_map($S_i$)
	\STATE $\delta =$ compute($S_i$, $\vec X$, $V$)
	\IF{$\delta \neq \emptyset$}
    \STATE send\_push($target$, $\vec X$, $\delta$, $V \circ i$)
  \ENDIF
	\STATE $history[V\circ i] = \{S_i, \vec X, \delta\}$ 
\ENDFOR
\end{algorithmic}
\ \\
ON Push($target$, $\vec X$, $\delta$, $V$)
\begin{algorithmic}[1]
\FORALL{Key $\vec E \in \delta$}
  \STATE sorted\_insert($target[\vec E]$, $V$, $\delta$)
  \COMMENT {Map entries are stored as a list of deltas, sorted by version}
\ENDFOR
\FORALL{$\{S, \vec X', \delta'\} \in history[V']$ with $V' > V$}
	\IF{$S$ read from $target$}
		\STATE $\delta_{new} =$ compute($S$, $\vec X'$, $V'$) - $\delta$
    \IF{$\delta_{new} \neq \emptyset$}
      \STATE send\_push(target\_map($S$), $\vec X'$, $\delta_{new}$, $V'$)
    \ENDIF
	\ENDIF
\ENDFOR
\end{algorithmic}
\ \\
ON compute($S$, $\vec X$, $V$)
\begin{algorithmic}[1]
\STATE $slice = \{[] \rightarrow 1\}$
\STATE \COMMENT {We assume there is at least one term.  Terms may be numerics (treated as $\{[]\rightarrow \#\}$) or map accesses.}
\FORALL{Term $T_i[\vec Y] \in $ terms($S$)}
  \STATE \COMMENT {$\times$ and $\bowtie$ are the cross product and natural join}
  \STATE $term\_slice[\vec Y] = \sum_{\vec Y \bowtie \vec X; V' \leq V} T_i[\vec Y][V']$
  \STATE $slice = slice \times term\_slice$
\ENDFOR
\STATE $\delta =$ project($slice$, keys(target\_map($S$)))
\STATE \textbf{return} filter(delete $[*]\rightarrow 0$, $\delta$)
\end{algorithmic}
\caption{The Eventual Consistency Update Algorithm (simplified).}
\label{fig:eventualconsistency}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=3.4in]{graphics/Datastructures}
\caption{Datastructures used at each node.  The update history stores updates since the last garbage collection together with a cache of precomputed map values at a particular version number.  One key-value store is used for each map.  If the key-value store does not support secondary indices, the same effect may be achieved by using an additional store for each secondary index required.  The key-value store maintains one delta for each version in which a map entry entry was modified.}
\label{fig:datastructures}
\end{center}
\end{figure}

The values stored in a map may not be correct at any given moment.  For example, consider the partial M3 program:
\begin{verbatim}
ON R(A,B): m1[] += m2[A]
           m2[A] += m3[B]
\end{verbatim}
This fragment of the program is deployed with no map segmentation on a 2 node system.  Node 1 stores m2 and processes updates to m1, while Node 2 stores m3 and processes updates to m2.  

In this scenario, two subsequent R events with identical A parameters are dispatched by the same coordinator nearly simultaneously.  Both nodes process the necessary updates to m1 and m2 respectively.  However, while node 1 is processing the second event, it does not have the value of m2 updated by the first event.  The result is a value of m1 that is incorrect.  However, when the updated value of m2 is received, Node 1 can overwrite the corresponding version of m1.  

This process maintains an eventually consistent version of all maps.  The value of the maps may not be consistent at any given moment, but the error is limited to values that have not fully propagated through the system.  Thus, if the system quiesces, maps will return to consistency.

\subsection{Data Dependencies}
Event $E_2$ is dependent on event $E_1$ if $E_2$ has a later timestamp, and reads reads from a map entry that $E_1$ writes to\footnote{Note that since M3 statements manipulate entire maps, dependencies at the entry granularity are data-dependent}.  Concretely, $E_2$ is dependent on $E_1$ if a non-zero entry appears in the intersection between a slice appearing on the right-hand side of a statement issued by $E_2$, and a left-hand slice from $E_1$.  

An unfulfilled dependency occurs at a node $N$ if an event $E_2$ depends on event $E_1$, but $E_1$'s deltas do not arrive at node $N$ until after $N$ has sent out the delta it produces for $E_2$.  Unfulfilled dependencies are handled by sending out a corrective delta, as described above.  However, the corrective delta may be the source of another unfulfilled dependency at the target node if the target has already processed an event with a timestamp after $E_2$'s.  We refer to such an event as a cascade.

We can examine the effects of a cascade by considering the dataflow hypergraph of an M3 program, such as the one in Figure \ref{fig:dataflow}.  In a dataflow graph, each node represents one map and each hyperedge represents a statement.  Hyperedges are labeled with the triggering event, and flow out of the source maps of a statement and into the target map.  Cascades are bounded by the maximum path length in the dataflow graph. 

\begin{figure}
\begin{center}
\includegraphics[width=3in]{graphics/q12_graph}
\end{center}
\caption{Dataflow graph for the M3 program representing a streaming TPC-H-style query $\ _{C.cid}G_{SUM(L.price)}(C\ldots \bowtie L\ldots \bowtie O\ldots)$}
\label{fig:dataflow}
\end{figure}

In \cite{dbtoaster-pods}, we show how to produce an M3 program for computing non-recursive SQL queries.  Such M3 programs always have acyclic dataflow graphs.  However, it is still possible to produce M3 programs with cycles.  

\begin{example}
\label{ex:cyclegraph}
Consider the program
\begin{verbatim}
ON R(A): m1[A] += 1
         m2[A] += m3[A]
ON S(A): m1[A] += m2[A]
ON T(A): m3[A] += m1[A]
\end{verbatim}
The dataflow graph for this program is presented in Figure \ref{fig:dataflowcycle}.  Though no single event uses or modifies the same maps twice\footnote{Even if one event were to read and write to/from the same map, there is still an explicit order of execution across statements}, it is possible for a poorly timed event arrival order to cause each successive event to generate a progressively larger number of messages.  In this example, consider the ordered event sequence \texttt{T[1],R[1],S[1]}, causing the following messages to be dispatched:
\begin{enumerate}
\item Event \texttt{T} arrives at m1.  m1 sends the current value of \texttt{m1[1]} as an delta for \texttt{m3[1]} to m3.
\item Event \texttt{R} arrives at m1 and m3.  m1 is updated.  m3 sends its current value \texttt{m3[1]} to m2.  
\item The delta for Event \texttt{T} arrives at m3, changing the value of \texttt{m3[1]}.  m3 must now update its delta to \texttt{m2[1]} and send a correction to m2.
\item Event \texttt{S} arrives at m2.  m2 sends its current value \texttt{m2[1]} to m1.  
\item The delta for Event \texttt{R} arrives at m2, changing the value of \texttt{m2[1]}.  m2 must send a correction to m3.
\item The correction to the delta for Event \texttt{R} arrives at m2, changing the value of \texttt{m2[1]} again.  m2 must send another correction.
\end{enumerate}
\end{example}

\begin{figure}
\begin{center}
\includegraphics[width=2in]{graphics/cycle_graph}
\end{center}
\caption{Dataflow graph for an M3 program with a dataflow cycle.}
\label{fig:dataflowcycle}
\end{figure}

We refer to such an event as a \textit{cascade}.  If there is no cycle in the graph, the number of messages in a cascade is bounded by 
$$\frac 12 [max\_path\_length(dataflow\_graph)]^2$$
Otherwise, this process can continue indefinitely, increasing the number of messages with each event.  

To prevent this, Cumulus buffers and aggregates deltas for a short period of time after the event that causes them.  While this does prevent a cascade, it bounds the number of messages sent by the cascade.

% Possible TODO: intelligent buffering; How we can stagger buffer periods for a dataflow DAG to limit unfulfilled dependencies down the line.

\subsection{Coordinator Favoritism}

Out of order event processing allows Cumulus to employ decentralized coordinators.  Coordinators are effectively assigned a priority order.  Nodes ensure consistency by consistently evaluating events in the order of their issuing coordinators, regardless of the order in which the events arrive.

This means that lower priority coordinators are much more likely to have unfulfilled dependencies with higher priority coordinators, and thus trigger a cascade.  Worse, the dataflow graphs produced by compiling SQL to M3 are symmetric; there is no assignment of events to coordinators that maps lower priority coordinators to maps further down the dataflow graph.

Cumulus removes this favoritism by staggering each coordinator's epoch transitions.  Each epoch is divided up into a number of periods equal to the number of coordinators.  Each period, the next higher priority coordinator transitions into the next epoch.  This is easily achieved by a token-passing system.  Only the coordinator with the token can switch epochs.  It waits for the duration of one period, switches, and passes the token to the next coordinator.

\subsection{Garbage Collection}
The eventual consistency algorithm uses two datastructures that at first glance appear to be both unbounded and continually growing: The update history, and map values being representated as a list of deltas.  In both cases, the datastructure is unbounded because a value is stored for each version identifier that affects it. 

However, the tail of both datastructures is only needed until updates with earlier version numbers have finished propagating.  The update history is no longer relevant after this point.  Similarly, once version $V$ has finished propagating, prior map entries with earlier versions are used only in the aggregate form.
$$\sum_{V' \leq V} E[V']$$

Thus, it is necessary for the system to be able to discover when all updates have completed propagating through the system.  Cumulus performs this discovery process periodically, once per epoch by default.  

%ABOVE: describe how TCP channels should be used in the section on source aggregation??

Garbage collection occurs in two phases: coordinator synchronization, and node synchronization.  Each coordinator maintains a record of all nodes it sends events to since the last garbage collection.  When garbage collecting, each coordinator sends an \texttt{epoch commit} message to all nodes on the list.  It then continues processing as normal.  The garbage collection process is also illustrated in Figure \ref{fig:GCflowchart}.

\begin{figure}
\begin{center}
\includegraphics[width=3in]{graphics/GCflowchart}
\caption{Timeline of Garbage Collection for one Epoch in Cumulus; The program being executed is \texttt{ON R() DO \{m1[] += 1; m2[] += m1[]\}; ON S() DO \{m2[] += 1; m3[] += m1[]\}} }
\label{fig:GCflowchart}
\end{center}

%ON R(): m1[] += 1
%        m2[] += m1[]
%ON S(): m2[] += 1
%        m3[] += m1[]
\end{figure}

\begin{figure}
ON Cleanup($V$)
\begin{algorithmic}[1]
\FORALL{Map $M$}
  \FORALL{Entry $E \in M$}
    \STATE $E'[V'] = E[V']$ where $V' > V$
    \STATE $E'[V] = \sum_{V' \leq V} E[V']$
    \STATE replace\_entry($E'$, $M$)
  \ENDFOR
\ENDFOR
\STATE $history =$ filter(delete $history[V']$ if $V' < V$, $history$)
\end{algorithmic}
\caption{Garbage Collection Algorithms}
\label{fig:gcalgorithms}
\end{figure}

When a node receives an \texttt{epoch commit}, it responds, confirming that it has received, processed, and inserted into its history all prior events from that coordinator.  Once a coordinator receives confirmation from all nodes it sent an \texttt{epoch commit} to, it broadcasts an \texttt{epoch end} to all other coordinators.  Once all coordinators have sent an \texttt{epoch end}, the coordinators split the work of broadcasting an \texttt{epoch end} across the cluster.

Once a node receives an \texttt{epoch end} message, it will never again receive an event for a prior epoch.  Now, the nodes must ensure that no further map deltas will need to be propagated.  Using the dataflow graph, a node can determine from where it receives deltas, and to whom it sends deltas.  

If the graph is a DAG, garbage collection is simple.  The node waits until it receives an epoch complete message from its coordinator, and an \texttt{epoch checkpoint} from all in edges.  It then sends an \texttt{epoch checkpoint} to each out edge, and invokes the cleanup process described in Figure \ref{fig:gcalgorithms}.

If the dataflow graph has cycles, as in Example \ref{ex:cyclegraph}, one node in the cycle is designated as the cycle leader.  When this node receives an \texttt{epoch end}, it sends out a \texttt{partial checkpoint} to the next node in the cycle.  The \texttt{partial checkpoint} is forwarded to subsequent nodes in the cycle.  Each node uses the checkpoint message to ensure that it has received an \texttt{epoch end} and \texttt{epoch checkpoint}s from all nodes not in the cycle and that it has processed all pending messages from the prior node before forwarding the \texttt{partial checkpoint}.  

If the cycle leader has not received any deltas from the prior node in the cycle by the time the \texttt{partial checkpoint} completes a full traversal, it sends a \texttt{partial checkpoint complete} to the nodes in the cycle, informing them that the cycle has completed its checkpoint. If it receives a delta in the interim, it begins the process anew.

An additional step may be introduced to deal with a node that belongs to two or more cycles.  One cycle leader is designated as the primary.  Before sending a \texttt{partial checkpoint complete}, the cycle leaders all block on the primary.  Once all cycle leaders are ready, the primary queries all nodes that cross cycles and the nodes respond whether or not they sent a delta after sending at least one \texttt{partial checkpoint} message.  If it did, the primary restarts the partial checkpointing process.  Otherwise, the primary proceeds as normal.

The act of garbage collection has a very useful added benefit.  In addition to limiting the amount of historical data nodes are required to store, aggregating all deltas prior to a particular version creates a version of the map segment that is consistent with other map segments with the same version across the entire cluster.

\section{Partitioning}
\label{sec:partitioning}

\section{Experiments}

Buffer size vs latency / max throughput.  Intelligent buffering (ie, buffering for greater periods as we go down the dataflow dag)


Eventual+Periodic Consistency.
Subquery computation

{\small
\bibliographystyle{abbrv}
\bibliography{main}
}


\end{document}  