%\documentclass{acm_proc_article-sp} 
\documentclass[11pt]{article} 
\usepackage{amssymb,amsmath} 
\usepackage{graphicx}
\usepackage{float}
\usepackage{hyperref}


\floatstyle{ruled}
\newfloat{code}{thp}{lop}
\floatname{code}{Code}

%\DeclareGraphicsExtensions{.eps}

\begin{document}
	
\title{DBToaster Runtime: System for Streaming Data Processing.}
\author{}
\maketitle

\section{Introduction}

DBToaster application involves three major components. Data Sources components are a source of the incoming data. The end user applications need these data to be processed by running various queries over it. In this particular example we will be using an Exchange Simulation Server application as a data source. Next component is the Algorithms Simulation Engine. This component is the end user program of interest. It is responsible for the decision making and outside influences (i.e. what ever user needs to be done with the help of the queries over the data streams). In out example it is a Trading Runtime Platform. The last component is DBToaster Runtime. The component is responsible for collecting the data from various data sources specified by the users, running appropriate queries over the data and conveying the query results to Algorithms Simulation Engine. 

The the flow of information between the three components is as follows. Once started data generation component generates data, these data comes from a variety of sources stock exchange, sensor network, remote applications etc. The data comes as a continuous stream of information, this information is presented in a well known format. In many of the cases it is possible that the end user does not have much control over the data stream but the ability to listen to the incoming data. These data is sent to DBToaster runtime. The runtime is responsible for collecting data from those data sources and running a dynamic set of user defined queries over these data. The queries are dynamically loaded, compiled and if needed removed during the execution of a runtime. The results of the queries are forwarded to Algorithms Simulation Engine (a.k.a the end user). The Engine's job is to get the data from the runtime. Based on the results of the queries Algorithm Engine performs tasks needed by the users. In our example, based on the information received Algorithm Engine buys and sell stocks on a NASDAQ exchange simulator. 

The particular application we have chosen to demonstrate the capabilities and performance of our system is a NASDAQ trading simulation. In this simulation there is a set of servers; each server arranges stock exchanges between different trading parties. The results of those exchanges as well as information about interest in stock sells and purchases is transmitted to all parties interested in such information. Based on these information different trading algorithms decide to buy and sell stocks. The arrangement of all the components in the Trading system can be seen in figure \ref{TheBigPicture}. 

\begin{figure}
  \includegraphics[width=4.50in]{../figures/finapp.pdf}
  \caption{Application Overview.}
  \label{TheBigPicture}
\end{figure}

%description of particularities of the system


\section{DBToaster Runtime}

DBToaster Runtime consists of three major components: Compiler, Query Processor and Data Sources Processor. Each of these components interacts with each other as well as has external interactions. The complete representation of DBToaster Runtime can be found in figure \ref{DBToasterPic}. Compiler and Query Processor have a server connection to handle user requests. Data Source Processor creates user specified clients to receive the data from various data sources. Compiler receives requests from the client to instantiate a Data Source Reader for receiving data or a SQL query to process data. The created Data Reader is passed to the Data Source Processor and a Query is passed to Query Processor to run on the data. 

\begin{figure}
  \includegraphics[width=4.50in]{../figures/DBToasterRuntime.pdf}
  \caption{DBToaster Structure.}
  \label{DBToasterPic}
\end{figure}



\subsection{Compiler}

Compiler consists of several components. Its structure can be seen in figure \ref{CompilerPicture}. Two components responsible for compilation are SQL query pre-compiler and C++ code compiler. 

Compiler is a starting point for all user interactions with the DBToaster Runtime. Once Runtime is up and running the user initiates the contact by specifying that they need to execute a query on some specific set of data sources. The query, types and parameters of data sources are sent to User Input Handler. The Input Handler collects the information in a structured format and passes it to the Compiler Admin. Compiler Admin converts a SQL query to C++ code and C++ code to binary format. The query is then passed to the Query Admin to be added to the list of active queries. If the user asks to add a Data Source Reader. The Reader of a specified type is created and passed to the Data Sources Admin to be added to the list of active Readers. The Data Adapter is compiled into a binary format and sent to Data Sources Admin to be added to Data Adaptors. 

\begin{figure}
  \includegraphics[width=4.50in]{../figures/compiler.pdf}
  \caption{Compiler Structure.}
  \label{CompilerPicture}
\end{figure}

\subsubsection{User Input Handler}
User Input Handler implements a server to respond to client requests. When a client connects to the Input Handler Server it sends a type of request and attributes of the request to the server:
\\
\\
\begin{tabular}{|l|l|}
  \hline
  Type & Request Attributes \\ \hline
  Query & Query Source file, list of Data Readers \\ \hline
  Reader & Connection Type, Adapter Source file \\ \hline
\end{tabular}
\\
\\*
The two different types of requests are either to add a query or add a data reader.
\\*
Add query request needs the following data from the user:

\begin{itemize}
	\item {\tt AddRequest(type="add query",\\
		 Source="SQL Source File",\\
	     DataReaders="list of Data Reader's IDs for Query");}
\end{itemize}

\noindent Add data reader request is as follows:

\begin{itemize}
	\item {\tt AddRequest(type="add reader", \\
	     AdaptorSource="C++ Adaptor Source file",\\
	     source="IPaddress and port or data input file");}
\end{itemize}

This information is then forwarded to the Compiler Admin in a form of a struct:

\begin{itemize}
	\item {\tt Struct InputRequest( int type, string fileName,\\
	       list<int> informationSources \\
		   list<string> sourceParameters)}
\end{itemize}

User Input Handler is implemented as a class with a server hidden inside. For now Input Handler also contains Compiler Admin. Compiler Admin can be abstracted to run in a separate thread. 

\subsubsection{Compiler Admin}

Once user request is received, Compiler Admin takes over. Depending on the type of the request Admin takes different actions. 

On add SQL query request Admin passes the SQL query source file to SQL Compiler. SQL compiler returns a pointer to an containing equivalent to a query C++ file. This file is then passed to the C++ compiler by the Admin. C++ compiler returns a function handle to the compiled binary of the query. The query handle is then passed to the Query Admin to be added to the catalog together with the list of Readers for the query.

On add Reader request Admin passes Data Adaptor file to C++ compiler which in turn returns a function handle to the Data Adaptor. Then Admin creates a Reader and opens a connection for a reader (either a socket or a file). The handle to a Reader together with the handle for a Data Adaptor and a list of queries for this reader is passed to the Data Sources Admin. 

\noindent Compiler Admin class looks as follows:
\begin{verbatim}
class CompilerAdmin
{
    void RunAdmin(InputRequest);
	
    SQLCompiler;
    CCompiler;
    QueryAdmin *;
    DataSourcesAdmin*;
}
\end{verbatim}

Method {\tt RunAdmin(InputRequest)} takes a user request to add a query or a data source. Compiler Admin contains a SQL Compiler and C++ compiler. It also contains pointers to Query Admin and Data Sources Admin to pass compiled handlers to them.

\subsubsection{SQL Compiler}

SQL complier takes a SQL query from a file and creates an optimized C++ coded to be added to the runtime. The file name containing the code is returned to the Admin.

\begin{itemize}
	\item {\tt cCodeFileName CompileSQL(SQL file name);}
\end{itemize}

\subsubsection{C++ Compiler}

C++ compiler takes file containing C++ code. C++ compiler creates a binary code version of the code and returns a function handle of it to Admin. \emph{not sure about the return form. it is possible we need to return several function handles ... not sure yet}

\begin{itemize}
	\item {\tt functionHandle CompileC++(C++ file name);}
\end{itemize}

\subsection{Data Source Processing}

Data Source Processing is done in series of steps. Once Data Sources Admin receives a function handles from Compiler Admin. The Reader handle is inserted into Collection of Data Readers. Each Reader is responsible for a data stream from outside. The handle for the Reader is for a function {\tt getNext()}.

The Data Readers Thread Pool is a collection of threads where each thread picks a Data Reader and executes a read at a time from one of them. Once read is done the results of the read are forwarded to a Data Structure Dispatch. The Dispatch uses the appropriate Data Adapter to handle the data into a structured format, then the structured data is forwarded to Query Processor, in particular to the Query Tuple Queue. The more pictorial description can be found in figure \ref{DataSourcePic}.

% processing consists of the list of list of stream points. Each point is given by the user and signifies a way for a DBToaster to connect with the input stream of some process which dispatches data to the DBToaster. Each Stream Point is written by the user and user can fully specify how the data should be received and processed. Stream point is responsible for sending data to the Data Dispatch unite in a pre-specified format. The structure for data source processing can be found in figure 

\begin{figure}
  \includegraphics[width=5.00in]{../figures/DataSources.pdf}
  \caption{Data Sources Management.}
  \label{DataSourcePic}
\end{figure}

\subsubsection{Data Sources Admin}

Data Sources Admin receives two function handles one to a data Reader and another one for a Data Adaptor from a Compiler. First Admin locks Data Adaptors and inserts a new Data Adaptor. Then Admin locks Data Readers Queue  and a handle is inserted. Each Data Readers handle implements a method \emph{readNext()} for reading data from some data source (typically a socket or a file).

\begin{itemize}
	\item {\tt rawDataTuple getNext();}
\end{itemize}

\noindent The {\tt rawDataTuple} is a pair {\tt <size, bit-string>}.

The Reader extracts the informations from either a file or a socket. Reader's communication with the outside world is in a standard format. On a read request, Reader first reads a header containing the information about the message followed by the message itself. For now we assume that a header is an integer containing the number of byte to be read. Once the read is completed the size and message are forwarded to Data Structures Dispatch.

\emph{TODO: expand this into a section with the full description of Readers input formats}.

\noindent Data Sources Admin class contains the following:
\begin{verbatim}
class DataSourcesAdmin
{
    addDataSource(readerID, sourceReader, dataAdaptor);

    dataReadersQueue*;
    dataAdaptorsMap*;
}
\end{verbatim}


\subsubsection{Data Readers Thread Pool}

Data Readers Thread Pool is a collection of identical threads. Each thread is responsible for getting a reader from the queue of Data Readers, executing a \emph{readNext()} message, collecting the message {\tt rawDataTuple} and sending it to Data Structures Dispatch together with the ID of a Reader.

The class of the Data Readers Thread Pool looks as follows:
\begin{verbatim}
class DataReadersPool
{
    list<thread>  runningThreads;
    dataReadersQueue;
    dataStructuresDispatch;
}
\end{verbatim}

\noindent The class of Data Readers Thread Pool contains a Data Readers Queue:

\begin{verbatim}
class DataReadersQueue
{
	functionHandler getNextReader();
	void            putNextReader(functionHandle);
	
    queue<functionHandles>
}
\end{verbatim}

\noindent and a Data Structures Dispatch.

\subsubsection{Data Structures Dispatch}

Data Structures Dispatch receives an ID identifying each Data Reader and a message read by the reader. 

\begin{itemize}
	\item {\tt structuredTuple DataStructuresDispatch(readerID, rawDataTuple);}
\end{itemize}

Based on the ID of a reader Dispatch Calls for on an appropriate Data Adaptor to convert the incoming {\tt rawDataTuple} message into a structure needed by the query. The structured tuple is then sent to an appropriate set of queues in the Data Queue for Processing. The {\tt structuredTuple} is a pair {\tt <size, bit-sequence>} such that the bit sequence can be interpreted by the query. 

\begin{verbatim}
class DataStructuresDispatch
{
	getNextMessage();
	
    dataAdaptorsMap;
    queryTupleQueue*;
}
\end{verbatim}

\noindent Data Adaptors map is a separate class just like Data Readers Queue

\begin{verbatim}
class DataSTructuresDispatch
{
    functionHandler getAdaptor(id);
    void            addAdaptor(id, functionHandle);
	
}
\end{verbatim}
\noindent They are needed to be separated into classes to deal with concurrency.

\subsection{Query Processor}

Query Processor is done mostly in Query Executor with a set of Query Threads each thread picks a query-datum pair from a pool of available pairs and processes it. As data comes in after been processed it is sent to the appropriate query. As soon as query receives a datum it becomes available for execution. The structure of query processor can be found in figure \ref{QueryProcessingPic}.

\begin{figure}
  \includegraphics[width=5.00in]{../figures/QueryProcessing.pdf}
  \caption{Query Processing Management.}
  \label{QueryProcessingPic}
\end{figure}

\subsubsection{Query Admin}

Query Admin receives a function handle to a query. It also receives a list of the readers IDs for that query. Once the handle is received Admin locks Query Catalog and inserts the query there, then Query Results Queue is locked and updated to have an output queues for just inserted query. Finally Admin locks Query Tuple Queue and inserts and additional queue for data associated with the query. 

Query Catalog is a map containing query ID and a handler for that query. 

Query Admin class looks as follows
\begin{verbatim}
class QueryAdmin
{
    addQuery(queryID, functionHandle, list<int> ReadersID);

	queryCatalog;
	networkCatalog;
	QueryTupleQueue*;
	
}
\end{verbatim}

\subsubsection{Query Executor}

Query Executor consists of two inter dependent components: Query Tuple Queue and Scheduler. Query Tuple Queue contains the data needed to execute each query. Scheduler contains a set of threads which run one query at a time on some data.  

\paragraph{Query Tuple Queue}

Query Tuple Queue is a class, which provides seamless access to the data. It contains a number of queues, such that a subset of queues corresponds to the query that can run on data items from those queues. When Query Tuple Queue receives a new item from Data Dispatch, together with an ID of the data Reader (in a format {\tt structuredTuple}), the new datum is inserted into appropriate queues. If the datum insert causes a query to be available then query handle is extracted from the catalog and inserted into Runable Query Queue. unable Query Queue contains handles to the queries which has data items available to them. If the query is already runable then the item is just inserted into queue.

Query Tuple Queue class has the following structure:

\begin{verbatim}
class QueryTupleQueue
{
    void              addDatum(sourceID, structuredTuple);
    structuredTuple   getDatum(queryID);
    void              addQuery(queryID, list<int> sourceIDs);
    void              removeQuery(queryID);
    void              hasNext(queryID);

	MemoryManager                      Jim;
	RunableQueryQueue*                 schedulingQueue;
	map<sourceID, list<TupleQuery*> >  sourceAccess;
	map<queryID, list<TupleQuery*> >   queryAccess;
	map<sourceID, list<queryID> >      availableQueries;
	
}
\end{verbatim}

\noindent Function {\tt addDatum(sourceID, structuredTuple)} is called by the Data Dispatch to add a new data item. Function {\tt getDatum(queryID)} is required to extract data items available for the query to be executed. Functions {\tt addQuery(queryID, list<int> readersID)} and {\tt removeQuery(queryID)} are needed by Query Admin to insert or remove a new query. 

To enable this functionality we need to consider concurrency and efficiency. 

\subparagraph{Concurrency:} Here we mean that no data tuple can be lost or received out of order by the query. This can be enabled by a simple procedure of locking each queue when data is inserted or removed from each queue.

\subparagraph{Efficiency:} The main inefficiency comes form the fact that data from a data source is needed by several queries. If we adopt a simple model (case A) where each query has its own set of queues from each of the required sources, we have inefficiency of coping data to each such queue. The benefit of such model is simplicity of implementation and use. Another model (case B) is to have only one queue for each data source. In this case each queue will need to keep track of which query requires what item from this queue as well as making sure that the items not required by any query are removed. To support both approaches we can hide all this details into the implementation of the queues. By requiring the following specification:

\begin{verbatim}
class TupleQueue
{
	TupleQueue*       join(queryID);
	void              removeQuery(queryID);
	
	void              addDatum(structuredTuple);
	structuredTuple   getDatum(queryID);
	bool              isEmpty(queryID);
	
	map<queryID, pointerToPositionInQueue>     nextDatum;
	queue                                      datum;
}
\end{verbatim}

When a new query is inserted by Query Admin into QueryTupleQueue with {\tt addQuery(queryID, list<int> sourceIDs)}. Internally we go through the list of sources and creating pointers to TupleQueue for {\tt map<queryID, list<TupleQuery*> >   queryAccess}. If the source is not present (we can find it though {\tt map<sourceID, list<TupleQuery*> >}) we create a new TupleQueue, but if it is we call {\tt join(queryID)}. {\tt join(queryID)} in case A will create a new TupleQueue and return a pointer to it or in case B it will return a pointer to this queue and change internal structure to accommodate a new query. ({\tt removeQuery(queryID)} is a reverse. It is either a destructor or changes internal representation). When a new datum comes in, by the call of {\tt addDatum(sourceID, structuredTuple)} in QueryTupleQueue. It looks up a list of queues for the source and calls {\tt addDatum(structuredTuple)} on each of them. (in case B the list will only have one TupleQueue).

\paragraph{Scheduler}

Query Processor Thread Pool is a set of identical threads. Each thread is responsible for picking up and query handle from Scheduling Queue. Then thread runs a query on the datum on top of the appropriate query queue in Query Tuple Queue. The results are put into a Query Results Queues; into a set of queues for the users which are interested in the results of this query. 

The Query Processor Thread Pool class contains the following:

\begin{verbatim}
class Scheduler
{
    void   Run();

    RunableQueryQueue   availableQueries;
    list<thread>        executionThreads;
    QueryResultQueue*   readyData;
	
}
\end{verbatim}

\noindent With SchedulingQueue as follows:

\begin{verbatim}
class RunableQueryQueue
{
    
    funcitonHandle nextQuery();
    void           addQuery(queryID);
    bool           isActive(queryID);

    queryCatalog*                 catalog;
    queue<functionHandle>         readyQueries;
	
}
\end{verbatim}


\subsubsection{Query Results Queue}

Query Results Queue implements a server, where each client upon connection indicates a set of queries they are interested in.

\begin{itemize}
	\item {\tt userDataStructuresDispatch(userID, Query ID list);}
\end{itemize}

\noindent The server creates a map of query IDs and query results for each client. When a query produces an output, it is sent to the Query Result Queues. In the Result Queues the query results are dispatched to the appropriate queues.

\begin{itemize}
	\item {\tt queryDataStructuresDispatch(queryID, queryMap);}
\end{itemize}

\noindent When a client needs a new piece of information it sends a request to the server with the ID of a query it is interested in and the number of tuples it needs. Server returns the results of the query indicated by the ID and the tuples it produced. 

Query Result Queues implements two ways for the users to access data. One is when a user asks for a complete query result and another is when a client asks for an curser to query results.

Query Result Queue class is:

\begin{verbatim}
class SchedulingQueue
{
    
	map<queryID, queryResult>
	
	getFullQueryResult(clientID, queryID);
	
	startCurserResult(clientID, queryID);
	
}
\end{verbatim}




\section{Data Generation}

Data can come in a variety of forms. For our particular application we have developed an Exchange Simulation Server. The server is responsible for processing and broadcast of all stock exchanges to the clients that are connected to it. Server can also is capable to augment data flow by adding data from files containing historical traces of real stock exchanges. At the moment the Exchange Server designed to simulate trade exchanges for one stock.   

\subsection{Exchange Server Simulator}
ExchangeServer creates server port and handles all incoming client connections. Each connected client is given it's own thread (ExchangeThread). ExchangeServer also creates a DataThread, which is responsible for reading historic trace data. The server also creates structure for datastorage which is shared by all of the clients.

\begin{code}
  \begin{verbatim}
Server_socket;
SynchronizedBooks DataBook;
Data_socket(inputDataFile.cvs);

while (Server_socket.listen)
{
	get(client);
	run(client, DataBook);
}
  \end{verbatim}
  \caption{ExchangeServer: inputDataFile.cvs}
\end{code}



\subsection{Exchange Thread}
On a client connection Server creates an ExchangesThread to deal with client needs. All ExchangesThreads share data storage structure DataBook as well as a list of all currently active clients. On a start up thread expects to receive a message from a client indicating the type of a client.
\\
\\
\begin{tabular}{|l|l|}
  \hline
  \multicolumn{2}{|c|}{Client type} \\
  \hline
  0 & Interactive\\ \hline
  1 & Passive Listener \\
  \hline
\end{tabular}
\\
\\*
\emph{Interactive} clients send data and are only interested in receiving messages in response to their transactions. \emph{Passive Listener} clients are interested in all of the transactions but tend not to send the messages themselves.

The messages from clients are coming in the pre-specified format. Depending on the client's request several actions occur:
\\
\\
\begin{tabular}{|l|l|}
  \hline
  Request & Action \\ \hline
  'B' & buy request, check ask book for match if no add to bid book \\ \hline
  'S' & see request, check bid book for match if no add to ask book \\ \hline
  'D' & delete request from appropriate book if one exists \\ \hline
  'X' & cancel order remove trade request from appropriate book\\
  \hline
\end{tabular}
\\
\\
Each transaction is announced to all Passive Listeners as well as to Interactive client which intiated the transaction. 

% the first  message showing an interest in exchange ExchangesThread checks if there is an appropriate match in the data structures (i.e. if some one wants to sell some amount of stocks the thread will look if someone wants to buy stocks at the given price), if so the match is executed if not the request is stored in the data structures. The results and the transaction are announced to all the connected clients. 

\subsection{Data Storage}
SynchronizedBooks is a shared data structure. It is shared between all of the clients. Each client can modify it by adding/removing buy/sell requests. The data structure consists of two SortedBooks. Each SortedBook is a structure that has properties of a sorted set and a hashtable. 

When an addition of a sell/buy request is invoked data structure tries to find a matching buy/sell request and if found the update message is sent to the clients additionally to the original sell/buy request. The update message contains information about the fact that one or both of the orders are partially/fully satisfied. The exchange message is of the form:
\\
\\
\begin{tabular}{|l|l|}
  \hline
  Request & Action \\ \hline
  'E' & order number, update amount \\ \hline
  'F' & order was executed in full \\

  \hline
\end{tabular}
\\

\subsection{Data Generation}

ExchangeServer is capable of augmenting the data flow between clients with the additional data from historical data files. To accomplish this goal server has a DataThread. It can be initiated at any point by a server (for example, once there is a client connected to the server). DataThread reads a historical trace file in a .cvs format and transmits each message to the server as if it was a real request.


\section{Algorithms Engine}

Algorithm Engine code developed by the end users. The code makes use of DBToaster Runtime to query the continuous data from input streams specified by the users. The query results are then processed by user defined algorithms to achieve their goals. 

In our example Algorithmic Engine creates and runs a set of monitoring queries for the stock trading depending on the results of the queries and the internal mechanics - algorithms decide to buy or sell stocks on a NASDAQ Trading Exchange Simulator.

\subsection{Data Sources}

Algorithmic Engine interacts with environment in three different ways. First of all the Engine needs to receive  the information from from DBToaster runtime. This interaction is unavoidable since the Engine needs the results of the queries. Another interaction which needs to be added is the way to specify the information stream received by a DBToaster runtime. This information needs to be passed the the Runtime in the form of c++ code for the Runtime to compile. Another optional source of interaction is infulencing the enviroment. In our example is the Engine's capability to add/remove stock requests from the Exchange Server. 

\subsubsection{Data Stream}

There are two interactions with outside systems needed to be implemented by the user. The first interaction is with DBToaster. A system needs to pull information from the DBToaster runtime. \emph{TODO description of the the pulling mechanism}.

Another interaction, which needs to be passed to the DBToaster Runtime, is a way for Runtime to access the data streams relevant to the Engine and over which the queries needed to be post. This information is specified as a C++ code which is sent for the compilation to the Runtime to be added to the system.

\subsubsection{External Influence}

If the Engine implements interactive process, which needs to access outside processes or interact (influence) them then this is interactivity needs also to be added. In our example Algorithmic Engine can buy and sell stock on our Exchange Simulation Server. Thus acting like one of the clients of the server. \emph{TODO describe the system in more details}

\subsection{Algorithm}

This is the part which is the most relevant to the user and will vary from system to system. In out example we encoded the learning and trading strategies for NASDAQ stock exchange. \emph{TODO describe the system in more details}

\subsection{Query creator (or something like that)}

Queries posed to the DBRuntime can come from various sources they can either be predefined by the user system or written and modified dynamically. In order to be active they need to be passed to the Runtime where they are compiled and added to stream observations. \emph{TODO describe the process more}



\end{document}





