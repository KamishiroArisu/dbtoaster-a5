Typical DDMS workloads require large state, necessitating the use of more intelligent storage techniques.  However, compared to traditional DBMS, DDMS have more information to use when constructing a task-specific storage solution.  

We now examine two components of DBToaster's solution to storage in a DDMS: (1) The DBToaster compiler produces datastructures designed specifically for the compiled DDMS' target query workload. (2) By analyzing the patterns with which data is accessed, DBToaster constructs a data layout strategy (for pages on a disk, servers in a cluster, etc...) that limits IO overhead.

\subsection{Datastructures}
Regardless of whether data is stored in memory, on disk, or across an entire cluster, efficient disk access begins with good representation.  As its data storage primitive, DBToaster uses multi-key maps: key-value datastores with iteration capabilities (eg., \cite{berkeleydb}).  

Iteration, both for reads and for writes, is a point of interest; generated statements do not typically iterate over the entire datastructure.  Rather, in the common case, statements iterate over values matching a selection predicate.  For example, when updating Section \ref{sec:dbtoaster}'s $m_c$ after an insertion into \texttt{lineitem}, we iterate over all values matching the predicate \texttt{@ok = orderkey}.  A logical approach to this problem would be to create a simple secondary index.  More complex predicates can be answered with more complex indices\cite{rangequeries,rstartree,srtree}.

Though this might initially appear as a stale problem - selection predicates are the bread and butter of databases - The DBToaster compiler has access to all of the DDMS's selection predicates and can \textit{identify all useful indexes at compile time}.  Even if an online optimizer is used to select indexes at runtime, the optimizer has to explore a much more limited search space than it would normally be working with.  

An interesting extension to this idea is that this key-value-iterator pattern is general enough to admit a wide range of underlying mechanisms; For example, $m_c$ could be represented by a two-level nested hash-map, indexed on \textit{orderkey} and \textit{custkey}.  However, the same interface can manage parametric models (eg., markov random fields[cite]), computed functions (eg., hashes), or compressed data (eg., encoded photos, sparse matrices, etc\ldots).

\subsection{Messaging, Storage and Partitioning}
Even with good datastructures, haphazard data placement leads to poor performance.  Though precise workload data may not be available at compile time, DBToaster optimizes the way it lays out its database across memory, a disk, or even a server cluster, based on the query workload it is constructed with.  An elegant abstraction for doing this is the \textit{messaging graph}.

Each transition function can be represented as a bipartite directed hypergraph; nodes on the left represent portions of the database being read from, nodes on the right represent a portions being written to, and each hyperedge represents an independent subtask of the transition function.  An example is shown in Figure \ref{fig:diag:messagingGraph}a.

\begin{figure}
\begin{center}
\includegraphics[width=2.5in]{graphics/MessagingGraph}
\end{center}
\caption{An example of a messaging graph based on Section \ref{sec:dbtoaster}'s example query.  (a) The messaging graph for the \texttt{on\_insert\_customer} event.  (b) The effects of splitting view $m_c$ on \texttt{ordkey}.  (c) The effects of splitting $m_c$ on \texttt{custkey}.}
\label{fig:diag:messagingGraph}
\vspace*{-0.2in}
\end{figure}

For example, consider the transition function that results from an update to the customer table.  One specific subtask of this transition reads $m_c$ and writes to $m$.  Treating each view as a node, this task has one edge with one read node and one write node.  

DBToaster considers database layout in terms of how it partitions data across a physical medium (i.e., memory, disk pages, or a cluster).  Viewed through the messaging graph, a partitioning is an assignment of all nodes in the graph to one (or more, in the case of replication) partition.  For example, if they were small enough, $m$ and $m_c$ might be placed on one disk page each.  Thus, the subtask requires a read on one page, and an update to a second.

Subdivision of individual views is represented in the messaging graph by splitting of graph nodes.  Of particular interest is how the new nodes interact with the hyperedge(s) connected to the original node.  As the split occurs, a node may stay connected to a hyperedge, the hyperedge may likewise be split, or in some cases, only one node will remain connected (see Figure \ref{fig:diag:messagingGraph}b,c).  DBToaster can exploit the limited range of node split/hyperedge interactions to select an effective partitioning scheme.

For example, when partitioning $m_c$, horizontal partitioning on the value of \texttt{ordkey} increases the number of nodes connected to the \texttt{on\_insert\_customer} task edge, while using \texttt{custkey} does not provoke an increase.  If the data represented by these nodes is split across multiple disks or servers, the computation must still access all of them.  The roles are reversed for the \texttt{on\_insert\_order} task edge.  Under (the false) assumption that both insert events occur with identical frequency, DBToaster can partition on both keys to minimize the number of connected nodes.

Additional knowledge about the dataset enhances the messaging graph produced by DBToaster.  For instance, the E-R diagram can be integrated into the messaging graph; the chain of foreign key dependencies in $q$ is strictly hierarchical.  DBToaster uses this information and creates partitions along a single axis with a secondary index to bound the number of partitions accessed by each update subtask, with respect to the total number of partitions generated.  Similarly, this information is used by DBToaster to select a partitioning scheme that places nodes typically connected by a subtask into a single partition; this is analagous to co-clustering in a traditional DBMS.
