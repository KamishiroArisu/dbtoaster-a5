A DDMS is created with knowledge of the workload it will be used for; though the set of transition functions may be infinite, there is a finite set of classes of transition function that we can analyze, not only to simplify incremental computation, but also to streamline the storage mechanisms underpinning the database.

For example, consider a DDMS constructed to maintain a view of the base relations as well as the query\texttt{\\
SELECT p.id, COUNT(DISTINCT e.student)\\
FROM Professors p, Classes c, Enrollment e\\
WHERE p.id = c.teacher AND c.id = e.class\\
GROUP BY p.id\\
}
Here, \texttt{c.teacher} and \texttt{e.class} are foreign keys mapped to \texttt{p.id} and \texttt{c.id} respectively.  We can implement this query in the DDMS by using two views: one of students for each professor, and another containing the count of students for each professor.  Though the data is drawn from two distinct logical views, the most efficient storage mechanism would place the student list on the same page as the student count.

\subsection{Partitioning}
Compared to stream processing systems, DDMSes have much more state, necessitating the use of more intelligent storage techniques.  However, compared to traditional DBMSes, DDMSes have more information to use when constructing a task-specific storage solution.  Regardless of the exact nature of the storage being used: whether it be pages on a disk, disks in a RAID, or servers in a cluster, the DDMS' goal is to select an efficient allocation of data to physical storage units.

The notion of transition functions presents us with a clean mechanism for quantifying the cost (or benefit) of applying a particular partitioning scheme.  Given a specific transition - a parametrized transition function that is - we can quantify not just the cost of a partitioning scheme, but also the cost of further sub-partitioning techniques.  Integrating over the transition function's parameters and all transition functions, we obtain the cost (or benefit) of a partitioning scheme for the entire DDMS.  

For example, in the DDMS example above, we have three views.  Transitions triggered by insertions into the Professors and Classes tables involve only writes (due to the foreign key constraints).  Conversely, transitions triggered by an insertion into the Enrollment table, first require a read (to validate the DISTINCT keyword), and two subsequent updates: one for the professor statistics and one for the student list.

\subsection{Secondary Storage}
Each transition function can be represented as a bipartite directed hypergraph; each node on the left represents a segment of the database being read from, each node on the right represents a segment being written to, and each hyperedge represents an independent subtask of the transition function.

For the running example above, inserting a row into the Enrollment table might result in a read on a single entry in the Classes table, and read/writes on an intermediate view representing the students taught by each professor and the Professor's statistics.  This includes three nodes on the left (the views being read from), two node on the right (the views being written to), and one edge.

Every time a portion of the database is split into multiple components, nodes on either side of the graph are likewise split into multiple components.  More importantly, depending on the workload, zero or more additional edges will also be created, depending on the query.  The mechanism by which additional edges are created follows one of a very small number of patterns that can be used to inform the partition allocation scheme.  

For example consider the following variation on the previous query:\texttt{\\
SELECT p.school, COUNT(DISTINCT e.student)\\
FROM Professors p, Classes c, Enrollment e\\
WHERE p.id = c.teacher AND c.id = e.class\\
GROUP BY p.school\\
}
The DBToaster algorithm will create an intermediate view containing a slightly extended version of the Classes table.  In the absence of a key/foreign key constraint, each insertion into the Enrollment table must iterate over all classes and professors with matching identifiers.  This is done by using an intermediate view containing the results of a join on the Professors and Classes tables.  

We could, for instance, partition the Professors\_Classes view into 4 components by horizontally splitting the c.teacher and c.id columns in half.  However, every subsequent insertion into Enrollment must read from each of the two partitions with a matching c.id, and every insertion into Professors must update each of the the two partitions with a matching c.teacher.  

Conversely, if we split the view into four components based on the professor id, updates to Professors could be done instantly, but updates to Enrollment must now read from four components.  By establishing a clear hierarchy of foreign key relationships, we limit the explosion of reads required to one single partition.

1-par implementation of secondary storage optimization: minimize \# of unbound
edge reads (ie, find me whether or not students are present)
%%%%%%%%%%%%%

1-par implementation of distributed optimization: minimize \# of cross-partition
edges
%%%%%%%%%%%%%

\subsection{Datastructures}
Not only does advance knowledge of the query workload streamline persistent data storage, it also enables the use of more effective in-memory datastructures.  For instance, input data can be categorized based on whether it is used in a conditional expression, or purely arithmetically.  In the running example, the number of distinct students a professor teaches is an example of the latter, while the student, course, and professor keys are all examples of the former.

This categorization into key and data columns, respectively, can make very effective use of multi-key hash-table.  Rather than storing views as a series of rows, they are stored as a series of nested hash-maps, where each layer of nesting represents an additional key-column.  Sometimes iteration is necessary, as in the following query:\texttt{\\
SELECT w.owner, SUM(i.count * p.price)\\
FROM Warehouse w, Inventory i, Product p\\
WHERE w.id = i.warehouse AND p.id = i.product\\
GROUP BY w.country\\
}
An update to the Products table, such as an updated price for hand-woven baskets, requires an iteration over warehouses stocking them.  For this purpose, the DBToaster algorithm would produce a view that extends the contents of the Inventory table with the inventory count, part price and warehouse owner.  This view is stored as a multi-key map with the product id and warehouse id used as keys.

When product information is updated, the product id is bound, and we iterate over all matching warehouse ids.  This is easily done if the product id key is used as the key for the outermost hash-table.  However, when warehouse information is updated, the warehouse id is bound, and we iterate over all matching product ids.  

This sort of arbitrary scanning is achieved by maintaining secondary indices.  Because the query workload is known in advance, the exact set of secondary indices required can be computed at compile time.  

%%%%%% should we make any comments here about the relationship between the presence of secondary indices and partitioning schemes?  Otherwise, I'm not sure we have anything to say about maps and datastructures other than (like tables) they're very amenable to horizontal partitioning.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%In keeping with the DDMS' update-centric view of the world, we view each transition function as a series of write operations, each of which may require one or more read operations.  Both read and write operations may operate on a single row, or iterate over all rows in the table matching a selection predicate.  We can construct a directed hypergraph out of the write operations, with each node representing a table, each out-edge representing a read, and each in-edge representing a write.  We refer to this hypergraph as the transition function's data-flow graph.
%
%This data-flow graph provides a useful mechanism for analyzing the storage, processing, and IO requirements of a query.  In particular it simplifies the analysis of schemes that partition data across multiple physical units, be they disk blocks, disks, or storage servers.  
%
%Regardless of the partitioning scheme selected, each table partitioning will introduce new nodes and edges into the data-flow graph.  Based on the query involved, we can accurately predict how many and which new edges will be introduced.  From this point, we can allocate graph nodes to each partition; 
%
%
%From this point, the partitioning problem begins to resemble a weighted set-cover problem; .  Separating 
%Consider a horizontal partitioning scheme.  Table partitions introduce new nodes and edges into the dataflow graph; we can 
%
%As we partition each table into multiple components, we introduce new nodes and edges into the dataflow graph.  
%
%For example, consider a simple database transition function that emulates the following query\texttt{\\
%INSERT INTO T(a,c')\\
%SELECT a, SUM(c)\\
%FROM R(a,b), S(b, c)\\
%WHERE R.b = S.b\\
%GROUP BY a\\
%}
%
%  partitioning the state of the database 
%This transition function consists of a single hyperedge with two inputs and one output.  The input tables are already narrow, so when, we consider only horizontal partitioning.  We can use the dataflow graph to 
%
%If it becomes necessary to partition the state of the database across multiple disk blocks, disks, or storage servers, we can use the , we could store the entire 
%
%reads from two tables and writes to a third.  This transition function's dataflow graph has a single directed hyperedge with two inputs and one output.  The first reader reads a single 
%
%\begin{itemize}
%\item Overview of read, write, and message costs
%
%\item Partitioning across multiple axis, expanding the dataflow graph into a messaging graph.  
%
%\item Messaging and computational (memory) resources, separability of subgraphs.
%
%\item Basic instantiation - 1 disk or cluster
%
%\item Extensions to the multi-disk case
%\end{itemize}