Scaling up a DDMS requires not only the storage of progressively more data, but also a dramatic increase in computing resources.  As alluded to in Section \ref{sec:storage}, DDMS are amenable to having their data distributed across a cluster.  

\comment{Unlike ad-hoc query systems [MapReduce,HDFS], a distributed DDMS can use advance knowledge of the data usage patterns to better coordinate the distribution of processing with its data placement scheme}

DBToaster considers two general placement techniques: (1) Compute the data where it will be stored, or (2) Store the data where it will be used.  The latter approach necessitates potentially extensive replication of data across the cluster, but enables the use of optimistic computation techniques that substantially reduce computational latency.

The total ordering of transitions between database steps provides DBToaster with a crucial synchronization mechanism; Each transition is conditioned on the prior state of the database and inter-transition interactions can be precomputed at compile time.  Unfortunately, imposing such a total ordering in a distributed setting is not straightforward.  Entities trying to trigger updates in parallel will easily overwhelm a centralized sorting node at the scales we are interested in.  

Instead, we can impose a logical total ordering by combining rough clock synchronization with a total ordering over the clients, and design clients to support out-of-order updates.  The total ordering prevents a transition from seeing later iterations of the database, and update logs make it possible to re-evaluate portions of the update processed on a premature view of the database.  

This out-of-order mechanism implements an eventual consistency model that may not be sufficient for all data processing tasks.  DBToaster uses a background task to periodically identify the furthest point in the update stream that has been fully propagated, and produces a consistent snapshot from the data at that point.  Interestingly, this background task is already necessary for garbage collection purposes.  Thus, the same system can simultaneously produce a low-latency eventual consistency view of the database as well as a periodic consistent snapshot of the same data, an interface similar that provided by\cite{bayou}.