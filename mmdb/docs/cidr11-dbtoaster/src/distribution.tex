Scaling up a DDMS requires not only the storage of progressively more data, but also a dramatic increase in computing resources.  As alluded to in Section \ref{sec:storage}, DDMSes are amenable to having their data distributed across a cluster.  Unlike ad-hoc query systems [MapReduce,HDFS], a distributed DDMS can use advance knowledge of the data usage patterns to better coordinate the distribution of processing with its data placement scheme

Two general placement techniques immediately suggest themselves: (1) Compute the data where it will be stored, or (2) Store the data where it will be used.  The latter approach necessitates potentially extensive replication of data across the cluster, but enables the use of optimistic computation techniques that substantially reduce computational latency.

The total ordering of transitions between database steps provides DDMS' with a crucial synchronization mechanism; Each transition is conditioned on the prior state of the database, but inter-transition interactions can be precomputed at compile time.  Unfortunately, imposing such a total ordering in a distributed setting is not straightforward.  Many disparate entities may be trying to trigger updates in parallel, and a single centralized sorting node has bounded scaling capacity.  Instead, we can impose a logical total ordering by combining rough clock synchronization with a total ordering over the clients.  This however, forces cluster nodes to support out-of-order execution of database transitions.

Fortunately, DBToaster provides a clean mechanism for implementing out-of-order execution.  Because all updates in DBToaster are deltas, they can be applied in arbitrary order.  The total ordering prevents a transition from seeing later iterations of the database, while update logs make it possible to re-evaluate portions of the transition processed optimistically on a premature view of the database.  

This out-of-order mechanism implements an eventual consistency model that may not be sufficient for all data processing tasks.  By using a background task to identify iterations of the database that have no pending modifications, we can produce a periodic consistent snapshot of the database.  Interestingly, this background task is already necessary for garbage collection purposes.  Thus, the same system can simultaneously produce a low-latency eventual consistency view of the database as well as a periodic consistent snapshot of the same data, an approach similar to [Bayou].