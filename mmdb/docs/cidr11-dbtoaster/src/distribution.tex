Scaling up a DDMS requires not only storing progressively more data, but also a dramatic increase in computing resources.  As alluded to in Section \ref{sec:storage}, DDMS are amenable to having their data distributed across a cluster.  We have implemented a cluster-based storage and processing system as part of DBToaster called Cumulus.

\comment{Unlike ad-hoc query systems [MapReduce,HDFS], a distributed DDMS can use advance knowledge of the data usage patterns to better coordinate the distribution of processing with its data placement scheme}

Cumulus considers two general placement techniques: (1) Compute the data where it will be stored, or (2) Store the data where it will be used.  The latter approach necessitates potentially extensive replication of data across the cluster, but enables the use of optimistic computation techniques that substantially reduce computational latency.

The total ordering of transitions between database steps provides Cumulus with a clean synchronization abstraction; Each transition is conditioned on the prior state of the database and potential inter-transition conflicts are known at compile time.  Unfortunately, imposing such a total ordering in a distributed setting is not straightforward.  Entities trying to trigger updates in parallel will easily overwhelm a centralized sorting node at the scales we are interested in.  

Instead, Cumulus imposes a logical total ordering by combining rough clock synchronization with a total ordering over the clients.  The total ordering prevents a transition from seeing later iterations of the database.  This requires Cumulus to support out-of-order updates, which it does by optimistically performing computations and partially recomputing the result if one of the inputs turns out to be wrong.

Cumulus' out-of-order mechanism implements an eventual consistency model that may not be sufficient for all data processing tasks.  Cumulus uses a background task to periodically identify the furthest point in the update stream that has been fully propagated, and produces a consistent snapshot from the data at that point.  Interestingly, this background task is already necessary for garbage collection purposes.  Thus, the same system can simultaneously produce a low-latency eventual consistency view of the database as well as a periodic consistent snapshot of the same data, an interface similar that provided by\cite{bayou}.