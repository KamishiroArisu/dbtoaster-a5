Scaling up a DDMS requires not only storing progressively more data, but also a
dramatic increase in computing resources.  As alluded to in Section
\ref{sec:storage}, DDMS are amenable to having their data distributed across a
cluster.  We have implemented a cluster-based runtime as part of DBToaster.

\comment{Unlike ad-hoc query systems [MapReduce,HDFS], a distributed DDMS can use advance knowledge of the data usage patterns to better coordinate the distribution of processing with its data placement scheme}

DBToaster considers two general techniques for distributing data and
computation: (1) Compute the data where it will be stored, or (2) Store the data
where it will be used.  The latter approach necessitates potentially extensive
replication of data across the cluster, but enables the use of optimistic
computation techniques that substantially reduce computational latency.

\medspace 

{\bf Creating a total order}\/.
The total ordering of transitions between database steps provides DBToaster with
a clean synchronization abstraction; Each transition is conditioned on the prior
state of the database and potential inter-transition conflicts are known at
compile time.  Unfortunately, imposing such a total ordering in a distributed
setting is not straightforward.  Clients trying to trigger updates in parallel
will easily overwhelm a centralized sorting node, at least at the scales we are
interested in.

Instead, DBToaster imposes a logical total ordering by combining rough clock
synchronization with an arbitrary total ordering over the clients.  The total
ordering ensures that a transition sees only the effects of prior transitions,
though it may not see all of them.  This forces DBToaster to support
out-of-order updates, by optimistically applying the transition and recomputing
those portions later changed by transitions occurring earlier in the logical
ordering.

\medspace

{\bf Hybrid consistency}\/.
DBToaster's out-of-order mechanism implements an eventual consistency model that
may not be sufficient for all data processing tasks.  DBToaster uses a
background task to periodically identify the furthest point in the update stream
that has been fully propagated, and produces a consistent snapshot from the data
at that point.  Interestingly, this background task is already necessary for
garbage collection.  Thus, the same system can simultaneously produce a
low-latency eventually consistent agile view, as well as a periodic consistent
snapshot of the same data, an interface similar to that provided
by~\cite{bayou}.

\medspace

{\bf Availability}\/.
One other interesting point in the distributed design space is that of
availability and recovery.
%Traditional stream processors dwell entirely in-memory for efficiency reasons, and rely on replication-based strategies to provide high availability.  Conversely, t
The large state found in a DDMS necessitates the use of out-of-core storage. 
Instead of over-provisioning replicas and consuming precious network bandwidth
maintaining them, DBToaster can ensure recoverability by maintaining data in a
persistent store.  Tuning the tradeoffs between these various options, however,
remains an open issue.
