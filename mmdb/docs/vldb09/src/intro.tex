\section{Introduction}
We present \compiler, a tool to support a novel query execution model, compiling
SQL queries into C programs, targeting the emerging hardware platform of
main-memory databases. At the heart of our work, we question the cost of
providing a highly flexible runtime, as found with modern databases in plan
interpreters and other runtime components including schedulers and optimizers.
Modern relational databases focus almost exclusively on providing flexible and
extensible querying capabilities in dynamic environments, supporting both schema
and data evolution. From a hardware perspective, rapidly increasing main-memory
capacities presents an opportunity to rethink database architectures. Today's
databases are heavily concerned with efficiency across the main-memory and
secondary storage boundary, and their design principles have spilled
into main-memory databases architectures.

Our work on \compiler\ is motivated by the following applications, which in
our view cannot be processed efficiently with any of the data management tools
available today.

\noindent \textbf{Processing order books in equities trading.}
Following a call for greater transparency~\cite{sec-orderbook:00} earlier this
decade, many stock exchanges provide investors with complete bid and ask limit
order books, enabling a superior view of the market microstructure for use in
trading algorithms. The bid order book consists of prices and volumes investors
are willing to buy equities in descending price and timestamp order, and
correspondingly the ask order book indicates investors' selling prices. Exchanges
execute trades by matching the top of the bid and ask order books, in addition to
handling market orders. Investors continually add, modify or withdraw limit
orders, thus we view order books as relations subject to high volumes of order
inserts, updates and deletes.

Investors and automated trading systems express diverse trading strategies on
these order books, and the success of trading depends critically on the speed at
which the programmed strategies process the data. Unlike stream processing
scenarios \cite{abadi-vldbj:03,motwani-cidr:03}, order books do not grow
unboundedly in practice, and cannot be expressed by windows given their arbitrary
inserts, updates and deletes. Thus in \compiler, we process continuous queries
over temporal snapshots of relations via delta processing.
\comment{
However in the order book scenario, neither time-, row- or punctuation-based
window semantics apply, rather each insert, update or delete statement defines
a new temporal snapshot of the order book relation. This data model is
particularly relevant since trading strategies continuously query the full
order book to determine an action.

However, trading strategies continuously require access to the full order book
relation, which cannot be expressed by windows given arbitrary inserts, updates
and deletes. Furthermore, order books do not grow unboundedly in practice, the
number of updates and deletes is proportional to the number of inserts and there
is no need for the system to manage the scope of the relation. Thus in
\compiler, we process continuous queries over temporal snapshots of relations
expressed as deltas to base relations.

High-level declarative languages have been applied in similar areas of finance,
such as technical analysis, for example through stream processing engines
\cite[abadi-vldbj:03,motwani-cidr:03]. However, trading strategies continuously
require the full order book and order books do not grow unboundedly in practice
-- they cannot be expressed by windows given arbitrary inserts, updates and
deletes. Thus in \compiler, we process continuous queries over temporal snapshots
of relations expressed as deltas to base relations.
}
To illustrate this, we provide a simple example query based on a Static Order
Book Imbalance (SOBI) trading strategy. SOBI computes a volume-weighted average
price (VWAP) over those orders whose volume makes up a fixed fraction of the
total stock volume in each of the bid and ask order books. SOBI then compares the two
VWAPs. For simplicity, we present the VWAP for the bids:

\begin{verbatim}
select avg(bcv.price*bcv.volume) as bid_vwap from
    (select sum(volume) as total_volume
        from bids) as bv,
    (select b2.price, b2.volume,
            sum(b1.volume) as cumsum_volume
        from bids b1, bids b2
        where b1.price > b2.price
        group by b2.price, b2.volume) as bcv
    where bcv.cumsum_volume < k * bv.total_volume)
    as vwap_input
\end{verbatim}

\noindent Above, the contents of the bids order book is continually changing,
and \compiler\ produces a new output on every insert, update or delete.

\comment{
Above, the total volume in the order book is continually changing, thus computing
the VWAP over those orders comprising the top $k$\% of volume requires the full
order book relation. This is easily expressed with SQL over standard relations.
\compiler\ produces a new output on every insert, update or delete to the order
book.
}
\comment{
\begin{verbatim}
select case
    when s.vwap - b.vwap > threshold then
        'buy', V, lastPrice - delta, 'sell', V, lastPrice + hedgeDelta
    when b.vwap - s.vwap > threshold then
        'sell', V, lastPrice + delta, 'buy', V, lastPrice - hedgeDelta
from
     (select avg(bcv.price*bcv.volume) as vwap from
        (select sum(volume) as total from bids) as bv,
        (select b2.price, b2.volume, sum(b1.volume) as cumsum from
            bids b1, bids b2 where b1.price > b2.price group by b2.price, b2.volume) as bcv
        where bcv.cumsum < k * bv.total) as b,
    (select avg(scv.price*scv.volume) as vwap from
        (select sum(volume) as total from asks) as sv,
        (select s2.price, s2.volume, sum(s1.volume) as cumsum from
        asks s1, asks s2 where s1.price < s2.price group by s2.price, s2.volume) as scv
    where scv.cumsum < k * sv.total) as s
\end{verbatim}
}
\comment{
Give a nice example of a trading strategy that delegates the bulk of the work to compiled SQL queries.
This is a nice scenario because order books do not get very large (if we
want to process several order books, we can nicely partition the work by
order book across several machines), change extremely rapidly, and the
success of trading depends crucially on the speed with which the programmed
strategies process the data.
}

\begin{itemize}
\item
Data warehouse loading: data integration and aggregation.
Commercial data warehouse loaders also use straight-line code for aggregation,
but the data integration queries that are executed before aggregation are
currently not compiled. Compiling them only yields a really substantial
improvement if the integration and aggregation queries are compiled together.
This may substantially reduce the time taken to load a warehouse. (Here we may
have to scale up by using several machines.)
\end{itemize}

In these applications, queries are \textit{ossified} during the development
cycle, with developers converging to a choice of schema, query structure and parameters.
Once hardened, queries are deployed into production environments, and reused
numerous times, executing non-interactively. We focus on applications that
do not require access to large static databases, rather our applications access bounded
size relations, where the contents of these relations change frequently through
inserts, updates and deletes, in streaming fashion. We assume a single entity
conveys these updates to the database, this is not a multi-user highly-concurrent
production environment as found in OLTP applications.
 
We introduce \compiler, a compiler that produces C code from hardened queries, in
contrast to executing a query plan comprised of a set of operators. 
While one could always naively generate a straight-line function from a query
plan, we devise our query kernel functions by factoring in the following
observations regarding query processing in main memory databases:

\begin{enumerate}
\item
When we compile queries to straight-line C code, it is best to use incremental
maintenance techniques to eagerly keep query results up to date.

\item
Index nested loop joins on main memory hash tables
dominate all other join techniques in the compiled
main-memory database setting.
It is worth having hash tables whose buckets are sorted, though (e.g. for
top-k query processing).

\item
There is something to be gained from bulk updates.
do we not just want to support bulk updates of one kind (e.g., just inserts),
but mixtures of inserts, updates, and deletes? Is this an optimization, and
worth the work?

\item
There is no point in adaptivity beyond bulk updates.
\end{enumerate}

Our compiler generates code to implement an incremental query execution strategy,
where deltas (inserts, deletes or updates) to base tables are processed with a
\textit{query kernel function}. These kernel functions compute a new query result
from every delta, and manipulate auxiliary data structures (conceptually
precomputed views) to support fully incremental processing (i.e. no blocking
operations). We choose our data structures intelligently to ensure efficient
delta processing, via a top-down decomposition of the query into a set of
associative data structures (maps). Our kernel functions correspond to
tuple-at-a-time processing through a query plan, and can be highly optimized by
modern C compilers to yield an efficient native binary for query execution.

Given the inherent streaming nature of the applications we consider, the maps we
maintain during query execution incur a different footprint compared to both
main-memory databases and stream processing engines, since in many cases we need
not keep around base relations, but on the other hand are maintaining precomputed
views. We analyse the memory utilization of the data structures resulting from
our decomposition. Additionally, we extend the functionality of our compiler
beyond that of the compiling a single query, including features such as optimized
execution for bulk operations based on loop optimization techniques, multi-query
compilation that can share data structures given commonality amongst queries, and
a limited form of query flexibility that allows users to change parameters during
query execution through parameterized compilation.

This paper is laid out as follows. Section 2 describes the role of the compiler,
and a high-level view of \project, a new project at Cornell investigating
database engine runtimes using SQL queries compiled with \compiler. As part of
its scope, \project\ will investigate scaling compiled queries over a distributed
main-memory database using a variety of parallelization and data partitioning
techniques customized to the execution strategy adopted during compilation.
Section 3 describes our query compilation algorithm in terms of a decomposition
algorithm that both selects the maps we maintain during execution, and derives
the maintenance tasks for each map. We also present a hypertree decomposition
algorithm that generalizes our compilation algorithm to apply to arbitrary join
graphs. Section 4 analyses the memory requirements of our maps, while Section 5
describes the aforementioned extensions to the compiler for bulk operations and
data structure sharing. Finally prior to discussing related work and concluding,
Section 6 presents experimental results demonstrating the significant benefits of
our techniques over both a standard relational database, as well as a naive
compilation of the query plan as a straight-line function.


\comment{
We compile queries into C programs. Queries are incrementally maintained.
We are smart about creating data structures for very efficiently keeping query
results up to date. The programming interface supports update triggers as
well as cursor-based iteration over the current query result.

Tasks:
\begin{itemize}
\item
Compile multiple queries into common code, share data structures.

\item
How to compile queries of which some selection conditions can be modified at
runtime? (As in ODBC prepare statements.)

\item
Scale up using data partitioning, parallelization. This is not the focus of
this paper.

\item
Should we say something about accessing secondary storage, or will we leave
this out of scope? If we are not restricted to streaming scenarios, we may
have broader applicability of our techniques.

\item
We have to analyse our main memory consumption.
\end{itemize}


Aim: query processing where the queries are fixed at compile time.
In particular applications where there is no large static database of which
only a small part needs to be processed. The approach works particularly
well if the data arrives on a stream or changes very rapidly

We assume that there is only a single user who performs updates, and that
we do not care about concurrency control.
This is not an OLTP scenario.

Applications:
\begin{itemize}
\item
Processing order books. Give a nice example of a trading strategy that
delegates the bulk of the work to compiled SQL queries.
This is a nice scenario because order books do not get very large (if we
want to process several order books, we can nicely partition the work by
order book across several machines), change extremely rapidly, and the
success of trading depends crucially on the speed with which the programmed
strategies process the data.

\item
Data warehouse loading: data integration and aggregation.
Commercial data warehouse loaders also use straight-line code for aggregation,
but the data integration queries that are executed before aggregation are currently not compiled. Compiling them only yields a really substantial improvement
if the integration and aggregation queries are compiled together. This may substantially reduce the time taken to load a warehouse. (Here we may have to scale up by using several machines.)
\end{itemize}

Claims / theses regarding query processing in main-memory databases (to be verified by experiments):
\begin{enumerate}
\item
When we compile queries to straight-line C code, it is best to use incremental
maintenance techniques to eagerly keep query results up to date.

\item
Index nested loop joins on main memory hash tables
dominate all other join techniques in the compiled
main-memory database setting.
It is worth having hash tables whose buckets are sorted, though (e.g. for
top-k query processing).

\item
There is something to be gained from bulk updates.
do we not just want to support bulk updates of one kind (e.g., just inserts),
but mixtures of inserts, updates, and deletes? Is this an optimization, and
worth the work?

\item
There is no point in adaptivity beyond bulk updates.
\end{enumerate}


Experiments:
Destroy classical databases (4 orders of magnitude improvement). Demonstrate
that incremental maintenance is a good idea for aggregates (two orders
of magnitude improvement over compiled non-incremental code).
}
