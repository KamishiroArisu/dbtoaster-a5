\section{Introduction}


We present \compiler, a tool for compiling
SQL aggregate queries into efficient C code for
continuous, incremental view maintainance.
%
At the heart of our work, we question the cost of
highly flexible interactive query processors as found in today's databases,
with their plan
interpreters and other runtime components including schedulers and optimizers.
Modern relational databases focus almost exclusively on providing flexible and
extensible querying capabilities in dynamic environments, supporting both
schema and data evolution.
%
However, a large fraction of the world's query workloads
are fixed and embedded into database application programs.
In these applications, queries are \textit{ossified} during the development
cycle, with developers converging to a choice of schema, query structure and parameters.
Once hardened, queries are deployed into production environments, and re\-used
numerous times, executing non-interactively.


From a hardware perspective, rapidly increasing main-memory
capacities create an opportunity to rethink database architectures. Today's
database query processors focus on efficiency across the main-memory and
secondary storage boundary, and their design principles have spilled
into main-memory database architectures. Apart from a number of traditional
data management applications such as data warehousing that are becoming
increasingly
plausible applications of (parallel) main-memory databases, there are also a
number of new applications that call for query processing support and are
not well served by existing systems.
These applications call for the rethinking of query processing in main
memory databases.





DBToaster is an   SQL query  compilation framework  which generates
native  code that incrementally  and continuously  maintains aggregate
views  in main  memory at  high  update rates  using delta  processing
techniques. DBToaster provides two  key advantages.
%
By generating C  code that  performs all the  processing required  for an
input  tuple as  a simple  straight-line function,  we expose  a fixed
query plan execution path to  a C compiler for optimization, and avoid
overheads that traditionally arise when query processors interpret
query plans stored in dynamic data structures.
%
Moreover, our new delta processing techniques
are designed specifically
for compilation to native code and support aggressive inlining
that leads to surprisingly small and simple
straight-line code sequences.

Thus, DBToaster is particularly well suited in continuous query applications
in with a view has to be maintained through high data update rates.
DBToaster is  motivated  by  applications  that  require  the  highly
efficient answering of fixed workloads of aggregation queries, such as
in  data stream  processing,  online data  warehouse  loading, and  in
financial applications.
Our work on \compiler\ was particularly motivated by the need for query
processing support in algorithmic equities trading, which in
our view cannot be efficiently served by any of the data management tools
available today, as well as aggregate processing in data stream systems and
data warehouse loaders.


\medskip


\textbf{Processing order books in equities trading.}
Following a call for greater transparency~\cite{sec-orderbook:00} earlier this
decade, many stock exchanges provide investors with complete bid and ask limit
order books, enabling a superior view of the market microstructure for use in
trading algorithms. The bid order book consists of prices and volumes of
orders of investors who
are willing to buy equities in descending price and timestamp order, and
correspondingly the ask order book indicates investors' selling orders.
Exchanges execute trades by matching the tops of the bid and ask order books,
in addition to
handling market orders. Investors continually add, modify or withdraw limit
orders, thus we view order books as relations subject to high volumes of order
inserts, updates and deletes.

Investors and automated trading systems express diverse trading strategies on
these order books, and the success of trading depends critically on the
speed at
which the programmed strategies process the data.
In fact, the availability of order book data has yielded substantial opportunities for automatic, algorithmic trading approaches, and in recent years,
algorithmic trading systems have come to account for a majority of 
volume traded at the major US and European stock markets.

Unlike stream processing
scenarios \cite{abadi-vldbj:03,motwani-cidr:03}, order books do not grow
unboundedly in practice, but cannot be expressed by windows given their arbitrary
inserts, updates and deletes. Thus in \compiler, we process continuous queries
over temporal snapshots of relations via delta processing.
Providing such aggregate views allows to run sophisticated trading strategies.
\comment{
However in the order book scenario, neither time-, row- or punctuation-based
window semantics apply, rather each insert, update or delete statement defines
a new temporal snapshot of the order book relation. This data model is
particularly relevant since trading strategies continuously query the full
order book to determine an action.

However, trading strategies continuously require access to the full order book
relation, which cannot be expressed by windows given arbitrary inserts, updates
and deletes. Furthermore, order books do not grow unboundedly in practice, the
number of updates and deletes is proportional to the number of inserts and there
is no need for the system to manage the scope of the relation. Thus in
\compiler, we process continuous queries over temporal snapshots of relations
expressed as deltas to base relations.

High-level declarative languages have been applied in similar areas of finance,
such as technical analysis, for example through stream processing engines
\cite[abadi-vldbj:03,motwani-cidr:03]. However, trading strategies continuously
require the full order book and order books do not grow unboundedly in practice
-- they cannot be expressed by windows given arbitrary inserts, updates and
deletes. Thus in \compiler, we process continuous queries over temporal snapshots
of relations expressed as deltas to base relations.
}
To illustrate this, we provide a simple example query that is at the heart of
a Static Order
Book Imbalance (SOBI) trading strategy. SOBI computes a volume-weighted average
price (VWAP) over those orders whose volume makes up a fixed top fraction of the
total stock volume in each of the bid and ask order books. SOBI then compares the two
VWAPs. For simplicity, we present the VWAP for the bids only:

\begin{verbatim}
select avg(b3.price*b3.volume) as bid_vwap
from   (select sum(volume) as total_volume
        from   bids) as bv,
       (select b2.price,
               sum(b1.volume) as cumsum_volume
        from   bids b1 right outer join
               (select distinct price from bids) b2
               on (b1.price > b2.price)
        group by b2.price) as bcv,
       bids b3
where  k * bv.total_volume > bcv.cumsum_volume
and    bcv.price = b3.price
\end{verbatim}
%    as vwap_input
%
Above, the contents of the bids order book is continually changing,
and \compiler\ produces a new output on every insert, update or delete.


\comment{
Above, the total volume in the order book is continually changing, thus computing
the VWAP over those orders comprising the top $k$\% of volume requires the full
order book relation. This is easily expressed with SQL over standard relations.
\compiler\ produces a new output on every insert, update or delete to the order
book.
}
\comment{
\begin{verbatim}
select case
    when s.vwap - b.vwap > threshold then
        'buy', V, lastPrice - delta, 'sell', V, lastPrice + hedgeDelta
    when b.vwap - s.vwap > threshold then
        'sell', V, lastPrice + delta, 'buy', V, lastPrice - hedgeDelta
from
     (select avg(bcv.price*bcv.volume) as vwap from
        (select sum(volume) as total from bids) as bv,
        (select b2.price, b2.volume, sum(b1.volume) as cumsum from
            bids b1, bids b2 where b1.price > b2.price group by b2.price, b2.volume) as bcv
        where bcv.cumsum < k * bv.total) as b,
    (select avg(scv.price*scv.volume) as vwap from
        (select sum(volume) as total from asks) as sv,
        (select s2.price, s2.volume, sum(s1.volume) as cumsum from
        asks s1, asks s2 where s1.price < s2.price group by s2.price, s2.volume) as scv
    where scv.cumsum < k * sv.total) as s
\end{verbatim}
}
\comment{ Give a nice example of a trading strategy that delegates the bulk of
the work to compiled SQL queries. This is a nice scenario because order books do
not get very large (if we want to process several order books, we can nicely
partition the work by order book across several machines), change extremely
rapidly, and the success of trading depends crucially on the speed with which the
programmed strategies process the data. }



\textbf{Stream processing with complex, nested aggregates}
Previous work on data stream processing and incremental view maintenance
has mostly neglected supporting the efficient processing of complex nested
aggregates. However, such queries do have important applications. For example,
the VWAP query above is highly challenging and, to the best of our
knowledge, cannot be efficiently processed by any of these systems.


\textbf{Data warehouse loading.}
Loading large data warehouses is a computationally costly process, which
causes most data warehouse loading to be performed offline.
While commercial warehouse loaders use specialized efficient code for 
aggregation, incoming data is often the result of data integration
queries that are costly and inefficient, and with may blow up data sizes
in such a way that loading remains inefficient.
Compiling data integration and aggregation queries together may yield efficient
code for loading the warehouse which may avoid the materialization of large
intermediate results.


\medskip



\nop{
We focus on applications that do not require access to large static
databases, rather our applications access bounded
size relations, where the contents of these relations change frequently through
inserts, updates and deletes, in streaming fashion. We assume a single entity
conveys these updates to the database, this is not a multi-user
highly-concurrent
production environment as found in OLTP applications.
}




Our query compilation framework is based on the notion of composing maps
definable in a {\em map algebra}\/ closely motivated by SQL.
We compile insert, update, and delete operations into efficient code for computing
deltas of these maps. This corresponds to a form of
tuple-at-a-time processing in a query plan, but can be highly optimized by
modern C compilers to yield an efficient native binary for query execution.

Given the inherent streaming nature of the applications we consider, the maps we
maintain during query execution incur a different footprint compared to both
main-memory databases and stream processing engines, since in many cases we need
not keep around base relations, but on the other hand are maintaining precomputed
views. We analyse the memory utilization of the data structures resulting from
our decomposition. Additionally, we extend the functionality of our compiler
beyond that of the compiling a single query, including features such as optimized
execution for bulk operations based on loop optimization techniques, multi-query
compilation that can share data structures given commonality amongst queries, and
a limited form of query flexibility that allows users to change parameters during
query execution through parameterized compilation.


As we  show, our techniques are several orders
of  magnitude   faster  than  state-of-the-art   database  and  stream
processing engines on such workloads.  In the case of queries on limit
order  book  data  as  required for  supporting  algorithmic  equities
trading, our approach currently stands alone in its ability to support
realistic  data rates  on contemporary  hardware without  resorting to
very substantial computing clusters.
It appears that the memory consumption of our main-memory techniques
is sufficiently low to support applications such as data warehouse loading.
Moreover, in most of these applications, our delta processing techniques,
which continuously maintain a current view of the query result, outperform
batch processing techniques even if we only want to access the
view once. That is, our incremental query result construction usually
outperforms one-of query evaluation with traditional techniques.





This paper is laid out as follows. Section 2 describes the role of the compiler,
and a high-level view of \compiler, a new project at Cornell investigating
database engine runtimes using executors custom compiled for repeated and
standing queries. As part of its scope, \compiler\ will investigate scaling
compiled queries over a distributed main-memory database using a variety of
parallelization and data partitioning techniques customized to the execution
strategy adopted during compilation. Section 3 describes our query compilation
algorithm in terms of a decomposition algorithm that both selects the maps we
maintain during execution, and derives the maintenance tasks for each map. We
also structural query decomposition tecniques for minimizing the memory
consumption of the internal data structures need by our techniques.
Section 4 analyses the memory
requirements of our maps, while Section 5 describes the aforementioned extensions
to the compiler for bulk operations and data structure sharing. Finally prior to
discussing related work and concluding, Section 6 presents experimental results
demonstrating the significant benefits of our techniques over both a standard
relational database, as well as a naive compilation of the query plan as a
straight-line function.






\comment{
We compile queries into C programs. Queries are incrementally maintained.
We are smart about creating data structures for very efficiently keeping query
results up to date. The programming interface supports update triggers as
well as cursor-based iteration over the current query result.

Tasks:
\begin{itemize}
\item
Compile multiple queries into common code, share data structures.

\item
How to compile queries of which some selection conditions can be modified at
runtime? (As in ODBC prepare statements.)

\item
Scale up using data partitioning, parallelization. This is not the focus of
this paper.

\item
Should we say something about accessing secondary storage, or will we leave
this out of scope? If we are not restricted to streaming scenarios, we may
have broader applicability of our techniques.

\item
We have to analyse our main memory consumption.
\end{itemize}


Aim: query processing where the queries are fixed at compile time.
In particular applications where there is no large static database of which
only a small part needs to be processed. The approach works particularly
well if the data arrives on a stream or changes very rapidly

We assume that there is only a single user who performs updates, and that
we do not care about concurrency control.
This is not an OLTP scenario.

Applications:
\begin{itemize}
\item
Processing order books. Give a nice example of a trading strategy that
delegates the bulk of the work to compiled SQL queries.
This is a nice scenario because order books do not get very large (if we
want to process several order books, we can nicely partition the work by
order book across several machines), change extremely rapidly, and the
success of trading depends crucially on the speed with which the programmed
strategies process the data.

\item
Data warehouse loading: data integration and aggregation.
Commercial data warehouse loaders also use straight-line code for aggregation,
but the data integration queries that are executed before aggregation are currently not compiled. Compiling them only yields a really substantial improvement
if the integration and aggregation queries are compiled together. This may substantially reduce the time taken to load a warehouse. (Here we may have to scale up by using several machines.)
\end{itemize}

Claims / theses regarding query processing in main-memory databases (to be verified by experiments):
\begin{enumerate}
\item
When we compile queries to straight-line C code, it is best to use incremental
maintenance techniques to eagerly keep query results up to date.

\item
Index nested loop joins on main memory hash tables
dominate all other join techniques in the compiled
main-memory database setting.
It is worth having hash tables whose buckets are sorted, though (e.g. for
top-k query processing).

\item
There is something to be gained from bulk updates.
do we not just want to support bulk updates of one kind (e.g., just inserts),
but mixtures of inserts, updates, and deletes? Is this an optimization, and
worth the work?

\item
There is no point in adaptivity beyond bulk updates.
\end{enumerate}


Experiments:
Destroy classical databases (4 orders of magnitude improvement). Demonstrate
that incremental maintenance is a good idea for aggregates (two orders
of magnitude improvement over compiled non-incremental code).
}
