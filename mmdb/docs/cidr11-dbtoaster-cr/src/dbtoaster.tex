\label{sec:compilation}

Agile views are database views that are maintained as incrementally as possible.
Despite more than three decades of research into incremental view
maintenance (IVM)
techniques~\cite{roussopoulos-tods:91,griffin-sigmod:95,zhou-icde:07,zhou-vldb:07},
agile views have not been realised, and one of our key contributions in handling
large dynamic datasets is to exploit further opportunities for incremental
computation during maintenance.
Conceptually, current IVM techniques use delta queries for maintenance. Our
observation is that the delta query is itself a relational query that is
amenable to incremental computation. We can materialize delta queries as
auxiliary views, and recursively determine deltas of delta queries
to maintain these auxiliary views.
Furthermore repeated delta transformations successively simplify queries.

\comment{
Conceptually, current IVM techniques
\cite{roussopoulos-tods:91,griffin-sigmod:95,zhou-icde:07,zhou-vldb:07} start
with a view definition query $q$ that operates on a set of base relations
$\{R_1, \ldots, R_n\}$. Using a functional notation of queries, we
write this as $q(\{R_1 \ldots R_n\})$. View maintenance results in a pair
$\tuple{m,\{q'_i\}}$, where $m$ is the materialization of $q$, and $\{q'_i\}$ is
a set of delta queries responsible for maintaining the materialization. Each
delta query $q'_i$ is a SQL query parameterized by an update to relation $R_i$,
and has signature $q'_i(\vec{x}, \{R_j | j \neq i\})$. This describes $q'_i$
as a query taking parameters from tuple $\vec{x}$, and all base relations
except $R_i$ as inputs. For an update tuple $\vec{u}$ on $R_i$, loosely speaking
IVM performs the work: $m \mbox{{\tt+}= } q'_i(\vec{u}, \{R_j | j \neq i\})$.
The delta query $q'_i$ must ensure
$m \equiv q(R_1, \ldots, \{R_i \pm {u}\}, \ldots, R_n)$ after any update to
yield the same result as the view definition query on a modified database.
\todo{Reconsider $R_i$ terminology/symbols, in favour of explaining ``simpler''
in terms of polynomials of unions of joins, where joins are getting simpler.}
}

\subsection{View Maintenance in DBToaster}
We present our observation in more detail and with an example. Given a query $q$
defining a view, IVM yields a pair $\tuple{m,Q'}$, where $m$ is the
materialization of $q$, and $Q'$ is a set of delta queries responsible for
maintaining $m$ (one for each relation used in $q$ that may be updated).
DBToaster makes the following insight regarding IVM: current IVM algorithms
evaluate a delta query entirely from scratch on every update to any relation
in $q$, using standard query processing techniques. DBToaster exploits that a
delta query $q'$ from set $Q'$
can be incrementally computed using the same principles as for
the view query $q$, rather than evaluated in full.

To convey the essence of the concept, IVM takes $q$, produces $\tuple{m,Q'}$
and performs $m \; \mbox{{\tt +}=} \; q'(u)$ at runtime, where $u$ is an update
to a relation $R$ and $q'$ is the delta query for updates to $R$ in $Q'$. We call this one step of \textit{delta (query)
compilation}. This is the extent of query transformations applied by IVM for
incremental processing of updates. DBToaster applies this concept
\textit{recursively}, transforming queries to \textit{higher-level deltas}.
DBToaster starts with $q$, produces $\tuple{m,Q'}$ and then recurs, taking
each $q'$ to produce $\tuple{m', Q''}$ and repeating. Here, each $m'$ is
maintained as $m' \; \mbox{{\tt +}=} \; q''(v)$, where $v$ is also an update,
(possibly) different from $u$ above, and $q''$ is the delta query from $Q''$ for the relation being updated. We refer to $q'$ and $q''$ as first- and second-level
delta queries respectively. We again recur for each $q''$, materialize it as
$m''$, maintain it using third-level queries $Q'''$, and so forth.

\comment{
DBToaster performs recursive delta compilation to compute and
materialize higher-level delta queries. Let us consider higher-level deltas in
terms of the base relations they process by adding relation subscripts. The
delta compilation of $q$ yields $\tuple{m,\{q'_i\}}$, where as above each delta
query is $q'_i(\vec{x}, \{R_j | j \neq i\})$ for $i \in [1,n]$. Notice this
query is simpler than the view definition $q$, it has one fewer input relation
whose attributes have been replaced by parameters in tuple $\vec{x}$.
Recursively applying delta compilation on $q'_i$ yields
$\tuple{m'_i, \{q''_{ij}\}}$ for $j \in [1,n], j \neq i$. Here each second-level
delta has signature $q''_{ij}(\vec{y},\{R_k | k \in [1,n], k \notin \{i,j\}\})$.
Thus the second-level delta $q''_{ij}$ expects an update $\vec{y}$ (on relation
$R_j$), and operates on all base relations except $R_i$ and
$R_j$.~\footnote{Strictly speaking, $q''_{ij}$ may also contain additional
parameters based on the update $u$ from the previous compilation step, as we
will see in our compilation example.}
}

While delta queries are relational queries, they have certain characteristics
that facilitate recursive delta compilation. First, DBToaster delta queries are
parameterized SQL queries (with the same notion of parameter as in, say,
Embedded SQL), with parameter values taken from updates. Thus, in particular,
higher-level deltas are just (parameterized) SQL queries, but are not
{\em higher-order}\/ in the sense of functional programming as some queries in
complex-value query languages are \cite{buneman-kleisli:95}.

To illustrate parameters, we
apply one step of delta compilation on the following query $q$ over
a schema {\tt R(a int, b int), S(b int, c int)}:

$q = \mbox{\tt select sum(a*c) from R natural join S}$

\vspace{1mm}
\noindent For an update $u$ that is an insertion of tuple
$\tuple{\mbox{{\tt @a}}, \mbox{{\tt @b}}}$
into relation $R$, the delta for $q$ is:

\vspace{-6mm}
\begin{align*}
q_R =
\Delta_{u}(q) =
      & \ \mbox{\tt select sum(@a*c) from values(@a,@b), S}\\ 
      & \ \mbox{\tt \ \ where S.b = @b}\\
    = & \ \mbox{\tt @a*(select sum(c) from S where S.b=@b)}
\end{align*}

\vspace{-2mm}
\noindent The {\tt values (...)}
clause is PostgreSQL syntax for a singleton relation defined in the query.
Transforming a query into its delta form for an update $u$ on $R$ introduces
parameters in place of $R$'s attributes. We also apply a rewrite
exploiting distributivity of addition and multiplication to factor out parameter
{\tt @a} from the query.

The second property that is key to making recursive delta processing feasible is
that, for a large class of queries, delta queries are structurally strictly
simpler than the queries that the delta queries are taken off. This can be made
precise as follows. Consider SQL queries that are sum-aggregates over positive
relational algebra. Consider positive relational algebra queries as unions of
select-project-join (SPJ) queries. The {\em degree} of an SPJ query is the
number of relations joined together in it. The degree of a positive relational
algebra query is the maximum of the degrees of its member SPJ queries and the
degree of an aggregate query is the degree of its positive relational algebra
component.
The rationale for such a formalization -- based on viewing queries as
polynomials over relation variables -- is discussed in detail in
\cite{koch-pods:10}. It is proven in that paper that the delta query of a
query of degree $k$ is of degree $\max(k - 1, 0)$. A delta query of degree $0$
only depends on the update but not on the database relations.
So DBToaster guarantees that a \textit{k}-th level delta query $q^{(k)}$ has lower
degree than a \textit{(k-1)}-th level query $q^{(k-1)}$.
Recursive compilation terminates when all conjuncts have degree zero.

Consider the delta query $q_R$ above, which is of degree 1 while $q$ is of
degree 2. Query $q_R$ is simpler than $q$ since it does not contain the relation
$R$. We can further illustrate the point by looking at a recursive compilation
step on $q_R$. The second compilation step materializes $q_R$ as:

\vspace{1mm}
$m_R = \mbox{{\tt select sum(c) from S where S.b=@b}}$

\vspace{1mm}
\noindent omitting the parameter {\tt @a} since it is independent of the above
view definition query. DBToaster is capable of incrementally maintaining $m_R$,
with the following delta query on update
$v$ that is an insertion of tuple $\tuple{\mbox{{\tt @c}},\mbox{{\tt @d}}}$ into relation $S$:

\vspace{1mm}
$q_{RS} = \Delta_v(q_R) = \mbox{\tt select @c from values(@c,@d)}$

\vspace{1mm}
\noindent The delta query $q_{RS}$ above has degree zero since its conjuncts
contain no relations, indeed the query only consists of parameters. Thus
recursive delta compilation terminates after two rounds on query $q$.
In this compilation overview, we have not discussed the maintenance code for
views $m$, $m_R$, and $m_{RS}$ to allow the reader to focus on the core recursive
compilation and termination concepts. We now discuss the data structures used to
represent auxiliary materialized views, and then provide an in-depth example of
delta query compilation including all auxiliary views created and the code
needed to maintain these views.


\def \sql#1{{\scriptsize {\tt #1}}}
\begin{figure*}[htbp]

\vspace{-8mm}

\hspace{-3mm}\begin{tabular}{ll|l|l|ll}
\multicolumn{2}{l}{Input (parent query)}
& Update 
& \multicolumn{3}{l}{Output: auxiliary map, delta query}
\\
\hline
$q =$
& \sql{select l.ordkey, o.sprior,}
& \texttt{+Customer}
& $m[][ordkey,sprior]$
& $q_c =$
& \sql{select l.ordkey, o.sprior,}
\\
& \sql{\ \ \ \ \ \ \ sum(l.extprice) from}
& \texttt{(ck,nm,nk,bal)}
& & & \sql{sum(l.extprice)}
\\
& \sql{Customer c, Orders o, Lineitem l}
& & & & \sql{from Orders o, Lineitem l}
\\
& \sql{where c.custkey = o.custkey}
& & & & \sql{where @ck = o.custkey} 
\\
& \sql{and l.ordkey = o.ordkey}
& & & & \sql{and l.ordkey = o.ordkey}
\\
& \sql{group by l.ordkey, o.sprior;}
& & & & \sql{group by l.ordkey, o.sprior;}
\\
\hline
$q_c$:
& Recursive call,
& \texttt{+Lineitem} 
& $m_c[][custkey,ordkey,sprior]$
& $q_{cl} =$ & \sql{select @ok, o.sprior,@ep*sum(1)}
\\
& see previous output
& \texttt{(ok,ep)} & & & \sql{from Orders o where}
\\
& & & & & \sql{@ck = o.custkey and @ok = o.ordkey}
\\
\hline
$q_{cl}$:
& Recursive call,
& \texttt{+Order} 
& $m_{cl}[][custkey,ordkey,sprior]$
& $q_{clo}=$ & \sql{select @sp, count()}
\\
& see previous output
& \texttt{(ck2,ok2,sp)}
& & & \sql{where @ck = @ck2 and @ok = @ok2;}
\end{tabular}
\caption{Recursive query compilation in DBToaster. For query $q$, we produce a
sequence of materializations and delta queries for maintenance: $\tuple{m,q'},
\tuple{m',q''}, \tuple{m'',q'''}$. This is a partial compilation trace, our
algorithm considers all permutations of updates.}
\label{fig:compex}
\vspace{-4mm}
\end{figure*}

\tinysection{Agile Views}
DBToaster materializes higher-level deltas as agile views for high-frequency
update applications with continuous group-by aggregate query workloads. Agile
views are represented as main memory (associative) map data structures with two
sets of keys (that is a doubly-indexed map $m[\vec{x}][\vec{y}]$), where the
keys can be explained in terms of the delta query defining the map.

As we have mentioned, delta queries are parameterized SQL queries. The first set
of keys (the \textit{input} keys) correspond to the parameters, and the second
set (the \textit{output} keys) to the select-list of the defining query. In the
event that a parameter appears in an equality predicate with a regular
attribute, we omit it from the input keys because we can unify the parameter. We
briefly describe other interesting manipulations of parameterized queries in our
framework in the following section, however a formal description of our
framework is beyond the scope of this paper.

\tinysection{Example}~Figure~\ref{fig:compex} shows the compilation of a query
$q$:

{\footnotesize\begin{verbatim}
select l.ordkey, o.sprior, sum(l.extprice)
from   Customer c, Orders o, Lineitem l
where  c.custkey = o.custkey and l.ordkey = o.ordkey
group by l.ordkey, o.sprior
\end{verbatim}}


\noindent inspired by TPC-H Query 3, with a simplified schema:

\vspace{1mm}
\texttt{Customer(custkey,name,nationkey,acctbal)}

\texttt{Lineitem(ordkey,extprice)}

\texttt{Order(custkey,ordkey,sprior)}

\vspace{1mm}
\noindent 
The first step of delta compilation on $q$ produces a map $m$. The aggregate for
each group $\tuple{ordkey,sprior}$ can be accessed as $m[][ordkey,sprior]$. We
can answer query $q$ by iterating over all entries (groups) in map $m$, and
yielding the associated aggregate value.
The first step also computes a delta query $q_{c}$ by applying standard
delta transformations as defined in existing IVM literature
\cite{griffin-sigmod:95,roussopoulos-tods:91,zhou-icde:07,zhou-vldb:07}. In
summary, these approaches substitute a base relation in a query with the
contents of an update, and rewrite the query. For example, on an insertion to
the {\tt Customer} relation, we can substitute this relation
with an update tuple $\tuple{\mbox{{\tt @ck,@nm,@nk,@bal}}}$:

\begin{verbatim}
select l.ordkey, o.sprior, sum(l.extprice)
from   values (@ck,@nm,@nk,@bal)
         as c(custkey,name,nationkey,acctbal),
       Orders o, Lineitem l
where  c.custkey = o.custkey and l.ordkey = o.ordkey
group by l.ordkey, o.sprior
\end{verbatim}

\noindent Above the substitution replaces the {\tt Customer} relation with a
singleton set consisting of an update tuple with its fields as parameters. We
can simplify $q_{c}$ as:

\def \ql#1{{\tt #1}}
\vspace{1mm}
\begin{tabular}{ll}
$q_c =$  & \ql{select\ \ \ l.ordkey, o.sprior,sum(l.extprice)}\\
        & \ql{from\ \ \ \ Orders o, Lineitem l}\\
        & \ql{where\ \ \ \ @ck = o.custkey}\\
        & \ql{and\ \ \ \ \ \ l.ordkey = o.ordkey}\\
        & \ql{group by l.ordkey, o.sprior;}
\end{tabular}

\vspace{1mm}
The query rewrite replaces instances of attributes with parameters through
variable substitution, as well as more generally (albeit not seen in this
example for simpler exposition of the core concept of recursive delta
compilation), exploiting unification, and distributivity properties of joins and
sum aggregates to factorize queries~\cite{koch-pods:10}.

This completes one step of delta compilation. Our compilation algorithm also
computes deltas to $q$ for insertions to {\tt Order} or {\tt Lineitem} (i.e.
$q_{o}$ and $q_{l}$). We list the full transition program for all insertions at
the end of the example (deletions are symmetric, and ommitted due to space
limitations).
IVM techniques evaluate $q_{c}$ on every insertion to {\tt Customer}. To
illustrate the recursive nature of our technique, we walk through the recursive compilation
of $q_c$ to $m_c,q_{cl}$ on an insertion to {\tt Lineitem} (see the second row
of Figure~\ref{fig:compex}). At this second step, DBToaster materializes $q_c$
with its parameter {\tt @ck} and group-by fields as
$m_c[][custkey,ordkey,sprior]$, and uses this map $m_c$ to maintain the query
view $m$:

{\footnotesize
\begin{verbatim}
on_insert_customer(ck,nm,nk,bal):
  m[][ordkey,sprior] += m_c[][ck,ordkey,sprior];
\end{verbatim}
}

\noindent
As it turns out, all maps instantiated from simple equijoin aggregate queries
such as TPCH Query 3 have no input keys. Maps with input keys only occur as a
result of inequality predicates and correlated subqueries, for example the VWAP
query from Section~\ref{sec:intro}.

Above, we have a trigger statement in a C-style language firing on insertions to
the {\tt Customer} relation, describing the maintainence of $m$ by reading the
entry $m_c[\mbox{{\tt ck}},ordkey,sprior]$ instead of evaluating $q_c(\mbox{{\tt
ck}}, Orders, Lineitem)$. Notice that the trigger arguments do not contain
$ordkey$ or $sprior$, so where are these variables defined? In DBToaster, this
statement implicitly performs an iteration over the domain of the map being
updated. That is, map $m$ is updated by looping over all $\tuple{ordkey,sprior}$
entries in its domain, invoking lookups on $m_c$ for each entry and the trigger
argument {\tt ck}. Map read and write locations are often (and for a large class
of queries, always) in one-to-one correspondence, allowing for an embarrassingly
parallel implementation (see Section~\ref{sec:distribution}). For clarity, the
verbose form of such statement is:

\begin{verbatim}
on_insert_customer(ck,nm,nk,bal):
  for each ordkey,sprior in m:
    m[][ordkey,sprior] += m_c[][ck,ordkey,sprior];
\end{verbatim}

\noindent Throughout this document we use the implicit loop form.
Furthermore, this statement is never implemented as a loop, but relies on a map
data structure supporting partial key access, or \textit{slicing}. This is
trivially implemented with secondary indexes for each partial access present in
any maintenance statement, in this case a secondary index yielding all
$\tuple{ordkey,sprior}$ pairs for a given \texttt{ck}.
This form of maintenance statement is similar in structure to the concept of
\textit{marginalization} in probability distributions, essentially the map $m$
is a marginalization of map $m_c$ over the attribute \texttt{ck}, for each
\texttt{ck} seen on the update stream.

Returning to the delta $q_{cl}$ produced by the second step of compilation, we
show its derivation and simplification below: 

\vspace{1mm}
\hspace{-5mm}
\begin{tabular}{lcl}
\begin{minipage}{1.5in}
\begin{verbatim}
select l.ordkey, o.sprior,
       sum(l.extprice)
from  Orders o, values
      (@ok,@ep) as
      l(ordkey,extprice)
where @ck = o.custkey
and   l.ordkey = o.ordkey
\end{verbatim}
\end{minipage}
&
{\tt =>}
&
\hspace{-2mm}
\begin{minipage}{1.3in}
\begin{verbatim}
select @ok, o.sprior,
       @ep*sum(1)
from  Orders o
where @ck = o.custkey
and   @ok = o.ordkey
\end{verbatim}
\end{minipage}
\end{tabular}

\vspace{1mm}
Notice that $q_{cl}$ has a parameter {\tt @ck} in addition to the substituted
relation {\tt Lineitem}. This parameter originates from the attribute
{\tt c.custkey} in $q$, highlighting that map parameters can be passed
through multiple levels of compilation.
The delta $q_{cl}$ is used to maintain the map $m_c$ on
insertions to {\tt Lineitem}, and is materialized in the third step of
compilation as $m_{cl}[][custkey,ordkey,sprior]$. The resulting maintenance code
for $m_{c}$ is (corresponding to Line 8 of the full listing):

{\footnotesize
\begin{verbatim}
on_insert_lineitem(ok,ep) :
  m_c[][custkey, ok, sprior] +=
    ep * m_cl[][custkey, ok, sprior];
\end{verbatim}
}

Above, we iterate over each $\tuple{custkey,sprior}$ pair in the map $m_c$, for
the given value of the trigger argument {\tt ok}. Note we have another slice
access of $m_{cl}$, of $\tuple{custkey,sprior}$ pairs for a given \texttt{ok}.
The third step of recursion on insertion to {\tt Order} is the terminal step, as
can be seen on inspection of the delta query $q_{clo}$:

\vspace{1mm}
\hspace{-5mm}
\begin{tabular}{lcl}
\begin{minipage}{1.68in}
\begin{verbatim}
select o.sprior, count()
from values (@ck2,@ok2,@sp)
as o(custkey,ordkey,sprior)
where @ck = o.custkey
and   @ok = o.ordkey
\end{verbatim}
\end{minipage}
&
{\tt =>}
&
\hspace{-3mm}
\begin{minipage}{1.3in}
\begin{verbatim}
select @sp, count()
where @ck = @ck2
and   @ok = @ok2
\end{verbatim}
\end{minipage}
\end{tabular}

\vspace{1mm}
In the result of the simplification, the delta $q_{clo}$ does not depend on the
database since it contains no relations, only parameters. Thus the map $m_{cl}$
can be maintained entirely in terms of trigger arguments and map keys alone.
Note this delta contains parameter equalities. These predicates constrain
iterations over map domains, for example the maintenance code for $q_{clo}$
would be rewritten as:

\vspace{1mm}
\begin{tabular}{lcl}
\hspace{-6mm}
\begin{minipage}{1.65in}
{\footnotesize
\begin{verbatim}
on_insert_order(ck2,ok2,sp) :
  m_cl[][ck, ok, sp] +=
    if ck==ck2 && ok==ok2
    then 1 else 0;
\end{verbatim}
}
\end{minipage}
&
{\tt =>}
&
\hspace{-3.5mm}
\begin{minipage}{1.6in}
{\footnotesize
\begin{verbatim}
m_cl[][ck2, ok2, sp]
  += 1;
\end{verbatim}
}
\end{minipage}
\end{tabular}

\vspace{1mm}
\noindent where, rather than looping over map $m_{cl}$'s domain and testing the
predicates, we only update the map entry corresponding to {\tt ck2}, {\tt ok2}
from the trigger arguments.

We show the trigger functions generated by DBToaster below, for all possible
insertion orderings, including for example deltas of $q$ on insertions to {\tt
Order} and then {\tt Lineitem}.  We express the path taken as part of the map
name as seen for $m_{c}$ and $m_{cl}$ in our walkthrough. Some paths produce
maps based on equivalent queries; DBToaster detects these and reuses the same
map.  Due to limited space, we omit the case for deletions, noting that these
are symmetric to insertions except that they decrement counts.


{\footnotesize
\begin{verbatim}
1.  on_insert_customer(ck,nm,nk,bal) :
2.    m[][ordkey, sprior] +=
3.                     m_c[][ck, ordkey, sprior];
4.    m_l[][ordkey, sprior] +=
5.                    m_cl[][ck, ordkey, sprior];
6.    m_o[][ck] += 1;
7. 
8.  on_insert_lineitem(ok,ep) :
9.    m[][ok, sprior] += ep *  m_l[][ok, sprior];
10.   m_c[][custkey, ok, sprior] +=
11.            ep *  m_cl[][custkey, ok, sprior];
12.   m_co[][ok] += ep;
13.
14. on_insert_order(ck,ok,sp) :
15.   m[][ok, sp]        += m_co[][ok] * m_o[][ck]; 
16.   m_l[][ok, sp]      +=  m_o[][ck];
17.   m_c[][ck, ok, sp]  += m_co[][ok];
18.   m_cl[][ck, ok, sp] += 1;
\end{verbatim}
}

\noindent We briefly comment on one powerful transformation that is worth
emphasizing in the above program, as seen on line 15. Notice that the
right-hand side of the statement consists of two maps -- all other statements
are dependent on a single map. This line is an example of {\em factorization}
applied as part of simplification. This statement is derived from the query $q$
given at the start of the example, when considering an insertion to the {\tt
Order} relation with tuple $\tuple{\mbox{{\tt @ck,@ok,@sp}}}$:

\vspace{1mm}
\begin{tabular}{ll}
$q_o =$  & \ql{select\ \ \ @ok,@sp,sum(l.extprice)}\\
         & \ql{from\ \ \ \ \ Customers c, Lineitem l}\\
         & \ql{where\ \ \ \ c.custkey = @ck}\\
         & \ql{and\ \ \ \ \ \ l.ordkey = @ok;}
\end{tabular}

\vspace{1mm}
\noindent The group-by clause of $q$ can be eliminated since all group-by
attributes are parameters. Note that the two relations {\tt Customer}, and {\tt
Lineitem} have no join predicate, thus the query uses a cross product. By
applying distributivity of the sum aggregate, we can separate (factorize) the
above query into two scalar subqueries:

\vspace{1.5mm}
\hspace{-5mm}
\begin{tabular}{lcl}
\begin{minipage}{1.2in}
\begin{verbatim}
select @ok,@sp,
  sum(l.extprice)
from
  Customers c,
  Lineitem l
where c.custkey = @ck
and l.ordkey = @ok;
\end{verbatim}
\end{minipage}
&
{\tt =>}
&
\hspace{-2mm}
\begin{minipage}{1.3in}
\begin{verbatim}
select @ok,@sp,
((select sum(l.extprice)
  from   Lineitem l
  where  l.ordkey = @ok)
 *
 (select sum(1)
  from   Customers c
  where  c.custkey = @ck));
\end{verbatim}
\end{minipage}
\end{tabular}

\vspace{1.5mm}
DBToaster materializes the scalar subqueries above as $m_co$ and $m_o$ in its
program, and in particular note that prior to factorization we had a single
delta query of degree 2, and after factorization we have two delta queries of 1.
The latter is clearly simpler and more efficient to maintain.
Factorization can be cast as the generalized distributive law
(GDL)~\cite{aji-toit:00} applied to query processing. GDL facilitates fast
algorithms for many applications including belief propagation and message
passing algorithms, and Viterbi's algorithm. With this analogy, we hope to
leverage other techniques from this field, for example approximation techniques.

\tinysection{Transition program properties}
For many queries, compilation yields \textit{simple} code that has no joins
and no nested loops, only single-level loops that perform probing as
in hash joins.
\comment{
The transition program also exploits distributivity to push down aggregates
through joins, yielding small map sizes since maps maintain group-by aggregates.
}
Simple code is beneficial for analysis and optimizations in machine
compilation and code generation.

Transition programs leverage more space to trade off time by materializing delta
queries. These space requirements are dependent on the active domain sizes of
attributes, and often attributes do not have many distinct values, for example
there are roughly 2800 unique stock ids on NASDAQ and NYSE. Additionally pruning
duplicate maps during compilation facilitates much reuse of maps given recursion
through all permutations of updates. Finally, there are numerous opportunities
to vary space-time requirements for transitions: we need not materialize all
higher-level deltas. For example we could maintain $q$ with $m^{(i)}$, a
materialized $i$-th level delta and perform more work during the update to
evaluate $q^{(i-1)}, \ldots, q^{(1)}$. We could further amortize space by
exploiting commonality across multiple queries, merging maps to service multiple
delta queries.



\tinysection{Insights}
\comment{
We highlight a few insights drawn from our experience in
several iterations of algorithm design. Compiling the transition function for
view maintenance is advantageous and feasible (in that it terminates) due to the
property that higher-level delta queries successively get simpler and simpler.
The terminal delta consists of parameters alone, and does not depend on the
database.
}
Queries are closed under taking deltas, that is, a delta query is of the same
language as the parent query. This allows for processing delta queries using
classical relational engines in IVM.
However, the aggressive compilation scheme presented above allows to innovate in
the design of main-memory query processors.
In the above example, we have materialized all
deltas, thus the transition program consists of simple arithmetics on parameters
and map lookups.

Our concept of higher-level deltas draws natural analogies to
mathematics. Our framework, and the compilation algorithm described here
-- but restricted to a smaller class of queries without nested aggregates
-- was described and proven correct in~\cite{koch-pods:10}.
In this framework, queries are based on polynomials in an
algebraic structure -- a {\em polynomial ring} --
of generalized relational databases.
This quite directly yields the two main properties that make recursive
compilation feasible -- that the query language is closed under taking deltas
and that taking the delta of a query reduces its degree, assuring termination of
recursive compilation.
Unfortunately, the second property is not preserved if one extends the
framework of \cite{koch-pods:10} by nested aggregates. We describe below
how DBToaster handles this generalized scenario.

\tinysection{Discussion}
To summarize, in contrast to today's IVM, DBToaster uses materialization of
higher-level deltas for continuous query evaluation that is \textit{as
incremental as possible}. DBToaster is capable of handling a wide range of
queries, including, as discussed next, nested queries. This has not been
addressed in the IVM literature, and lets our technique cover complex, composed
queries, where being as incremental as possible is highly advantageous.



\subsection{Compilation Enhancements}
\noindent We briefly discuss further compilation issues and optimizations beyond
the fairly simple query seen in Figure~\ref{fig:compex}.


\comment{
\tinysection{Deletions}
Our example above only mentions inserts, but handling deletions turns out to be
straightforward. Our framework represents both the database and queries entirely
as maps\footnote{We have toyed with calling our system a MapStore, but prefer a
name with mystique, DBToaster.}, including base relations. Base relations are
multisets, or, maps with keys according to the relation's schema and values from
tuple's cardinalities, e.g. a tuple $\tuple{a=3,b=5}$ occurring three times in
relation $R(a,b)$ is a map entry $m[3,5] \mapsto 3$. Deletions are simply tuples
with negative multiplicities, and are implemented in their own transition
function.

\tinysection{Simplifying Delta Queries}
Transition compilation internally uses several techniques to simplify queries
as we are taking deltas. We present two of these here, starting with
unification leading to variable elimination. Consider the SQL query:
\begin{verbatim}
select sum(l.extendedprice) from partsupp ps, lineitem l
where @pk = ps.partkey and ps.partkey = l.partkey
      and ps.suppkey = l.suppkey and l.quantity < 10
\end{verbatim}
By unifying the parameter {\tt @pk}, \todo{finish, needs a better example\ldots} 

The second simplification is factorization. Consider the query:
\texttt{select sum(c.acctbal*(l.extprice-l.discount))
from customer c, lineitem l}.
We can write this as a product of two separate, simpler, aggregates:
\begin{verbatim}
(select sum(c.acctbal) from customer c) * 
(select sum(l.extprice-l.discount) from lineitem l)
\end{verbatim}

\noindent and independently compute deltas for each aggregate. Factorization is
frequently possible when considering star schemas in analytics applications, and
generalizes to structural decompositions, such as hypertree decomposition
\todo{[REF Gottlob]}. While there has been much work in decomposing join
hypergraphs, structurally decomposing aggregate queries would be interesting
future work.
}

\tinysection{Nested queries}
We can compile transitions for nested queries, which has not been feasible in
existing IVM techniques. In particular nested scalar subqueries used in
predicates are problematic because taking deltas of such predicates does not
result in simpler expressions. Our algorithm would not terminate if we did not
handle this: we explicitly find simpler terms and recur on them. VWAP
in Section 1 exemplifies a nested query.

Nested subqueries contain correlated attributes (e.g. price in VWAP)
defined in an outer scope. We consider correlated attributes as parameters, or,
internally in our framework, as binding patterns as seen in data integration.
Nested queries induce \textit{binding propagation}, similar to sideways
information passing in Datalog. That is, we support the results of one query
being used (or \textit{propagated}) as the parameters of a correlated subquery,
indicating an evaluation ordering.
We transform queries to use minimal propagation, which performs additional
aggregation of maps, over dimensions of the map key that are not propagated. For
example a map $m[x,y,z]$ would be aggregated (\textit{marginalized}) to
$m'[x,y]$ if $x,y$ were the only correlated attributes.



\comment{
\tinysection{Initial values}

\tinysection{Windows, Constraints, Other Aggregates}
\begin{itemize}
  \item Do you really want to include this section? What do you have to say
  that's strong here \note{i.e. what can you say about compiling to specific
  data structures beyond maps?}
  \item Our model of transitions generally addresses the issues of incremental
  processing and can handle stream processing features such as windows.
  \item In fact we can exploit additional schema information such as
  key/foreign-key relationsips, integrity constraints and so forth to further
  simplify our programs. \note{Give simple example with an integrity
  constraint, i.e. we can avoid generating a statement if we know an entry
  cannot exist in another relation due to integrity.}
  \item Min/max handling. The hard case is deletion, since we have to recompute
  from scratch. Our compilation framework uses a query representation where
  everything is a map, including base tables. Thus we can easily express
  materializing relations in addition to group-by aggregates, requiring no
  change to handle other aggregates. This also applies to queries without
  aggregation, for example a SPJ query. \footnote{We have toyed with calling our
  system a MapStore, but prefer a name with mystique, DBToaster.}
\end{itemize}
}


\tinysection{Rethinking query compilation with programming languages and
compiler techniques}
Our current compilation process involves implementing transition programs in a
variety of target languages, including OCaml and C++. We currently rely on OCaml
and C++ compilers to generate machine code, and observe that there are a wide
variety of optimization techniques used by the programming languages (PL) and
compiler communities that could be applied to query compilation. Compiling
queries to machine code is not a novel technique, and has been applied since the
days of System R~\cite{chamberlin-tods:81}. However there have been many
advances in source code optimization since then, as evidenced by several
research projects aimed in this
direction~\cite{arumugam-sigmod:10,krikellas-icde:10}.


\comment{
To better capture the evaluation of queries with parameters, binding
patterns and propagation, we are designing a main-memory query processor based on
functional programming (FP) primitives and optimizations.
}
We believe the advantage of incorporating methods from the PL and compiler
communities directly into our compiler framework is that it facilitates
whole-query optimizations, programmatic representation and manipulation of
physical aspects of query plans such as pipelining and materialization
(memoization), and opportunities to consider the interaction of query processing
and storage layouts via data structure representations.
Specifically, we have developed a small functional programming (FP) language as
our abstraction of a physical query plan, unlike the operator-centric low-level
physical plans found in modern database engines (e.g. specific join
implementations, scans, sorting operators that make up LOLEPOPs in IBM's
Starburst and DB2~\cite{mcpherson-debull:87}, and similar concepts in Oracle, MS
SQL Server amongst others).

\comment{
Modern relational engines, and their operator and Volcano-style
iterator abstractions, lack a clear, flexible representation of many aspects of query
processor design choices, particularly those relating to entire query plans
resulting from composing operators, for example the degree of pipelining to use
(or alternatively blocking and materializing intermediates), and its impact on
end-to-end performance metrics such as query processing throughput and latency.
This has tended to result in query engine codebases that are rigid, and
ultimately require significant re-engineering whenever architectural changes are
desired (as evidenced in ground-up stream and column-store engine construction).

We believe that employing an expressive low-level plan representation will let
us capture, and critically programmtically manipulate, many of these whole-query
(whole-program) evaluation decisions, which are currently deeply embedded into
query engines. We hope to leverage the significant body of work on program
analyses and optimization in the programming languages and compiler communities
to aggressively optimize a rich physical plan representation. To this end, we
briefly discuss two aspects of whole-query evaluation below, where our FP
framework is capable of representing non-traditional evaluation strategies:
i) pipelining and intermediate result materialization, that is the issue of
tuple- vs. set-at-a-time processing and its role in the space of join-tree
plans; ii) the use of non-first normal-form datastructures, as seen with nested
relational algebras, and its relevance to vectorized processing.
}

The primary features of our FP language are its use of nested collections (such
as sets, bags and lists) during query processing, its ability to perform
structural recursion (SR)~\cite{buneman-kleisli:95} optimizations on nested
collections, and its support for long-lived persistent collections. Structural
recursion transformations enable the elimination of intermediates when
manipulating collections, and when combined with primitive operations on
functions, such as function composition, yields the ability to adapt the
granularity of data processing. Consider a join-aggregate query
$\sum_{a*f}((R \bowtie S) \bowtie T)$ with schema
$R(a,b)$, $S(c,d)$, $T(e,f)$, where the natural joins are Cartesian
products. While such a query would not occur as a delta query in DBToaster
(it would be factorized as discussed above), it suffices to serve as a
toy example. Our functional representation is:

\begin{verbatim}
aggregate(fun < <t,u,v,w,x,y>, z>. (t*y)+z, 0,
  flatten(
    map(fun <w,x,y,z>.
          map(fun <e,f>.<w,x,y,z,e,f>, T),
        flatten(
          map(fun <a,b>.
                map(fun <c,d>.<a,b,c,d>, S),
              R)))))
\end{verbatim}

\noindent Above, {\tt fun} is a lambda form, which defines a function possibly
with multiple arguments as indicated by the tuple notation. Next, {\tt map}, and
{\tt aggregate} are the standard functional programming primitives that apply a
function to each member of a collection, and fold or reduce a collection
respectively. For example, we can use {\tt map} to add a constant to every
element of a list as:

\vspace{-2mm}
\begin{verbatim}
map(fun x. x+5, [10;20;30;40]) => [15;25;35;45]
\end{verbatim}

\vspace{-2mm}
\noindent Similarly we can use aggregate to compute the sum of all elements of a
list, with initial value 0 as:

\vspace{-2mm}
\begin{verbatim}
aggregate(fun <x,y>. x+y, 0, [10;20;30;40]) => 100
\end{verbatim}

\vspace{-2mm}
\noindent The {\tt flatten} primitive applies to a
collection of collections (i.e. a nested collection), yielding a single-level
collection. For example:

\vspace{-2mm}
\begin{verbatim}
flatten([[1;2]; [3]; [4;5;6]]) = [1;2;3;4;5;6]
\end{verbatim}

\vspace{-1mm}
\noindent Our functional representation first joins relations R and S, before
passing the temporary relation created to be joined with T, again yielding an
intermediate, that is finally aggregated. Notice the intermediate flatten
operations to yield a first normal form.

A standard implementation of a join as a binary operation forces the
materialization of the intermediate result $R \bowtie_{\theta} S$. There are
numerous scenarios where such materialization is undesirable, and has led to the
development of multiway join algorithms such as XJoin~\cite{urhan-debull:00} and
MJoin~\cite{viglas-vldb:03}. With a few simple transformation steps, we can
rewrite the above program to avoid this intermediate materialization as:

\begin{verbatim}
aggregate(fun < <t,u,v,w,x,y>, z>. (t*y)+z, 0,
  flatten(flatten(
    map(fun <a,b>. map(fun <c,d>. map(fun <e,f>.
      <a,b,c,d,e,f>, T), S), R))))
\end{verbatim}

This is a three-way nested loops join $\bowtie_3(R,S,T)$ with no intermediate
materialization of a two-way join, that can also be pipelined into the
aggregation. In general, we can apply well-established folding,
defunctionalization~\cite{danvy-ppdp:01} and deforestation~\cite{marlow-fp:92}
techniques from functional programming, which when combined with data structures
such as tree-based indexes and hash tables, can yield a rich space of evaluation
strategies that vary in their pipelining, materialization, ordering, nesting
strucure and vectorization characteristics.

The last item, nesting structure and vectorization, refers to concepts from
nested relational algebra~\cite{schek-infsys:86}, whereby query processing need
not occur in terms of first normal form relations. Our programs can use
non-first normal forms internally, and can apply compression and vectorized
processing over the nested relation attributes, much in the vein of
column-oriented processing. Furthermore, we can directly represent the
aforementioned data structures, such as indexes and DBToaster maps, in our
language, yielding a unified approach to representing both query processing and
storage layout. We know of no existing framework capable of such a rich
representation, and are excited by the potential to apply program
transformations to jointly optimize query processing and storage.


\comment{
\tinysection{Towards optimizing engine physicality}
We are adopting a functional programming (FP) approach to compilation to better
capture the need for binding patterns in delta queries, and to apply
optimizations from FP, such as exploiting structural recursion
(SR)~\cite{buneman-kleisli:95}. SR enable optimizations of arbitrarily nested
collections such as sets, bags and lists. We can use SR optimizations to
manipulate \textit{physical-level properties}, such as tuple construction and
pipelining.

Delta queries consist of cross products and joins of many maps, due to
factorization of, and binding propagation in queries. In a traditional DBMS,
cross products and joins consume and produce relations, ``flattening''
intermediate results into rows. Column-stores have argued for late tuple
construction~\cite{abadi-icde:07}. We produce intermediate results in nested
form (cf. nested relations and objects), potentially nested deeply after
combining several maps. With SR and function composition, we can
programmatically control where, and how much tuple construction we perform.

We can also entirely avoid constructing \textit{any} intermediate results, let
alone flattening intermediates. This enables \textit{automatic} transformation
of a join-tree into an n-way join, which, for example on a series of cross
products, yields an n-level pipelined nested loops implementation that avoids
expensive intermediates, leading to a much richer space of join plans. DBToaster
would be the first engine to incorporate automated, programmatic manipulation of
physical aspects of plans, since we reason about processing with data
structures, not an operator abstraction that hides and encapsulates data
structures. Our techniques will be equally applicable to main-memory DBMS due to
closed delta queries.
}
