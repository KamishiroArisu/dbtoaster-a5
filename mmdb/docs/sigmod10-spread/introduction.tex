\section{Introduction}


Recent years have seen the beginning of a paradigm shift in data management
research from incrementally
improving decades-old database technology
%
% (particularly, OLTP) -- System R and
% the long line of systems that have followed it --
%
to questioning established
architectures and creating fundamentally different, more lightweight systems
that are often domain-specific
(cf.\ e.g. \cite{DBLP:conf/vldb/StonebrakerMAHHH07,DBLP:journals/pvldb/KallmanKNPRZJMSZHA08}).
Part of the impetus for this change was given by 
potential users such as scientists and the builders of
large-scale Web sites and services such as Google, Amazon, and Ebay,
who have a need for data management systems but have found current databases
not to scale to their needs.
One can observe a trend to disregard database contributions
in these communities \cite{dbcolumn, DBLP:conf/sigmod/PavloPRADMS09}, and to build lightweight systems based on
robust technologies mostly pioneered by the operating systems and distributed
systems communities, such as large scale file systems, key-value stores, and
map-reduce
\cite{DBLP:journals/cacm/DeanG08, DBLP:journals/tocs/ChangDGHWBCFG08}.
Further impetus has resulted from the current need to develop data management
technology for multicore and cloud computing.
%
%, and by the example given by a
%number of innovative recent database startup companies that develop
%databases based on new architectures.
%\footnote{Examples are Vertica and Paraccel, who build column stores,
%and Greenplum, Asterdata, and Netezza, who take databases into the Cloud.}

There is a recent tendency among pundits outside the database community to
contest the need for powerful queries, and to
think of key-value stores -- with only the power to look up data by
keys -- as (much more efficient) database query engines.
%
%It shall not be denied that,
%with clever engineering, a surprising range of problems can be solved
%using key-value stores.
%
However, expressive query languages such as SQL do not cease to have
important applications and a substantial user base.
Alas, we do not know how to process SQL queries on updateable data
using a system as lightweight as a key-value store.

This paper contributes a fundamental and versatile building block for
enabling new, more lightweight and nimble data processing systems based
on SQL aggregation que\-ries. We believe that our contribution
constitutes an important
step towards achieving the contradiction in terms mentioned
above: executing complex aggregation queries on updateable data
using little more than a key-value store.

At the heart of our approach is a new aggressive recursive incremental
view maintenance mechanism.
In most traditional database query processors, the  basic building blocks of
queries are large-grained operators such as joins.
Our approach is based on compilation, reducing
queries to programs that are not based on classical query operators.
A large class of
SQL aggregation queries can be compiled down to very simple message
passing programs that incrementally maintain materialized views of
the queries. These message passing programs keep a hierarchy of map data
structures (which may be served out of a key-value store) up to date
and can share computation in the case that multiple aggregation queries (e.g.,
a data cube) need to be maintained.  Most importantly, though, these message
passing programs can be massively parallelized to the degree that the updating
of each single result aggregate value can be done in constant time on normal
off-the-shelf computers.\footnote{That is,
we assume that addition and multiplication
of {\em two numbers} can be performed in constant time, which is true for
%
%bounded precision numbers such as 
%
standard base types such as int and float,
but we assume no unrealistic models such
as aggregators with unbounded fan-in, as used in some theoretical models of
parallel computation. Our implementation indeed incrementally maintains
individual aggregate values in constant time. Note that since there are
usually many more aggregate values to maintain than there are processors,
this does not mean that each update is processed in constant time.
``Constant time'' is with respect to the size of the data, not the compiled
query.}
To the best of our knowledge, it was not known before
that this is possible.

\begin{figure*}[!]
\begin{center}
\begin{tabular}{c|rc|rc|rc|rc}
$\Delta$Customer
& $\Delta$q[1] & q[1]
& $\Delta$q[2] & q[2]
& $\Delta$q[3] & q[3]
& $\Delta$q[4] & q[4] \\
\hline
insert (1,US) &         +1 & 1 &              &   &              &   &&   \\
insert (2,UK) &            & 1 &            1 & 1 &              &   &&   \\
insert (3,UK) &            & 1 &   +q2[2,UK] & 2 & +q1[UK] + 1 & 2 &&   \\
insert (4,US) & +q2[4,US] & 2 &              & 2 &              & 2 & +q1[US] + 1 & 2 \\
delete (3,UK) &            & 2 & $-$q2[3,UK] & 1 & $\underbrace{\mbox{$-$q1[UK]}}_{-2}$ $\underbrace{\mbox{$-$ q2[3,UK]}}_{-1}$ + 1 & 0 && 2 \\
insert (3,{\bf US}) & +q2[3,US] & 3 &   +q2[3,US] & 3 & +q1[US] + 1 & 3 & +q2[3,US] & 3  \\
\end{tabular}
\end{center}

\vspace{-4mm}

\caption{Runtime trace of the M3 program of Example~\ref{ex:self-join}.}
\label{fig:trace}
\end{figure*}


In comparison, no such constant-time parallel processing technique
is known for nonincremental query
evaluation: Indeed, it is unlikely to exist.\footnote{Constant-time
bounded fan-in nonincremental
parallel processing is known not to be possible for
the class of queries we address,
unless the complexity class TC0 collapses into NC0, which it is not known
to do \cite{Joh90}.} Classical incremental view maintenance approaches, which
express the delta (=change) to a query result given an update again
as a (slightly simpler) query, fare no better: Generally,
given a query, there is another query whose delta is the first query.
Thus, classical incremental view maintenance has the same limits to
parallelization as nonincremental evaluation.


\subsection{Message passing programs}


We introduce {\em map maintenance message}\/ (M3) programs and
compile SQL aggregation queries to M3.
All state including query results are represented by map data
structures. 
M3 programs consist of insert and delete triggers -- sequences
of {\em map update statements}\/ of the form
\begin{equation}
\mbox{{\tt $m[\vec{x}]$ $\pm$= $t$}}
\label{eq:nonforeach}
\end{equation}
or
\begin{equation}
\mbox{{\tt foreach $\vec{y}$ do $m[\vec{x}\vec{y}]$ $\pm$= $t$}},
\label{eq:foreach}
\end{equation}
where $\vec{x}$ are arguments of the trigger,
$m$ is a map name, and $t$ is essentially a $C$ or Java rvalue expression.
In a statement of form (\ref{eq:nonforeach}),
when we increment a field of $m[\vec{x}]$
that is undefined (i.e., $\vec{x}$ is not in the domain of $m$),
we use zero as the default value for $m[\vec{x}]$.\footnote{This is correct
for the examples of this section; the general case will be studied later.}
In a statement of the form (\ref{eq:foreach}),
$m[\vec{x}\vec{y}]$ is updated for all
values $\vec{y}$ such that $\vec{x}\vec{y}$ is currently in the
domain of $m$.


\begin{example}\em
\label{ex:self-join}
Given a relation Customer(\underline{cid}, nation).
We ask, for each customer id (cid),
for the number of customers of the same nation (including the customer
identified by cid).
The SQL query is
\begin{verbatim}
SELECT   C1.cid, SUM(1)
FROM     Customer C1, Supplier C2
WHERE    C1.nation = C2.nation
GROUP BY C1.cid;
\end{verbatim}
Our compiler produces the following M3 on-insert trigger:
\begin{verbatim}
on insert into Customer (cid, nation) {
  q[cid] += q1[nation];
  foreach cid2 do q[cid2] += q2[cid2, nation];
  q[cid] += 1;
  q1[nation] += 1;
  q2[cid, nation] += 1
}
\end{verbatim}
The on-delete trigger is just like the on-insert trigger with {\tt +=}
changed to {\tt -=} everywhere other than in the third statement
({\tt q[cid] += 1}), which remains unchanged.
We will establish later that this M3 program is indeed correct.
Figure~\ref{fig:trace} shows a trace of the result map $q$ as we perform
a sequence of insertions and deletions. The $\Delta q[i]$ columns
indicate the change made to $q[i]$ on this update.\footnote{We
challenge the reader to find a 
fundamentally different (ideally, simpler) way to
perform the incremental maintenance of the query of this example
which has the M3 property of embarassing parallelism, with each value
to be updated only requiring a constant amount of work.}
\punto
\end{example}


{\em Parallelization}.
The syntax of statements of the form (\ref{eq:foreach})
is misleading in that it suggests a loop --
that a nonconstant amount of work is needed to bring aggregate
values up to date. Polynomial amounts of work are in fact needed,
but only because in general there are many aggregate values
to be maintained -- in a
map representing the result of a group-by query or in an auxiliary map.
In fact, each statement of form (\ref{eq:foreach})
writes each value $m[\vec{x}\vec{y}]$ only once and admits
{\em embarassing parallelism}: $m$ can be partitioned across many machines
that share the work. A map update statement specifies a
pattern of {\em message passing} between nodes reading and nodes writing
map values.

\nop{
Assume that the storage of individual maps
is partitioned across several machines. To execute a statement of form
(\ref{eq:foreach})
in a trigger invocation with arguments $\vec{x} = \vec{a}$,
where term $t$ uses map lookups $m_i[\vec{x}_i, \vec{y}_i]$,
each node storing a value $m_i[\vec{a}_i, \vec{y}_i] = v$,
for $\vec{y}_i$ arbitrary, sends the message
$m_i[\vec{a}_i, \vec{y}_i] = v$ to the node managing value
$m[\vec{a}, \vec{y}]$. This way, that node receives all the values it
needs to update all $m$ values that it represents.
Of course this requires a suitable protocol to ensure overall consistency
and that the right versions of map values are read and written in the right
order.
} % end nop


\begin{example}[star-join decomposition]\em
\label{ex:ssb}
Con\-sider \\ a simplified version of the star schema
benchmark (SSB) schema with relations Date(\underline{datekey}, year),
Part(\underline{partkey}, partcat), where partcat stands for a part category,
and LineOrder(datekey, partkey, revenue), which may contain duplicate tuples.
The query asks for the total revenues grouped by year and part category.

\begin{verbatim}
SELECT   P.partcat, D.year, SUM(revenue)
FROM     Date D, Part P, LineOrder L
WHERE    D.datekey=L.datekey
AND      P.partkey=L.partkey
GROUP BY P.partcat, D.year;

on insert into Date (datekey, year) {
  mPL[datekey, year] += 1
}
on insert into Part (partkey, partcat) {
  mDL[partkey, partcat] += 1
}
on insert into LineOrder (datekey, partkey, revenue) {
  foreach (partcat, year) do
  m[partcat, year] += revenue
                    * mDL[partkey, partcat]
                    * mPL[datekey, year]
}
\end{verbatim}
The delete-triggers are precisely
like the insert-triggers, but with {\tt +=} replaced by {\tt -=}.
We assume here that there are no cascading deletes.

The maps {\tt mPL} and {\tt mDL} have value at most
one at each position because datekey and partkey are keys for Date and Part,
respectively.

Observe how the {\tt on insert into LineOrder} code for incrementally
maintaining the query result {\tt m} decomposes into
two parts with disjoint variables, 
{\tt mDL[partkey, partcat]} and {\tt mPL[datekey, year]}.
Given values for datekey and partkey, we instruct nodes
to send their {\tt mDL[partkey, x]} and {\tt mPL[datekey, y]} values,
for any {\tt x} and {\tt y},
to nodes maintaining {\tt m[x, *]} and {\tt m[*, y]}, respectively.
A node managing {\tt m[u, v]} receives, possibly from distinct nodes,
{\tt mDL[partkey, u]} and {\tt mPL[datekey, v]}
and can increment {\tt m[u, v]} by
{\tt revenue*mDL[partkey, u]*mPL[datekey, v]}.
%
%We only send nonzero {\tt mDL[partkey, x]} and {\tt mPL[datekey, y]} values,
%but at least an empty message so that the node managing m knows that it does
%not have to wait for anything more.
\punto
\end{example}


It shall be emphasized that for each of the examples of this section,
and the paper as a whole, our compilation approach produces exactly
the M3 programs shown.


The fragment of SQL queries that we can compile to M3 essentially comprises
SUM-agg\-regation queries with group-by.
COUNT and AVG queries can be defined by arithmetic expressions over these.
We exclude MIN and MAX queries, aggregation
nested into FROM or WHERE clauses, the DISTINCT and HAVING keywords,
outerjoins,
and the relational difference operation.
We discuss which of these features can be added without fundamental
difficulties at the end of this paper.


\subsection{The Cumulus System}


We have developed a system, Cumulus\footnote{A Cumulus cloud is an
aggregation cloud, thus the name.}, that parallelizes the
execution of M3 programs in a cluster or computing cloud.
Cumulus performs incremental maintenance of exact aggregation views online.
It executes an efficient
protocol to ensure consistency of map data and query results.

While it is no fundamental requirement of our compilation
approach, we have chosen to use the resources of the cloud to maintain
the data {\em in main memory}\/,
allowing for very low latency updating and querying.
At the time of writing this,
Terabyte-sized memory chips (DIMMs and flash) have already been announced by
manufacturers, and already now, main memory capacities of clusters
are sufficient to run substantial data warehouses in
main memory, the financial premium of which over disks is
offset by greater robustness of the system, lower maintenance
costs, lower heat production \cite{1154557}, and of course by of orders
of magnitude better speed and latency characteristics.

The Cumulus protoype aims at demonstrating our results in the context of
pushing OLAP into the cloud.
Cumulus automates  the process of loading
and  maintaining  in-memory  data  warehouses.
(Optional logging of updates to secondary storage
for persistency is supported.)
Cumulus targets OLAP applications  that perform real-time analytics of
relational data.  By feeding it an SQL query, Cumulus's infrastructure
becomes linked to  a set of OLTP databases.  Cumulus  keeps the
data warehouse synchronized with the source databases via an
update stream. It achieves incremental aggregate
view maintenance {\em in realtime}
through parallelization, keeping data in main memory, and through
our approach of query processing by message passing.


\subsection{Contributions and Structure of the Paper}


Our main technical contributions are as follows.
\begin{itemize}
\item
We present M3, a massively parallelizable language
for message passing programs that can be used to incrementally maintain
SQL aggregation queries.

\item
We describe our compiler for translating SQL aggregation queries to M3
programs. Our compilation technique is based on a novel, aggressive, recursive
form of incremental view maintenance.

\item
We present Cumulus, our system for exact online aggregation in realtime.
We describe the Cumulus message passing protocol, which assures
consistency of the maps using only few messages, and
infrastructure, and show how it can be used to efficiently distribute the
processing and storage requirements of query processing and
the incremental maintenance of large aggregate views and datacubes.

\item We show evidence for the scalability of our approach by examining the
performance of Cumulus on examples drawn from the TPC-H\cite{tpch2008}
benchmark. 
\end{itemize}


The remainder of this paper is organized as follows.
Section \ref{sec:compiler} describes our SQL to M3 compiler.
In Section \ref{sec:architecture}, we provide an overview of Cumulus's online
infrastructure and discuss how data is managed within that infrastructure.
Section \ref{sec:experiments} presents
experimental results that demonstrate the viability and scalability of
Cumulus.
Section \ref{sec:relatedwork} discusses related work.
The paper concludes with Section \ref{sec:conclusions}.




