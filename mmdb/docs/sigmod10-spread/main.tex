\documentclass{sig-alternate}

\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{algorithmic}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}



\def\punto{$\hspace*{\fill}\Box$}
\newcommand{\nop}[1]{}
\newcommand{\tuple}[1]{{\langle#1\rangle}}
\def\lBrack{\lbrack\!\lbrack}
\def\rBrack{\rbrack\!\rbrack}
\newcommand{\Bracks}[1]{\lBrack#1\rBrack}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{todo}[theorem]{ToDo}
\newtheorem{algorithm}[theorem]{Algorithm}

\nop{
\newtheorem{metatheorem}{Metatheorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{property}[theorem]{Property}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proviso}[theorem]{Proviso}
} % end nop




\conferenceinfo{SIGMOD}{'10 Indianapolis, IN}


%\title{The Cumulus System for Exact Online Aggregation in the Cloud}
%\title{Cumulus: A Distributed Exact Online Aggregation System}
%\title{Cumulus: Exact Online Aggregation by Message Passing}
\title{Exact Online Aggregation by Message Passing}


\numberofauthors{3}
\author{}%Oliver Kennedy, Yanif Ahmad, Christoph Koch}
\toappear{}


\begin{document}

\maketitle


\abstract{
We present Cumulus, a distributed exact online aggregation system.
Cumulus compiles a set of SQL aggregation queries
%
% or a datacube
%
down to a message-passing program that incrementally maintains
the query results by simple and highly efficient local modifications.
Surprisingly, for each tuple inserted, updated, or deleted,
our message passing programs
have to perform only a {\em constant}\/ amount of work per
aggregate value to be maintained.
%
%assuming constant-time read/write access to individual data items.
%This can be guaranteed if the data is kept in main memory.
%
Our message-passing protocol allows for massive parallelization.

The Cumulus runtime system is a distributed key-value store for
aggregate-group-by query results that implements our message passing protocol
for incremental result maintenance.
Cumulus efficiently answers OLAP queries using this store.
Cumulus performs the incremental maintenance of large views under
considerable update loads in realtime, leveraging parallelism
and using the resources of the cloud to keep its store in main memory.
}


\input{introduction}
\input{compiler}
\input{architecture}
\input{experiments}
\input{relatedwork}

\section{Conclusions}
\label{sec:conclusions}


We have excluded a number of features from the SQL fragment we support.
MAX and MIN aggregates and the DISTINCT keywords can we added without
fundamental problems, needing only moderate extensions of M3; however,
additional data structures are needed to keep actual database relations
and auxiliary relations
around to check e.g.\ whether a new tuple is already in a relation or what the
second-best tuple is when a maximum or minimum value is removed. We
have built a prototype compiler that implements these features, but lacked the
space for treatment.

Aggregates nested inside FROM and particularly WHERE clauses in general
do not admit recursive compilations of deltas: While it is possible to express
deltas for such queries, the deltas are not structurally simpler than the
input query, and in general, recursive compilation does not terminate,
or does not result in query operator-free code.

It may be rightly observed that we only address single-tuple updates
in this paper and that batching and set-at-a-time techniques have often been
very successful in query processing. We freely admit that our current prototype
does not support batching of updates and the experiments do not report on it.
However, we believe that the compilation approach
should remain unchanged -- delta processing with sets of updates does not
result in code as simple as M3, while the
execution of M3 triggers as generated by our current compiler
can easily be batched. We can profit
particularly from batching message content to send fewer messages in the
distributed system. 

Future work will also address dynamic data partitioning and placement,
and using redundant nodes for reducing view maintenance and query processing
latencies and dealing with node failures.


\bibliographystyle{abbrv}
\bibliography{main}


\input{appendix}


\end{document}  
