
\section{Experiments}
\label{sec:experiments}

We have implemented a prototype of the sliceDBread system, written in approximately 6,000 lines of Ruby (the runtime), and 10,000 lines of Ocaml (the compiler).  We evaluate the scaling properties of our implementation on a database generated by the TPC-H benchmark database generator, using data warehouse construction queries modeled after the TPC-H benchmark queries.  

Unless otherwise specified, queries are evaluated on a data warehouse initialized with a 0.5 TB benchmark database (TPC-H scaling factor 500).  Load is generated by applying the contents of a benchmark update (refresh factor 10, or 0.1\% of the dataset) in a random order.  Unless otherwise noted, trials take place on a cluster of 40 nodes.  When a smaller cluster size is used, the size of the database used is decreased proportionately.  Each node runs on an independent 2.66 GHz core, each with a 4MB cache, and 16 GB of 667 MHz memory.  Nodes are connected over a Gigabit Ethernet link. 

Though the TPC-H benchmark query is geared towards decision support queries, asking specific questions of a database.  Conversely, sliceDBread constructs a materialized view to answer such queries quickly and efficiently.  While we do consider the response time of a queries posed over the materialized views produced by sliceDBread, our analysis focuses on the construction of that materialized view rather than the final query workloads.  Thus, instead of using the TPC-H workload as-is, we consider what materialized views could be used to efficiently answer TPC-H queries.

%TODO: verify the number of queries expressible in this way.
We select two materialized views to be representative of the TPC-H workload.  12 of the 22 queries in the TPC-H query workload can be expressed as OLAP queries over a view corresponding to one of the following natural joins:
\begin{itemize}
\item customer $\bowtie$ orders $\bowtie$ lineitem
\item part $\bowtie$ partsupp $\bowtie$ supplier
\end{itemize}
We study the behavior sliceDBread with respect to these two materialized views, respectively denoted COL and PPsS for their join paths.  Specifically, our experiments are performed with materialized projections of each view; The view construction queries are presented in Figure \ref{fig:experimentqueries}.  As we show in Section \ref{sec:exp:complexity}, these results generalize to larger views, where fewer columns are projected out.

\begin{figure*}
\begin{center}
\begin{minipage}{3.4in}
\begin{verbatim}
SELECT
  COUNT(*), customerkey, nationkey, orderkey, 
  orderpriority, shippingpriority, shipmode,
  case(when shipdate > commitdate then 1 else 0) 
    AS lateship,
  case(when receiptdate > commitdate then 1 else 0) 
    AS latereceipt
FROM
  customer JOIN orders JOIN lineitem
GROUP BY
  customerkey, nationkey, orderkey, 
  orderpriority, shippingpriority, shipmode,
  lateship, latereceipt
\end{verbatim}
\begin{center}
(a)
\end{center}
\end{minipage}
\begin{minipage}{3.4in}
\begin{verbatim}
SELECT
  SUM((retailprice - supplycost) * availqty), 
  partkey, mfgr, type, suppkey, 
  size, container, nationkey
FROM
  part JOIN partsupp JOIN supplier
GROUP BY
  partkey, mfgr, type, suppkey, 
  size, container, nationkey
\end{verbatim}
\begin{center}
(b)
\end{center}
\end{minipage}
\end{center}
\caption{Example queries used in evaluating sliceDBread.  (a) A COL query class example that produces a materialized view for analysis of order fulfilment delays, and (b) A PPsS query class example that produces a materialized view for analysis of potential asset acquisition.}
\label{fig:experimentqueries}
\end{figure*}

\subsection{Performance and Scalability}
\label{sec:exp:performance}
\label{sec:exp:scalability}

\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
((Table 1)) & ((Graph 1)) & ((Graph 2)) \\
(a) & (b) & (c)
\end{tabular}
\caption{Baseline performance characteristics of sliceDBread.  (a) Per-node memory usage for a range of data-sizes and query classes.  (b) Average CPU usage at each node for a fixed-size cluster as data update rate increases.  (c) Network bandwidth usage across the network as data update rate increases.}
\label{fig:performance}
\end{center}
\end{figure*}

We first obtain a performance baseline for sliceDBread by installing our example queries on a 10 node cluster.  Figure \ref{fig:performance}a shows sliceDBread's memory usage on a variety of dataset sizes; the overhead of storing materialized subqueries is only (TODO: Fill in number).  In Figures \ref{fig:performance}b and \ref{fig:performance}c, we measure the processing and bandwidth requirements of incrementally maintaining the example materialized views under varying load levels.

(TODO: Discuss the behavior of Figure \ref{fig:performance}b and c.)

\begin{figure*}
\begin{center}
\begin{tabular}{cc}
((Graph 3)) & ((Graph 4))\\
(a) & (b)
\end{tabular}
\caption{sliceDBread Scalability.  (a) Update and query processing improvements from adding new nodes to a sliceDBread cluster.  (b) Scalability of sliceDBread as cluster size and data volume are both increased proportionately.}
\label{fig:scalability}
\end{center}
\end{figure*}

\begin{figure}
\begin{center}
((Graph 5))
\caption{sliceDBread performance (CPU load and query latency) under variable load.}
\label{fig:variableload}
\end{center}
\end{figure}

We next analyze how sliceDBread scales with additional nodes.  Figure \ref{fig:scalability}a shows the response time of queries posed over a view materialized by the warehouse as a function of the rate at which one of the view's dependent tables is updated.  As the processing capabilities of the data warehouse become saturated processing updates, average query latency spikes dramatically.

We refer to the elbow in the latency vs update rate curve as the breakdown point, at which the processing capabilities of the warehouse have been reached and queuing delays account for most of the query latency.  Note however, that this behavior only occurs for sustained streams of updates at the indicated rate.  Figure \ref{fig:variableload} demonstrates sliceDBread's ability to absorb bursts of updates.

For all further experiments, we use the breakdown point to measure system performance in a given configuration.  Formally, the maximum update rate of the system in a given configuration is the maximum update rate (rounded to the next lower multiple of 50 updates/sec), such that query response time is at most 50\% higher that of an idle system.

Finally, we consider the performance impact of scaling sliceDBread to larger cluster sizes and data volumes.  Figure \ref{fig:scalability} shows the effect of growing the cluster while maintaining a constant volume of data stored on each node.  (TODO: discuss the graph).

\subsection{Query Complexity}
\label{sec:exp:complexity}
Up to this point, we considered sliceDBread's performance on two specific materialized views.  We next analyze how sliceDBread reacts to queries of varying complexity.  

First, Figure \ref{fig:scalinggroupby} shows performance with respect to the amount of projection performed.  While non-key columns have a relatively low impact on performance, by removing key columns from the map storing the materialized view, message locality decreases; more M3 statements need to cross node boundaries.  Conversely, smaller materialized views represent a higher degree of pre-aggregation, and have correspondingly less work to do when a query is issued.

\begin{figure}
((Graph 6))
\caption{Maximum update rate for a given number of columns in the materialized view}
\label{fig:scalinggroupby}
\end{figure}

Second, Figure \ref{fig:scalingjoinwidth}

\begin{figure}
((Graph 7))
\caption{Maximum update rate for a given number of columns in the materialized view}
\end{figure}

\subsection{Foreign Key Optimization}
\label{sec:exp:fkoptimization}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                 Notes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% TPCH Joins %%%%%%%%%%%%%%%%
% 
% Q01: li
% Q02: p, ps, s, n, r
% Q03: c, o, li
% Q04: o, li
% Q05: c, o, l, s, n, r
% Q06: l
% Q07: s, l, o, c, n, n
% Q08: p, s, l, o, c, n, n, r
% Q09: p, s, l, ps, o, n
% Q10: c, o, l, n
% Q11: p, ps, s
% Q12: o, l
% Q13: c, o
% Q14: l, p
% Q15: s, l
% Q16: p, ps, s
% Q17: l, p
% Q18: c, o, l
% Q19: l, p
% Q20: l, p, ps, s, n
% Q21: s, l, o, n
% Q22: c