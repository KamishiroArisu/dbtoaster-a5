
\section{Experiments}
\label{sec:experiments}

We have implemented a prototype of Cumulus, written in approximately 6,000 lines of Ruby, and 10,000 lines of Ocaml.  We evaluate the scaling properties of our implementation on a database generated by the TPC-H database generator, and use datacube equivalents of several TPC-H queries.  Unless otherwise noted, data nodes are primed with data equivalent to a TPC-H database built with a scaling factor of 5 (corresponding to a 5 GB database), up to 2\% of which is updated over the course of our trials cases.

Unless otherwise specified, trials take place on a cluster of 41 nodes, including one dedicated Switch node.  Each node was allowed one 2.66 GHz core with a 4MB cache, and 16 GB of 667 MHz memory.  Nodes are run on separate machines, connected by a Gigabit ethernet link.  TPC-H input data is reduced down to integer and floating point values prior to insertion by performing single-tuple transforms (as might normally happen during preaggregation).  These transforms include arithmetic, boolean, regular-expression, and hashing operations.

% Q01: li
% Q02: p, ps, s, n, r
% Q03: c, o, li
% Q04: o, li
% Q05: c, o, l, s, n, r
% Q06: l
% Q07: s, l, o, c, n, n
% Q08: p, s, l, o, c, n, n, r
% Q09: p, s, l, ps, o, n
% Q10: c, o, l, n
% Q11: p, ps, s
% Q12: o, l
% Q13: c, o
% Q14: l, p
% Q15: s, l
% Q16: p, ps, s
% Q17: l, p
% Q18: c, o, l
% Q19: l, p
% Q20: l, p, ps, s, n
% Q21: s, l, o, n
% Q22: c
We study the behavior of 3 categories of query.
\begin{itemize}
\item $Customer \bowtie Orders \bowtie Lineitem$
\item $Part \bowtie Partsupp \bowtie Supplier$
\item $Lineitem \bowtie Part \bowtie Partsupp \bowtie Supplier \bowtie Nation$
\end{itemize}
The selection of datacube construction queries is guided by the TPC-H query workload.  18 of the 22 queries in the TPC-H workload can be evaluated on a datacube generated over one of the enumerated joins.  Where the specific query being computed is not a variable, we use a slightly more complex version of TPCH Query 12 that corresponds to the first of these joins.  Unless otherwise noted, the query uses the following 6 group-by columns taken directly from the input tables: 

{\noindent $CustomerKey, NationKey, OrderKey,$ \\
$OrderPriority, ShippingPriority, ShipMode$}

\noindent and precomputes 2 group-by columns from the boolean expressions

{\noindent $ShipDate > CommitDate$ as $ShipLate$\\
$RecvDate > CommitDate$ as $RecvLate$}

\subsection{Scalability}

The first point of our analysis is to study the behavior of Cumulus under increasing load on nodes.  We achieve this by starting with an empty database and issuing inserts that trigger progressively more complex inserts.  Data is drawn from a TPC-H database generated with a scaling factor of 0.1.  Using the default query, we perform Customer, Order, and Lineitem inserts, interleaved with each other.  As the number of inserts grows, so does the frequency with which Orders correspond to already-inserted Customers (and visa versa).  With the increased frequency of correspondence, additional work needs to be done to insert tuples into maps.  

Figure \ref{fig:expandingbreakdown} tracks the breakdown points of individual Cumulus nodes at varying loads and cluster sizes.  As inserts occur, aggregation queries are posed to the cluster to measure performance.  Query latency remains relatively constant, only growing significantly once the system is saturated by updates.  The elbow in each graph represents the point at which the cluster has reached its processing capacity.  Note that the cluster's processing capacity scales linearly with the cluster size; a 20 node cluster can trivially handle the demands placed on it when every insert is guaranteed to trigger many map updates.

\begin{figure}
\begin{center}
\includegraphics[width=3.0in]{images/expandingbreakdown.pdf}
\caption{Query latencies leading up to the node processing breakdown point.}
\label{fig:expandingbreakdown}
\end{center}
\end{figure}

We expand on these results, showing the CPU utilization of the 20-node case in Figure \ref{fig:20nodes100mbUpdatesVsCPUCompletion}, and Memory utilization in Figure \ref{fig:20nodes100mbUpdatesVsMemory}.  Even as the domains of a given loop variable increase (the sharp increase in the slope of the completion line), per-node processor usage manages to tail off at about 30\%, increasing to 40\% only towards the very end.  Similarly, despite increased usage due to inefficient memory management in Ruby, node memory usage scales linearly with the amount of data being stored at the node.

\begin{figure}
\begin{center}
\includegraphics[width=3.0in]{images/20nodes100mbUpdatesVsCPUCompletion.pdf}
\caption{CPU Utilization as Query 1 approaches its increasingly CPU-intensive completion.}
\label{fig:20nodes100mbUpdatesVsCPUCompletion}
\end{center}
\end{figure}
\begin{figure}
\begin{center}
\includegraphics[width=3.0in]{images/20nodes100mbUpdatesVsMemory.pdf}
\caption{Map memory over the course of inserting a TPC-H database of scaling factor 0.1}
\label{fig:20nodes100mbUpdatesVsMemory}
\end{center}
\end{figure}

\subsection{Aggregation}
% Removal order: 
% COL: Order.OrderPriority, LineItem.ShipLate, Customer.NID, LineItem.ShipMode, Order.ShipPriority, LineItem.RecvLate
% PPSS: Type, MFGR
We next consider the behavior of Cumulus with respect to redundancy in the query workload.  Figure \ref{fig:aggvsupdates} shows the update rate as a function of the number of group-by aggregate columns in the query.  For each query, a randomly selected subset of the non-key group-by aggregate terms are removed.  While each group-by column adds another dimension to one or more maps, foreach loops iterate only over values actually defined in the input maps.  Because the removed columns are functionally dependent on at least one of the two remaining key columns, adding them to the query does not de-aggregate the query result and no aditional work is required.

\begin{figure}
\begin{center}
\includegraphics[width=3.0in]{images/aggvsupdates.pdf}
\caption{Update rate as a function of number of dependent group-by columns added}
\label{fig:aggvsupdates}
\end{center}
\end{figure}

Similarly, Figure \ref{fig:aggvslatency} shows the effect of de-aggregation on query latency.  As updates occurred within the cluster, th cluster was continuously asked to provide a running count of the sum with no group-by columns.  Shown in the figure is the average response time with respect to the number of group-by columns in the query itself.  Removing all the non-key group-by columns produces a query that groups line-items together.  The factor of four decrease in query processing time corresponds to the average number of lineitems per order.  Combined with Cumulus' processing scalability, this graph demonstrates the feasibility of implementing and incrementally maintaining an entire data-cube in the cloud; The extra work of processing partial aggregates (as in a partially materialized datacube) can be spread out across nodeds in the cloud.  Conversely, an OLAP user need only interact with map partitions for the aggregate closest to their query, limiting the number of message required to pose the query.

\begin{figure}
\begin{center}
\includegraphics[width=3.0in]{images/aggvslatency.pdf}
\caption{Query latency as a function of the number of dependent group-by columns added}
\label{fig:aggvslatency}
\end{center}
\end{figure}

