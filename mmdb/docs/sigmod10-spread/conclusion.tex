

\section{Conclusions and Future Work}
\label{sec:conclusions}


Our preliminary experiments show that the choices made in the Cumulus
system indeed allow us to realize the embarassing parallelism and
thus the scalability in a cluster that is promised by our approach of
compiling to M3 programs. Much work is left to be down, however.


We have excluded a number of features from the SQL fragment we support.
MAX and MIN aggregates and the DISTINCT keywords can be added without
fundamental problems, needing only moderate extensions of M3; however,
additional data structures are needed to keep actual database relations
and auxiliary relations
around to check e.g.\ whether a new tuple is already in a relation or what the
second-best tuple is when a maximum or minimum value is removed. We
have built a prototype compiler that implements these features, but lacked the
space for treatment.

\nop{
Aggregates nested inside FROM and particularly WHERE clauses in general
do not admit recursive compilations of deltas: While it is possible to express
deltas for such queries, the deltas are not structurally simpler than the
input query, and in general, recursive compilation does not terminate,
or does not result in query operator-free code.
} % end nop


We only address single-tuple updates in this paper.
Batching and set-at-a-time techniques have often been
very successful in query processing.
We believe that the compilation approach
should remain unchanged -- delta processing with sets of updates does not
result in code as simple as M3, while the
execution of M3 triggers as generated by our current compiler
can easily be batched. We can profit
particularly from batching message content to send fewer messages in
Cumulus runtime. 


%The Cumulus runtime system currently does only static data placement
%and does not migrate
%data to balance load across DW nodes.
%Moreover, we currently do not deal with node failures (which would naturally
%be addressed using redundant data placement).

Future work will also address dynamic data partitioning and placement,
and using redundant nodes for
% reducing view maintenance and query processing latencies and
dealing with node failures.


