Predictions made by statistical models, scientific applications, and data extraction from unstructured text are well known sources of uncertain data.  Measurements have error margins while predictions are typically drawn from well known distributions.  Traditional database management systems (DBMS) are ill-equipped to manage this kind of uncertainty.  For example, consider a risk-management application that uses statistical models to evaluate the long term effects of corporate decisions and policies.  This application may use a DBMS to store predictions and statistical measures (e.g., error bounds) of those predictions.  However, arbitrary queries made on the predictions do not translate naturally into queries on the corresponding statistical measures.  A user who requires error bounds on the sum of a join over several tables of predictions must first obtain a formula for computing those bounds, assuming a closed form formula even exists.

Probabilistic  database  management  systems \cite{dalvi07efficient, WidomTrio2008, KochMayBMS2008, SD2007, ORION, MCDB, BayesStore} aim at providing better support for querying uncertain data.  Queries in these systems preserve the statistical properties of the data being queried, allowing users to obtain metrics about and representations of query results.  The previously mentioned risk-management application, built on top of a probabilistic database, could use the database itself to obtain error bounds on the results of arbitrary queries over its predictions.  By encoding the statistical model for its predictions in the database itself, the risk-management application could even use the probabilistic database to estimate complex functions over many correlated variables in its model.  In effect, the application could compute all of its predictions within the probabilistic database in the first place.

Few systems are general enough to efficiently query probabilistic data defined over both discrete and continuous distributions.  Those that are, generally rely on sampling to estimate desired values, as exact solutions can be hard to obtain.  If a query contains a selection predicate, samples violating the predicate are dropped and do not contribute to the expectation.  The more selective the predicate, the more samples are needed to maintain consistent accuracy.  For example, a query may combine a model predicting customer profits with a model for predicting dissatisfied customers, perhaps as a result of a corporate decision to use a cheaper, but slower shipping company.  If the query asks for profit loss due to dissatisfied customers, the query need only consider profit from customers under those conditions where the customer is dissatisfied (ie, the underlying model may include a correlation between ordering patterns and dependence on fast shipping).  

Without knowing the likelihood that customer A is satisfied, the query engine must over-provision and waste time generating large numbers of samples, or risk needing to re-evaluate the query if additional samples are needed.  This problem is well known in online aggregation, but ignored in general-purpose (i.e., both discrete and continuous) probabilistic databases.  


\begin{example}\em
\label{ex:intro}
Suppose a database captures customer orders expected for the next quarter,
including prices
and destinations of shipment. The order prices are 
uncertain, but a probability distribution is assumed.
The database also stores
distributions of shipping durations for each location.
Here are two c-tables defining such a probabilistic database:
\[
\begin{tabular}{c|ccc}
Order & Cust & ShipTo & Price \\
\hline
& Joe & NY & $X_1$ \\
& Bob & LA & $X_3$ \\
\end{tabular}
%\hspace{5mm}
\]\[
\begin{tabular}{c|cc}
Shipping & Dest & Duration \\
\hline
& NY & $X_2$ \\
& LA & $X_4$ \\
\end{tabular}
\]
We assume a suitable specification of the joint distribution $p$ of the random
variables $X_1,\dots,X_4$ occurring in this database.

Now consider the query
{\small\begin{verbatim}
  select expected_sum(O.Price)
  from   Order O, Shipping S
  where  O.ShipTo = S.Dest
  and    O.Cust = 'Joe'
  and    S.Duration >= 7;
\end{verbatim}}
asking for the expected loss due to late deliveries to customers named Joe,
where the product is free if not delivered within seven days.
%
This can be approximated by monte carlo sampling from $p$,
where $q$ represents the result of the sum aggregate query on a sample,
here
\[
q(\vec{x}) =
\left\{
\begin{array}{lll}
x_1 & \dots & x_2 \ge 7 \\
0 & \dots & \mbox{otherwise.}
\end{array}
\right.
\]

In a naive sample-first approach, 
if $x_2 \ge 7$ is a relatively rare event, a large number of samples will be
required to compute a good approximation to the expectation.
Moreover, the profit $x_1$ is independent from the shipping time $x_2$.  Despite this, samples for $x_1$ are still discarded if the constraint on $x_2$ is not met.

\end{example}

\subsection{Contributions}

Selective queries exemplify the need for contextual information when computing expectations and moments.  This paper presents PIP, a highly extensible, general probabilistic database system built around this need for information.  PIP evaluates queries on symbolic representations of probabilistic data, computing a complete representation of the probabilistic expression to be evaluated before an expectation or moment is taken.  To our knowledge, PIP is the first probabilistic database system supporting continuous distributions to evaluate queries in this way.

{\em PIP}\/'s approach encompasses and extends the strengths of discrete systems that use c-tables such as Trio \cite{WidomTrio2008}, MystiQ \cite{dalvi07efficient}, and MayBMS \cite{AJKO2008}, as well as the generality of the sample-first approach taken by MCDB \cite{MCDB}.  It supports both discrete and continuous probability distributions, statistical dependencies definable by queries, expectations of aggregates and distinct-aggregates with or without group-by,  and the computation of confidences. The detailed technical contributions of this paper are as follows.

\begin{itemize}
\item We propose PIP, the first probabilistic database system based on c-tables to efficiently support continuous probability distributions.

\item We show how PIP acts as a generalizable framework for exploiting information about distributions beyond simple sampling functionality (e.g., inverse cdfs) to enhance query processing speed and accuracy.  We demonstrate this framework by implementing several traditional statistical optimizations within it.

\item We propose a technique for identifying variable independence in c-table conditions and exploit it to accelerate sampling.

\item
We provide experimental evidence for the competitiveness of our approach.  We compare PIP with a reimplementation of the refined sample-first approach taken by MCDB by using a common codebase (both systems are implemented on top of Postgres) to enable fair comparison.  We show that PIP's framework performs considerably better than MCDB over a wide range of queries, despite applying only a relatively straightforward set of statistical optimizations.  Even in the worst case, PIP remains competitive with MCDB (it essentially never does more work).

\end{itemize}

The structure of this paper is as follows. Section~\ref{sec:background} describes related work and the c-tables primitive. Section~\ref{sec:design} provides a high level overview of PIP's architecture.  Section~\ref{sec:sampling} studies sampling and presents several techniques used by PIP to compute expectations and moments.  PIP's implementation is described in Section~\ref{sec:implementation}.  Finally, in Section~\ref{sec:evaluation}, we present the outcomes of our experiments with PIP and our MCDB reimplementation.

