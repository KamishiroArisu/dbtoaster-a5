In order to evaluate the viability of PIP's c-tables approach to continuous variables, we have implemented an initial version of PIP as an extension to the PostgreSQL DBMS.  PIP's extended functionality is provided by a set of user-defined functions written in C.  

\subsection{Query Rewriting}
Much of this added functionality takes advantage of PostgreSQL's extensibility features, and can be used ``out-of-the-box".  For example, we define the function 

\textbf{CREATE\_VARIABLE($distribution$[,$params$])} \\
which is used to create variables.  Each call allocates a new variable and initializes it with the specified parameters.  When defining selection targets, operator overloading is used to make random variables appear as normal variables; arbitrary equations may be constructed in this way.  Similarly, double angle brackets around a random variable equation ($<< Y >>$) are shorthand for the equation's in a vaccum (without external constraints).  The \@ sign may be used to specify context for the expectation.  $<< Y @ row >>$ computes the expectation of variable $Y$ under the constraints in the corresponding $row$.

%Angle brackets around a random variable are shorthand for the variable's expectation.  All instances of this are replaced by a call to pip's expectation sampling function.  

To complete the illusion, we have modified PostgreSQL itself to add support for C-Table constructs.  Under the modified PostgreSQL when defining a datatype, it is possible to declare it as a CTYPE; doing so has the following two effects:
\begin{itemize}
\item CTYPE columns (and conjunctions of CTYPE columns) may appear in the WHERE and HAVING clauses of a SELECT statement.  When found, the CTYPE components of clause are moved to the SELECT's target clause.  For example, if (X>Y) resolves to a CTYPE variable, 
\begin{verbatim}
select *
from   inputs
where  X>Y and Z like '%foo'
\end{verbatim}
is rewritten to
\begin{verbatim}
select *, X>Y
from   inputs
where  Z like '%foo'
\end{verbatim}

\item SELECT target clauses are rewritten to ensure that all CTYPE columns in input tables are passed through.
\end{itemize}

Note that these extensions are not required to access PIP's core functionality; they exist to allow users to seamlessly use deterministic queries on probabilistic data.

PIP takes advantage of this by encoding constraint atoms in a CTYPE datatype; Overloaded $>$ and $<$ operators return a constraint atom instead of a boolean if a random variable is involved in the inequality, and the user can ignore the distinction between random variable and constant value (until the final statistical analysis).

\subsection{Defining Distributions}
PIP's primary benefit over other c-tables implementations is its ability to admit variables chosen from arbitrary continuous distributions.  These distributions are specified in terms of general distribution classes, a set of C functions that describes the distribution.  In addition to a small number of functions used to parse and encode parameter strings, each PIP distribution class defines one or more of the following functions.
\begin{enumerate}
\item $Generate(Parameters, Seed)$ uses a pseudorandom number generator to generate a value sampled from the distribution.  The seed value allows PIP to limit the amount of state it needs to maintain; multiple calls to Generate with the same seed value produce the same sample, so only the seed value need be stored.
\item $PDF(Parameters, x)$ evaluates the probability density function of the distribution at the specified point.  
\item $CDF(Parameters, x)$ evaluates the cumulative distribution function at the specified point.
\item $InverseCDF(Parameters, Value)$ evaluates the inverse of the cumulative distribution function at the specified point.
\end{enumerate}

PIP requires that all distribution classes define a Generate function.  All other functions are optional, but can be used to improve PIP's performance if specified.  Consequently, the supplemental functions need only be provided when it is possible to evaluate them efficiently.  Depending on the user's needs however, it may be reasonable to provide estimates instead of exact values.  For example, the CDF of a Normal distribution is a complex integral, but may be efficiently estimated by interpolating between precomputed values.  

Future implementations could conceivably generalize the sampling process.  A sample may be generated using any of the four functions: The Metropolis-Hastings algorithm can sample from an arbitrary PDF, the inverse CDF evaluated on a uniform random value produces a sample, and a binary search may be used to evaluate the inverse CDF given the CDF.

\subsection{Sampling Functionality}
PIP provides several functions for analyzing the uncertainty encoded in a c-table.  The two core analysis functions are conf() and expect().

conf() performs a conjunctive integration to estimate the probability of a specific row being present in the output.  It identifies and extracts all lineage atoms from the row being processed and then performs the conjunctive integration over them as normal.

aconf(), a variant of conf(), is used to perform general integration.  This function is an aggregate that computes the joint probability of at least one aggregated row being present in the output.  Two variants of aconf() exist: a naive one-pass version, and a Karp-Luby variant that builds a transient table on which it performs two more aggregation passes.

expected() computes the expectation of a variable by repeated sampling.  If a row may be specified when the function is called, the sampling process is constrained by the constraint atoms present in the row.

stddev() computes the standard deviation of the expectation by repeated sampling.  As with expected() a row may be used to constrain the range of the variable.

histogram() is similar to expected() in that it performs repeated sampling, or constrained sampling if appropriate.  However, instead of outputting the average of the results, it instead outputs an array of all the requested samples.

Aggregates pose a challenge for the query phase of the PIP evaluation process.  Though it is theoretically possible to create composite variables that represent aggregates of their inputs, in practice it is infeasible to do so.  The size of such a composite is not just unbounded, but linear in the size of the input table.  A composite aggregate variable could easily grow to an unmanageable level.  Instead, PIP limits random variable aggregation to the sampling phase.  

Many aggregates such as sum() and average() are linear; the expectation of the sum is the sum of the expectation.  PIP implements these by combining Postgres' existing aggregate operator with PIP's expectation operator.  PIP also implements histogram variants of both operators.  Nonlinear aggregates such as max(), min(), and stddev() are special-cased.  

