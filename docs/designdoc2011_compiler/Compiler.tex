\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{color}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\title{The DBToaster Compilation Algorithm}
\author{Yanif Ahmad, Oliver Kennedy, Christoph Koch}

\newcommand{\todo}[1]{\textcolor{red}{[#1]}}
\newcommand{\note}[1]{\textcolor{blue}{[#1]}}
\newcommand{\tuple}[1]{\left<{#1}\right>}
\newcommand{\parsection}[1]{\smallskip\noindent{\bf #1.}}

\newcommand{\AggSum}{{\tt AggSum}\ }
\newcommand{\AggSums}{{\tt AggSum}s\ }

\newtheorem{example}{Example}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\begin{document}
\maketitle

\section{Workflow Overview}

\includegraphics[width=3in]{CompilerFlow}


\section{Calculus}

\begin{verbatim}

type calc_type_t = 
  Int_Type | Double_Type | String_Type

type const_value_t = 
  Integer of int | Double of double | String of string

type var_t = string * calc_type_t

type value_t = 
  Var   of var_t
| Const of const_value_t

type cmp_t = Lt | Gt | Lte | Gte | Eq | Neq

type 'a calc_t =
  Sum        of calc_t list               (* c1 + c2 + ... *)
| Prod       of calc_t list               (* c1 * c2 * ... *)
| Neg        of calc_t                    (* -c *)
| Cmp        of value_t * cmp_t * value_t (* c1 [cmp] c2 *)
| AggSum     of var_t list * calc_t       (* AggSum([v1,v2,...], c) *)
| Value      of value_t                   (* Var(v) | Const(#) *)
| Relation   of string * var_t list       (* R(v1, v2, ...) *)
| External   of string * var_t list * 'a  (* {M1(v1, v2, ...) := 'a} *)
| Definition of var_t * calc_t            (* v <- c *)

type calculus_t = unit calc_t
\end{verbatim}

Most of the above is straightforward.  We parameterize the calc\_t type, because  the External type is, by definition outside of Calculus.  It is a reference to something outside, and we don't want Calculus to have to care about anything other than the API (a name and a set of parameters).  

Components using Calculus, conversely, need to be able to associate metadata with the External (i.e., initializers, etc...).  In particular, we would like to be able to associate invocations with the External, and we don't want Calculus to have any idea what an invocation is.  Parameterizing calc\_t is the cleanest way to accomplish this.

\section{Incremental Plan (IP)}


\begin{verbatim}

type schema_t = var_t list

type memo_t = 
    string                (* Name *)
    schema_t              (* Schema *)
  * calc_t                (* Defining calculus expression (the calculus 
                             expression that the memo is designed to 
                             maintain/compute *)
  * (   pm_t * string     (* Insert/Delete + Base Relation Name *)
      * map_op_t option   (* LHS initializer *)
      * map_op_t list     (* Update operations *)
    ) list

and map_op_t = (* += or := *)
    schema_t              (* LHS slice being updated/set *)
  * invocation_t          (* Invocation generating the slice *)

and invocation_t =
    calc_t                (* Calculus expression that the statement invokes *)
  * (                     (* Alternative ways of partitioning the invoked 
                             expression into different memos.  The head of this
                             list is the default approach. *)
      * calc_t            (* An expression over zero or more memos that is 
                             semantically equivalent to the invoked expression*)
      * list of memo_t    (* References to memos used by the above expression *)
    ) list

\end{verbatim}

\subsection{Assertions}
Assertions about the above memo and invocation types (the hypergraph):
\begin{itemize}
\item Initializers initialize the entire Memo, and therefore have no externally bound variables other than the invoked expression's input variables, and the variables which appear in the corresponding initializer.  If an input variable is not bound, it is a loop variable.

\item The access patterns that a Memo is required to support are defined by the set of bound variables in all of its incoming edges.

\item Initializers for memos on the RHS of an expression are stored as part of the calculus expression (External leaves) \footnote{Storing the whole initializer will make working with nonstandard maps (e.g., future work on range trees) difficult.  In effect, the External needs to store a reference to the entire process required to initialize a memo.  On the other hand, this initializer can and should be simplified/compiled given that the set of bound input variables will change.  A reasonable compromise might be to store a list of expressions that the initializer will need to evaluate}.  The LHS memo does not need to be initialized, since the output portion of the memo is guaranteed to be initialized and we'll never see a new input variable in a delta statement.

\item The root of an IP can be either a Memo or an Invocation.  The former corresponds to compiling a datastructure which continually maintains the exact query result.  The latter corresponds to compiling an executable which can efficiently (using incrementally maintained views) answer the posed query.

\end{itemize}

\begin{example}
The following image shows the incremental plan for the query:
$$Q = Sum([], R(a,b) * S(b,c) * T(c,d) * a * d)$$

\includegraphics[width=5.0in]{IPExample}
\end{example}

\section{Compilation}

The goal of the compilation phase is to translate an arbitrary calculus expression into an Incremental Plan, as described above.  At an extremely high level, this process breaks down into a recursive loop over three steps: (1) {\bf Simplify} the expression, (2) {\bf Extract} portions of the expression for incremental maintenance, (3) Compute the {\bf delta} terms, and recurse for each.

To mirror the structure of the IP, make it possible to compile queries into both Memos and Invocations, and because step 1 in the above process produces a semantically equivalent query, the compiler is divided into two parts - {\bf compile\_memo} and {\bf compile\_invocation}.

\subsection{Calculus and Code}

Ultimately the goal of DBToaster is to produce code.  The structure of a calculus expression translates directly into how that expression will be evaluated.  Correspondingly, the compiler phase must be designed to ensure efficient code evaluation.  We now make several meaningful observations about calculus expressions and how they translate to the generated code.

\parsection{Input and Output Variables}
Variables appearing in a calculus expression take one of two forms: Input and Output.  Output variables appear on the LHS of definition terms or as part of a Relation term.  Given an expression, we can always identify its Output variables.  

Input variables appear in the expression as values, but are not bound within the expression (i.e., they are not Output variables).  Given an expression, we can always identify its Input variables.  Input variables can be thought of as {\it parameters} to the expression.

\parsection{Bags and Arities}
As a data parallel language, the fundamental datatype in calculus is the set (of tuples), or more precisely a bag.  Like a traditional bag, each tuple is associated with an arity, or number of times that the tuple appears in the bag.  Unlike traditional bags however, a tuple's arity can be negative, or even non-integral.  In this sense, the set is a map, the tuple is a key, and the arity is the corresponding value.  These two interpretations are mostly interchangeable.  

It is important to understand them both to understand how Calculus interacts with both the relational and aggregation aspects of SQL, and how to correctly interpret the {\tt Sum} and {\tt Prod} operators (discussed below).  The use of arity also explains why \AggSum acts as both a projection and an aggregation operator, since post-projection duplicates have their arities merged.

An important consequence is that in calculus, there is no distinction between $\{(\left<\ldots\right> = 0)\}$ and $\emptyset$.  That is, a tuple with arity 0 is not at all distinct from a tuple not present in a result set; Thus typically, when referring to the domain of an expression, we will consider only the elements of the set with arity $\neq 0$ \footnote{It should also be noted that while calculus does not make this distinction, later stages do; When storing a partially materialized result set, a zero arity indicates a tuple not present in the complete result set, while absence from the set indicates that the tuple's arity has not yet been computed.}

\parsection{Schemas and Singletons}
An expression's output variables correspond to the schema of all of its result tuples\footnote{{\tt Sum} can potentially be viewed as generating hybrid schemas, but like SQL's UNION we require that it operate only over subexpressions with identical schemas}.  The behavior of the {\tt Prod} operator in particular relates very closely to the schemas of its subexpressions -- in particular those with no output variables, termed {\it singletons}.  These are particularly important, because they have a finite domain at compile time (i.e., they do not require looping).  More on this later.

\parsection{Loops} 
Our current infrastructure focuses on single-threaded operation, so it is important to recognize that all calculus expressions represent an implicit loop over elements of the set of values produced by the expression.

If an expression appears nested inside a calculus operator, the calculus operator explicitly performs an operation over each element in the set (e.g., negation negates every element of the set).  

Even the top level expression can (and should) be interpreted as a loop; Whatever is evaluating the expression loops over each of its elements.  For example, the {\tt :=} and {\tt +=} operators loop over the entire domain, setting or incrementing the corresponding LHS map entry.

Note that there is a key difference between the traditional imperative notion of loops and the more set-oriented approach taken by calculus: the loop domain comes from within the loop block itself.  In terms of implementation each non-leaf, or operator term in calculus applies an operation to every element of the domain of its child expressions.  For unary operators (i.e., {\tt Neg} and \AggSum), the desired operation and imperative interpretation is clear; \AggSum for example, can be thought of as an inner loop.  The operator asserts that a particular subset of the loop domain is meaningful only within a specific subtree of the code.

N-ary operators (i.e., {\tt Prod} and {\tt Sum}) have a slightly different implementation interpretation.  Though Calculus is a primarily functional language, {\tt Sum} can be thought of as an imperative ``step," or operation in the loop.  From a set-loop-oriented perspective, one can think of this (roughly) as first iterating over the values on the left-hand side, and subsequently the right-hand.  Formally, the sum operator must merge duplicate values.  However, when converting a calculus expression to an imperative execution plan, it helps to think of each expression in the sum as an independent step in the loop.

\parsection{{\tt Prod} (and Singletons and Single-entry tables)}
If {\tt Sum} is an imperative step, then {\tt Prod} is more closely related to a continuation.  The intuition is that a calculus expression is parameterized (by input variables), and can influence control flow by returning a carefully selected set of results (e.g., a zero arity singleton to terminate the current loop iteration).  

A common instantiation of this behavior is via singleton expressions, which can affect the arity of tuples in the result set without affecting the domain: (1) {\tt Value} terms, (2) {\tt Cmp} terms, and (3) \AggSum terms with no group-by variables.

\begin{example}
Consider the following expression:
$$expr * (a \theta b)$$
This expression represents a simple filtering predicate: $(a \theta b)$.  More complex incremental predicates can be applied by using definition terms.  For example:
$$\AggSum(expr_{in}.schema, expr_{in} * (a \gets expr_a) * (b \gets expr_b) * (a \theta b))$$
\end{example}

\parsection{{\tt Definition}}
Consider what happens if $expr_a$ and $expr_b$ are both singletons themselves.  Definition terms with singletons on the RHS do not affect the arity (they always evaluate to 1), nor do they affect the size of the tuple set (they always have a single value).  They do, however, extend the domain with additional variables (that can be used for filtering or further computation).  One way to think about this sort of behavior is as a continuation.

\parsection{Commutativity (and Associativity) of {\tt Prod}}
{\tt Prod} has some non-straightforward commutativity properties: Product is commutative {\bf unless} an Input variable appearing on the right hand side (i.e., a parameter) is present as an Output variable on the LHS \footnote{An exception to this rule occurs when there is a triple product with the variable in question being used as a parameter to the third subterm.  If the first and second subterms both output the variable in question, the third and second subterms commute, assuming no other conflicts occur.}.  In effect, a product term has a binding pattern, where variables are declared by their presence as output variables on the left, and used as input variables on the right.  As in programming languages, a variable's declaration must always precede its use.  

On a similar note, though {\tt Prod} is associative, the order of operations can impact the efficiency of evaluating the expression.  

\begin{example}
For example, consider the expression:
$$\AggSum([c], f_1[][a] * f_2[][b] * f_3[a,b][c])$$
\end{example}

It is important to note that the variable $a$ does not affect evaluation of $f_2$.  Consequently, it might be more efficient to evaluate this expression as right-associative (especially if it is possible to curry $f_3$).  This process is analogous to join ordering.

\parsection{The (ir)relevance of \AggSum s}
Though it might appear otherwise, the role of \AggSums is purely as a convenient annotation tool.  Without them, evaluating a calculus expression would require a repeated loop through the entire domain of the result set.  \AggSums are based around the observation that the typical usage pattern of a calculus expression involves aggregating over subsets of the result domain.  Furthermore, certain variables only appear in parts of the expression and are not required as part of the output domain (i.e., we aggregate over them).  By wrapping the portions of the expression that use a particular variable in \AggSum s, we indicate that this subset of the overall loop domain is isolated to within the {AggSum} (and doing so, simplify code generation).

\subsection{Compiler Challenges}

\parsection{Extraction}
Given an arbitrary calculus expression, we can create a memo to persist and incrementally maintain its result set.  However, it is often inefficient to do so for the entire calculus expression.  

\begin{itemize}
\item The expression may consist of two or more independent sub-queries.  For example, the expression computes the numerical product of two (potentially group-by) aggregate queries.  Storing the full cross-product of all group-by terms involved is unnecessary. as the expression must iterate over each of them; its time complexity is the same regardless of whether the cross-product is materialized or generated at run-time.  Conversely, materializing the cross product generates unnecessary storage (and consequently maintenance) overhead. 
 
\item Portions of the expression may involve constants.  Applying the constant when the overall expression is evaluated does not increase the time complexity of doing so.  Moreover, the resultant memo is simpler and easier to re-use across multiple evaluations.

\item Similarly, the expression may contain parameters (i.e., Input Variables), either loose or within inequalities.  We can treat the parameters as constants (and perform any necessary filtering when the expression is evaluated), or fold them into the memo expression and create a caching memo.

\item Definition terms do not get simpler when their delta is taken.  Thus, for definitions that can not be eliminated (e.g., by variable unification), it is necessary to perform the definition computation at evaluation time.  
\end{itemize}

As a consequence, the compiler must include an extraction step, where it identifies portions of the expression that are to be represented as memos.  There are three components to this process (assuming that the expression has already been separated into monomials):

\begin{algorithmic}[1]
\STATE Constant and (if applicable) expressions involving input variables\todo{There are countless ways of defining ``expressions involving input variables''.  We should pick which (and how many) the compiler should use.} are pulled out of the expression and used solely in the invocation.
\STATE Definition terms are pulled out and ignored for the next step.
\STATE The expression is factorized into separate cross-product subexpressions. Each subexpression is replaced by a memo.
\STATE Definition terms are used in the memo, and we recurse over the expression that appears on the RHS of the definition term
\end{algorithmic}

\parsection{Variable Unification}
The input and output variables (both input and output, as well as their types) of a calculus expression can always be computed from the expression; external metadata is not required.  

Variable unification has an impact on the schema of an expression.  Not only can it potentially remove input and output variables, but it can turn input variables into output variables (i.e., if an input variable is unified with an output variable) \footnote{Of course, this poses some limitations with respect to extracted expressions.  The schema of a memo can not be changed after the fact.  Even if it is possible to perform further unification on a delta expression, the schema must be further adjusted to match the memo being updated.}

Variable unification itself takes two forms: (1) downward, and (2) upward.  

Upward unification is the harder of the two, because it requires knowledge of input variables.  A comparison between two variables $x = y$ can be unified upwards using one of the following two rules:
\begin{itemize}
\item $e := f(x) * (x = y)$ becomes $(x \gets y) * f(x)$ iff $x$ is not already defined in the evaluation environment \footnote{note that it is not just an assertion that $x$ is not an input variable; $x$ can not be bound anywhere to the left of $e$, even if it is present as an output variable in $f(x)$.}.
\item $e := f(y) * (x = y)$ becomes $(y \gets x) * f(y)$ iff $y$ is not already defined in the evaluation environment.
\end{itemize}

It is possible for a variable to be bound at multiple points in an expression (i.e., it appears in multiple relation terms).  Thus, the upward unification process requires that the newly bound variable be lifted above all of the variable's relation bind-points.

Downward unification is simpler.  An expression of the form:
$$\AggSum\left([dom(f(x)) - \{x\}], (x \gets y) * f(x)\right)$$
can be unified down to $f(y)$.  Interestingly, note that this process is sort of reversible.  Given a variable $x$ with a name not defined in the environment or domain of expression $e$ (and just to be safe, not used in the expression either), we can transform $g()$ into
$$\AggSum\left([dom(g())], (x \gets \{anything\}) * g()\right)$$
Here, $\{anything\}$ can in fact be replaced by anything with an empty input and output domain.

In particular, this allows us to perform factorization over variable definitions.  For example
$$\AggSum\left([dom(f(x)) - x], (x \gets y) * f(x)\right) + g()$$
$$\AggSum\left([dom(f(x)) - x], (x \gets y) * f(x)\right) + \AggSum\left([dom(g())], (x \gets y) * g()\right)$$
$$\AggSum\left([dom(f(x)) - x], ((x \gets y) * f(x)) + ((x \gets y) * g()) \right)$$
$$\AggSum\left([dom(f(x)) - x], (x \gets y) * (f(x) + g()) \right)$$

In other words, we can lift definition terms as far up through summation terms as we like, as long as there are no variable naming conflicts in the terms we distribute across.  Once a definition term is at the very top, we can perform downward unification as desired.

\subsection{Code-Level Optimizations}
Code generated by DBToaster goes through two independent optimization stages: (1) Factorization and Materializaton, and (2) Structural Rewriting.  The first stage decides how to factorize each invocation and what components of the invocation can be extracted as memos.  The second stage focuses on optimizing the evaluation of a given invocation.

Calculus provides a clean and simple abstraction for implementing extraction and  delta processing, making it ideal for the first optimization stage.  However, (1) we express update statements as an abstraction around calculus and (2) we express the process of initializing a memo as a black box nested in the memo term.  These changes allow us to construct a calculus without (1) imperative blocks or (2) null values or if-then-else forks.

Once the appropriate memos have been extracted and delta computation has been completed, further optimization options are available.  Each invocation is rewritten into a hybrid imperative/functional language: K3.  K3 is focused around functions applied to sets.  Functions appear in K3 for the following reasons:
\begin{itemize}
\item \AggSum terms are translated into a collection operator function (sum) that iterates over each element of each group.
\item {\tt Prod} terms are translated into functions: Multiplication by a set produces a map operation that maps each LHS set element onto a RHS set which is subsequently flattened, Multiplication by a number produces a map operation, and Multiplication by a condition produces a filter operation \todo{Filtering is not yet implemented correctly, it is implemented as a map operations returning the result of an IfThenElse0}
\end{itemize}

The following optimizations are performed on the resulting K3 expression:

\parsection{Function Composition}
Functions successively applied to the output of the next are composed into a single operation.  

Function composition is equivalent to adjusting the parenthesization of the calculus expression.  The advantage to implementing it in the K3 optimizer is that it simplifies the translation from calculus to a language closer (slightly at least) to the bare metal.  

A similar effect could be achieved by establishing certain design patterns (e.g., $(a < 0) * (b < 0)$ is translated into a filter of the conjunction of the two terms, rather than a two-stage filter), but the simpler way reduces bugs and has a significant impact when combined with the remaining rewrite rules.

\parsection{Function Inlining}
Functions applied directly to a value rather than being stored in a variable (more generally, if we can assert that the function is defined and applied only to a single code block) are inlined.  The function and associated application are replaced by the function with its variable replaced by the parameter.

This step could theoretically be avoided; Its primary purpose is to enable a more intuitive translation from Calculus to K3.  Rather than establishing all of the bindings from input to output variables by hand, we generate a large number of functions (one per product term) and then rely on the optimizer to simplify the generated expression down as far as possible.

\parsection{If-Lifting}
IfThenElse statements (not IfThenElse0s) are lifted as high in the syntax tree as possible.  Operations (i.e. Apply, Flatten, Function definition, etc...) with an IfThenElse node as a child switch places with the IfThenElse node.  The operation is applied to both the IfThenElse node's Then and the Else children, and the IfThenElse node becomes the new parent.

This swap is not always possible.  In particular, an IfThenElse can not be lifted so far that a variable in its If clause goes out of scope.  Because variables are defined exclusively by Function nodes, the IfThenElse node is lifted up to the highest possible function node in the syntax tree.  Having the conditional this far up (combined with the Inlining and Composition optimizations) ensures that if statements do not occur in deeply nested loops.

IfThenElse nodes appear exclusively as the result of map accesses with non-zero initializers.  Thus, this optimization has two roles: (1) a parenthesization optimization akin to what Function Composition does, and (2) un-nesting multiple levels of initializers.

The parenthesization optimization is the primary effect of if-lifting.  In particular, when dealing with the cross-product of two maps.  For example:
$$M_1[a][b] * M_2[c][]$$
In this expression, both $M_1$ and $M_2$ will need to be initialized, but without some messy factorization code the initializer for $M_2$ will be tested for, for each value of $b$ in $M_1$.   If lifting allows us to perform the test only once, effectively factorizing the expression.

Un-nesting multiple levels of initializers is of use, as the inner initializers are likely to be simpler.  If the inner initializer depends only on variables declared outside of the outer initializer (and fewer variables than the outer initializer as well) it may be possible to lift its initializer further up than the outer initializer, again reducing the if-clauses in an inner loop.  Note however, that DBToaster's output has a maximum initializer depth of 1, and there is little reason to think that it will ever need anything deeper.

\parsection{Block-Lifting}
\todo{Block-Lifting is not implemented}

Like If-Lifting, lift the terminal expression(s) (i.e., those evaluating to unit) of a block as far up the syntax tree as possible.  A terminal expression can not be lifted out of the scope of a variable it uses, and if lifted past an IfThenElse, the IfThenElse must be split.  For example:
$${\tt If}\ A\ {\tt Then}\ f(x) ; B\ {\tt Else}\ C \rightarrow ({\tt If}\ A\ {\tt Then}\ f(x) \ {\tt Else}\ UNIT) ; ({\tt If}\ A\ {\tt Then}\ B\ {\tt Else}\ C ) $$

Like If-lifting, performing a task every loop is inefficient.  The higher the expression can be lifted, the better.  

\todo{I suspect however, that we're actually doing as well as we can do.  There shouldn't be variables used in the then/else blocks of an IfThenElse which aren't used in the condition clause, since the initializer uses only those variables that appear in the membership test.  So, if the IfThenElse can't be lifted further up, the block probably won't be liftable either.}


\parsection{If-Factorization}
\todo{If-Factorization is not implemented}

Replace IfThenElse and IfThenElse0 blocks where the then and else clauses are equivalent or where the if clause is guaranteed to be true or false.  

\parsection{Block-Factorization}
\todo{If-Factorization is not implemented}
Merge IfThenElses in a single block with identical if clauses.  Merge functions being applied to the same value in a single block into one function with a single block.  For example.

$${\tt Apply}(\lambda x.f(x), A) ; {\tt Apply}(\lambda x.g(x), A) \rightarrow {\tt Apply}(\lambda x.(f(x);g(x)), A)$$


\subsection{Algorithms}
\ \\

\begin{algorithm}
\caption{compile\_invocation($definition$, $schema$}

This is the first of two mutually recursive steps in (and entry points into) the compilation process.  

The utility function {\tt make\_memo} constructs a memo by invoking {\tt compile\_memo}, assigns it a unique name, appends it to the returned list of memo $references$, and returns an {\tt External} reference to the memo.  

{\tt Neg}-ated terms are omitted below; Negations are assumed to be pushed to the bottom by simplify.  However

\begin{algorithmic}
\REQUIRE{calc\_t $definition$, schema\_t $schema$}
\ENSURE{calc\_t $implementation$, list of memo\_t $references$}
\STATE $monomials \gets {\bf monomialize}({\bf simplify}(definition))$
\STATE $impl \gets {\tt Const(0)}$
\FORALL{$m\_term \in monomials$}
  \STATE $factors \gets {\bf match}\ m\_term {\bf with} Prod[factors]$
  \STATE $groups \gets \emptyset$
  \STATE $consts \gets {\tt Const(1)}$
  \STATE $comparisons \gets {\tt Const(1)}$
  \FORALL{$factor \in factors$}
    \STATE Find a set $\{g_i\} \subseteq groups$ s.t. $\{g_i\}$ contains all elements of $groups$ that produce an output variable $v \not \in schema$ that appears (as an input or output variable) in $factor$.
    \IF{$|\{g_i\}| = 0$}
      \IF{$\left(factor\ {\bf matches}\ (i \gets f)\right)\ {\bf and}\ \left(f\ {\bf includes}\ {\tt Rel(*)}\right)$}
        \STATE $consts \gets consts * (i \gets {\tt make\_memo}(f))$
      \ELSIF{$factor\ {\bf matches}\ (i \gets f)\ {\bf or}\ {\tt Value(*)}\ {\bf or}\ {\tt Cmp(*,*,*)}$}
        \STATE $consts \gets consts * factor$
      \ELSE
        \STATE $groups \gets groups {\bf @} factor$
      \ENDIF
    \ELSE
      \IF{$\left(factor\ {\bf matches}\ {\tt Cmp(v_1, *, v_2)}\right) {\bf and}$\\ $\left(v_1\ {\bf or}\ v_2\ {\bf matches}\ {\tt Value(Const(*))}\ {\bf or}\ {\tt Value(Var(k))}\right.$ s.t. $\left.k\in schema\right)$}
        \STATE $comparisons \gets comparisons * factor$
      \ELSE
        \STATE $groups \gets (groups - \{g_i\}) {\bf @} (g_0 * \ldots * g_n * factor)$
      \ENDIF
    \ENDIF
  \ENDFOR
  \STATE $impl \gets impl {\bf +} \left(consts * \left(\prod_{g_i \in groups} {\tt make\_memo}(g_i)\right) * comparisons\right)$
\ENDFOR
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{compile\_memo($definition$, $schema$}

This is the second of two mutually recursive steps in (and entry points into) the compilation process

\begin{algorithmic}
\REQUIRE{calc\_t $definition$, schema\_t $schema$}
\ENSURE{memo\_t $memo$, memo\_t list $supplements$}
\FORALL{$R \in $ relations appearing in $definition$}
  \STATE $(memo.deltas[R], supplements_i) \gets {\tt compute\_delta}(R, definition)$
\ENDFOR
\STATE {\bf return} ($memo$, {\bf concat}($supplements_i$))
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{compute\_delta($R$, $term$}
A self-recursive process that computes the delta of a calculus expression.  In order to guarantee that the generated expression is simpler, the delta construction process may create additional memos which represent nested definition terms.  

\begin{algorithmic}[1]
\REQUIRE{relation $R(var_i)$, calc\_t $term$}
\ENSURE{calc\_t $delta\_term$, memo\_t list $supplements$}
\STATE ${\bf match}\ term\ {\bf with}$
\STATE $|\ {\tt Sum}[x_i] \rightarrow Sum[{\tt compute\_delta}(x_i)]$
\STATE $|\ {\tt Prod}[x_0, x_i] \rightarrow$\\ \hspace*{0.5in} $(d_0,supp_0) \gets {\tt compute\_delta}(x_0)$\\ \hspace*{0.5in} $(d_{rest},supp_{rest}) \gets {\tt compute\_delta}({\tt Prod}[x_i])$\\ \hspace*{0.5in} $delta\_term \gets x_0 * d_{rest} + d_0 * {\tt Prod}[x_i] + d_0 * d_{rest}$\\ \hspace*{0.5in} $supplements \gets supp_0 {\bf @} supp_{rest}$
\STATE $|\ {\tt Neg}(x) \rightarrow {\tt Neg}({\tt compute\_delta}(x))$
\STATE $|\ {\tt Cmp}(\_,\_,\_) \rightarrow (delta\_term,supplements)\gets (0, \emptyset)$
\STATE $|\ {\tt AggSum}(x) \rightarrow {\tt AggSum}({\tt compute\_delta}(x))$
\STATE $|\ {\tt Rel}(name,termvar_i) \rightarrow $\\ \hspace*{0.5in} $supplements \gets \emptyset$\\ \hspace*{0.5in} $delta\_term \gets{\bf if}\ name = R\ {\bf then}\ \prod_i (termvar_i \gets var_i)\ {\bf else}\ 0$
\STATE $|\ {\tt External}(\_) \rightarrow {\bf fail}$
\STATE $|\ {\tt Definition}(v \gets x) \rightarrow $\\ \hspace*{0.5in} $(x',supplements) \gets {\tt make\_memo}(x)$\\ \hspace*{0.5in} $delta\_term \gets ((v \gets (x' + {\tt compute\_delta(x)})) - (v \gets x'))$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{factorize($monomials$, $ivars$)}
Factorize terms from several monomial expressions back into a single term.
Works by a greedy heuristic, minimizing the number of sum terms between where a variable is defined and where it's used. 
\begin{algorithmic}[1]
\REQUIRE A list of $calc\_t$ $monomials$
\ENSURE A factorized $calc\_t$ expression equivalent to the sum of $monomials$
\STATE $candidates \gets \{\}$
\FORALL{$m \in monomials$}
  \STATE $candidateCosts \gets []$
  \FORALL{$t \in m$}
    \STATE $prev \gets $ everything in $m$ to the left of $t$
    \STATE $rest \gets $ everything in $m$ to the right of $t$    
    \IF{$(t\ {\bf commutesWith}\ prev) \wedge (t \not \in candidateCosts)$}
      \STATE $cost \gets {\bf costMetric}(t, rest)$
      \IF{$cost < \infty$}
        \STATE $candidateCosts \gets \{t:cost\}$
      \ENDIF
    \ENDIF
  \ENDFOR
  \FORALL{$\{t:cost\} \in candidateCosts$}
    \IF{$\{t:oldcost,oldcount\} \in candidates$}
      \STATE $candidates \gets \{t:oldcost+cost, oldcount+1\}$
    \ELSE
      \STATE $candidates \gets \{t:cost, 1\}$
    \ENDIF
  \ENDFOR
\ENDFOR
\IF{$max_{count}(candidates) = 1$}
  \RETURN $\sum monomials$
\ELSE
  \STATE $best \gets {\bf argmax}_{count}({\bf argmin}_{cost}(candidates))$
  \STATE $(withTerm,withoutTerm) \gets {\bf split}(({\bf contains}\ best),monomials)$
  \STATE $withTermRemoved \gets {\bf map}(({\bf removeTerm}\ best), withTerm)$
  \RETURN $(\sum {\bf factorize}(withoutTerm)) + (best * ({\bf factorize}(withTermRemoved)))$
\ENDIF
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{costMetric($term$, $rest$)}
In general, we want to factorize as aggressively as possible (to the limits of unification (i.e. propagate forward).  There are a few cases where this is a bad idea.  In particular, consider the following example:
$$(R(a,b) * (a > 5) * f(b)) + (R(a,b) * g(b))$$
In this example, it may not be a good idea to factorize, depending on what sorts of intermediate representations get materialized -- If we factorize out the $R$, we may end up needing to materialize $|R(a,b)|$ instances of $f(b)$, even if most of them are zero.  \todo{Is this really true?  The K3 optimizations should be able to pull up the if statement around the comparison.  Otoh, it may be more efficient to just materialize $R(a,b)*(a > 5)$ and separately $R(a,b)$}. 

This is an {\em example} cost metric that blocks any attempt at factorizing a relation which is subsequently filtered.
\begin{algorithmic}
\STATE $vars \gets {\bf outputSchemaOf}(term)$
\FORALL{$t \in rest$}
  \IF{${\bf match}\ t\ {\bf with}\ {\tt Cmp(a,op,b)}$}
    \IF{$a \in vars$ {\bf or} $b \in vars$}
      \RETURN $\infty$
    \ENDIF
  \ENDIF
\ENDFOR
\RETURN $0$
\end{algorithmic}
\end{algorithm}

%\begin{algorithm}
%\caption{factorize($monomials$)}
%Factorize terms from several monomial expressions back into a single term.  Works by a greedy heuristic (though it is possible to substitute something more complex in) -- the goal is to merge terms as extensively as possible, shifting plusses as deep in the factor tree as possible.  An intermediate {\tt factor\_tree} representation of the single term is used: 
%{\tt \noindent factor\_tree = (term:calc\_t) * (children:factor\_tree list)}
%\noindent Every node in this tree represents a single calc\_t.  A forest of such trees represents a sum of terms.  Thus, the calc\_t defined by the node is the product of the {\tt term} and the (parenthesized) sum of all of the children.  The {\tt term} {\bf must} be a monomial.
%\begin{algorithmic}[1]
%\REQUIRE{A calc\_t (as above) list $monomials$}
%\ENSURE{A factor\_tree list $forest$, which can be converted into a single polynomial with the following {\tt merge\_forest} function}
%\STATE $count_{*} \gets 0$
%  \COMMENT{Greedy heuristic: find the most common term}
%\FORALL{$m \in monomials$}
%  \FORALL{unique $term \in m$}
%    \STATE{$count_{term} \gets count_{term} + 1$}
%  \ENDFOR
%\ENDFOR
%\STATE $common\_term \gets {\tt argmax}_{term}(count_{term})$
%\IF{$count_{common\_term} \leq 1$}
%  \STATE $forest \gets \emptyset$
%    \COMMENT{No commonality between monomials -- return what we have}
%  \FORALL{$m \in monomials$}
%    \STATE $forest \gets forest \cap \{({\tt term}:m, {\tt children}:\emptyset)\}$
%  \ENDFOR
%\ELSE
%  \STATE $in\_terms \gets \emptyset$; $out\_terms \gets \emptyset$
%    \COMMENT{Split the monomials by presence of common term}
%  \FORALL{$m \in monomial$}
%    \IF{$common\_term \in m$}
%      \STATE $in\_terms \gets m$ {\bf else} $out\_terms \gets \frac{m}{common\_term}$
%    \ENDIF
%  \ENDFOR
%  \STATE $new\_tree \gets ({\tt term}:common\_term, {\tt children}:{\tt factorize}(in\_terms))$
%  \STATE $forest \gets {\tt factorize}(out\_terms) \cap \{new\_tree\}$
%\ENDIF
%%\REPEAT
%%  \STATE $derive\_candidates$
%%\UNTIL{}
%\end{algorithmic}
%\end{algorithm}
%
%\begin{algorithm}
%\caption{merge\_forest($forest$)}
%\label{alg:dbtoaster:mergeForest}
%\begin{algorithmic}
%\REQUIRE{A factor\_tree list $forest = {term_i, children_i}$}
%\ENSURE{$polynomial$, the calc\_t representation of $forest$}
%\STATE $polynomial \gets \sum_i term_i * {\tt merge\_forest}(children_i)$
%\end{algorithmic}
%\end{algorithm}


\end{document}  