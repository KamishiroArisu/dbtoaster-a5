\section{Query Compilation}

\begin{itemize}
  \item \todo{Need some kind of overview that relates calculus terms to
  incremental programs, i.e. an overview of how calculus can be evaluated
  incrementally by materializing intermediates. Should also give intuition
  about the problems of initial values.}
\end{itemize}

\subsection{Trigger Programs}

\begin{itemize}
  \item We first describe the outputs of our compilation process, a simple
  language for representing trigger programs, where triggers are executed on
  any tuple insertion or deletion arriving on an input stream. 
\end{itemize}

\def\mtins{\mbox{on\_insert}}
\def\mtdel{\mbox{on\_delete}}
\begin{align*}
e \; \mbox{::-} & \;
  c \;|\; x \;|\;
  m[\vec{x}][\vec{y}]\tuple{init}
\comment{
  \;|\; e[\vec{w}][\vec{x}] * e[\vec{y}][\vec{z}]
  \;|\; e[\vec{w}][\vec{x}] + e[\vec{y}][\vec{z}]
\\
& 
}
\comment{
  \;|\; e[\vec{w}][\vec{x}] \;\{+,*,-\}\; e[\vec{y}][\vec{z}]
  \;|\; e[\vec{x}][] \;\theta\; 0 \; ? \; e[\vec{y}][\vec{z}] :0
}
  \;|\; e + e \;|\; e * e \;|\; -e
\\
& 
  \;|\; e \;\theta\; 0 \; ? \; e : 0
  \;|\; \calcsum(\vec{x}, e)
\\
stmt \; \mbox{::-} & \; m[\vec{x}][\vec{y}]\tuple{init}
     \; \mbox{{\tt+}=} \; e\\
init \; \mbox{::-} & \; m[\vec{x}][\vec{y}]\comment{\tuple{init}}
     \; := \; e\\
trig \; \mbox{::-} & \; \mtins(R,\vec{v},\vec{stmt}) \;|\;
\mtdel(R,\vec{v},\vec{stmt})
\end{align*}

\begin{itemize}
  \item Core components of language: expressions, increment statements,
  initialization statements, and triggers.
  
  \item Statement ordering, update of old values only, atomic execution.
  
  \item Here, expressions are essentially calculus terms that we saw in the
  previous section, with one core difference: the basic atomic element here is
  a map datastructure, rather than a relation. Maps have input and output
  variables, corresponding to the first and second arguments of the finite
  functions we saw in the previous section, that is bound variables, and free
  variables. Note that a map can represent the materialization of any arbitrary
  calculus term, and not just relations. These each map atom contains an
  initializer expression in addition to input and output variables. We return
  to these initializers below.

  \item That is to say, the aggregate calculus is closed under deltas, that is
  the delta queries are expressed with the same language fragment as our original
  queries, especially when considering nested queries, whose deltas are no
  simpler than the original query. We return to this issue when discussing our
  compilation algorithm, particularly from the context of ensuring termination
  of our compilation algorithm.
  
  
  \item Note, not all expressions are technically two-level finite functions,
  e.g. constants ignore their bound variable arguments, thus is total function
  w.r.t bound variables, and similarly for variables, constraints, assignments
  and the operators. In fact, only those calculus expressions involving
  relations yield two-level finite functions, due to the consistency
  requirement between bound variables and the gmr.
  \todo{Actually this requirement is not sufficient for finiteness, since we
  can have an infinite number of bound variable tuples that are consistent,
  given consistency does not impose any restrictions on the arity of the tuple.}
  That is expressions involving only constants, variables and constraints, if
  they are safe in terms of range restriction, are total functions that can
  directly be represented as code, without any data structures.
\end{itemize}

\tinysection{Statement Semantics and Evaluation}

\begin{itemize}
  \item Map lookup semantics and description of loops: all maps have a defining
  calculus expression, with the type of the definition being a two-level ffn of
  bound and free variables. The brackets $m[\vec{x}][\vec{y}]$ represents an
  evaluation of the map (definition) expression, with arguments $\vec{x},
  \vec{y}$, however since we have materialized the expression with a map, this
  corresponds to a map lookup with the given arguments.
  
  \item Map evaluation (i.e. lookup) may be \textit{partial} in both its
  arguments, yielding a two-level ffn itself. That is given a trigger's bound
  variables $\vec{b}$, there may be map lookup arguments that are not bound, i.e.
  $\vec{b} \subset \vec{x} \cup \vec{b}$, and
  $\vec{b} \subset \vec{y} \cup \vec{b}$.
  Partial evaluation for a map's bound or free variables corresponds to
  enumerating valuations for the variables from the map's active domain for
  the variables (the two-level ffn yields zero outside the active domain).
  
  \item Partial map evaluation occurs because in vars arise as a result of
  specializing a calculus expression with a delta. Naturally we must be able to
  restrict the range of in vars when evaluating delta expressions, however, it is
  not necessarily the case that each in var will be bound by the trigger, since
  triggers correspond to updates on a specific relation, thus triggers bind
  different sets of variables. In cases where in vars are not bound by trigger
  vars, we perform range restriction based on the active domain of the in var,
  and we refer to this as partial map evaluation.

  \item We have a few options on how to perform this enumeration, or looping,
  over maps, given how maps are partially evaluated. \todo{Can the same LHS loop
  in var appear multiple times on the RHS? This boils down to a question of
  factorization, and whether bound variables can appear in both factors. Since
  bound variables are essentially singleton constants, thus should be consistent
  across both factors, this is safe. However our implementation does not support
  this currently, factorization takes no consideration of bound/free vars. E.g.
  the following should be fine, but our compiler yields the first delta
  expression:
  \begin{align*}
  \Delta_{S(XY)} & \calcsum(R(AB) * S(CD) * T(EF) * A * F \\
  & \qquad * B<C * C<E)\\
  & := \calcsum( R(AB) * T(EF) * A * F \\
  & \qquad * B<X * X<E)\\
  & := \calcsum( R(AB) * A * B<X) \\
  & \qquad * \calcsum(T(EF) * F * X<E)\\
  & := m1[X][] * m2[X][]
  \end{align*}
  }
  \note{Let's move forward with the assumption that we can do this kind of
  factorization, thus in vars can appear in multiple maps. Next part is can
  this occur for loop in vars? Sure, all LHS vars are loop vars by default,
  unless there is an equality constraint present against a trigger var, but in
  queries consisting of only range predicates, this will never happen, as with
  the above example when updating either of the two factors (i.e. +R or +T).}

  \item Given multiple occurrences, we then have the issue of determining which
  map to use for enumerating valuations of a loop in var. Note that loop in vars
  must appear as a map parameter on the LHS, thus for multiple occurrences, we
  may use the LHS map for valuations, as in our current implementation.
  Alternatively we can take the intersection of all RHS maps' active domains,
  however computing this intersection may be more costly than simply looping over
  the LHS map. Consider $RST$, where $m1=sum(d, S * T * B=S.b * S.c=T.c)$, and
  $\delta T \rightarrow $\texttt{m1[B][] += d*m2[B,xC][]}. This has a loop in-var
  of $B$, and currently we would loop over the existing domain of $B$ in $m1$,
  whereas we could simply use the existing domain in $m2$. In addition to
  yielding more sparsely populated maps, this would result in fewer failed
  lookups, and thus IVCs. Thus this is simply an optimization for the case where
  the RHS contains a single map, which may still be useful since many statements
  may consist of a single in case we cannot factorize. Ideally though, we would
  have a cost model of the cardinality of the intersection and subsequently
  valuation enumeration cost.
  
  \item Does this also apply to out vars? We should not be able to have
  multiple occurrences a var as out vars, since all vars should be unique. Note
  of course an out var can be propagated, but it's remaining occurrences would
  be as in vars.

  \item The remaining components are constructs for incremental evaluation.
  Incremental statements indicate updates applied to maps from the result of an
  expression. \todo{Given $e$ returns a two-level ffn, what are the semantics
  of += for incrementing? Is this also schema-less union, and if so, what are
  the variables $x,y$ appearing with the LHS map, the ``type'' of this
  expression $e$? Similarly, what are the semantics for assignment in IVC? This
  is related to what we decide to materialize, since these are always
  $\calcsum$, there is a single-value interpretation, w/ loops as necessary
  given the presence of bound group-bys, etc.}
  
  \item The {\tt +}= operator in an increment statement is not a
  schema-less union, since it sums up over (a sequence of) updates, each update
  resulting in a ffn with the same schema. It is a union, due to the possibility
  of new values changing the domain of the LHS ffn, although with its current
  syntax, the focus is on updating the value of the ffn. Furthermore, given
  what the kinds of expressions we extract (aggregates), the update reflects a
  new aggregate value (potentially for groups in the case of a group-by
  aggregate).
  
  \item \note{For a statement, the in-vars of the RHS maps may be a superset of
  the LHS in-vars, with the additional vars being trigger vars, thus bound
  (otherwise the calculus expression would not be range restricted).}
  
  \item \note{The overall inconsistency right now with claiming the RHS as
  standard calculus terms is that we have a fully defined semantics for two-level
  ffns, unlike our implementation, which always assumes in vars are bound (by
  enumerating in vars), and the result of the RHS is a one-level ffn (i.e. an
  out-slice). I am trying to resolve this inconsistency by determining if we can
  design a fully two-level implementation, which specifically avoids this step of
  binding all in-vars prior to evaluating the incremental RHS.}

  \item \note{It would be worthwhile writing down the semantics of {\tt +}= in
  terms of unseen LHS values and IVC, vs. increments.}
  
  \comment{
  \item Motivate grammar by explaining the above annotated form indicates
  everything can be represented as maps, thus we can simplify our evaluation
  framework to one that applies sum/products/conditionals to maps alone.
  Furthermore during compilation, the delta queries produced are from the same
  closed language as our input queries. Thus the above grammar can capture
  queries, but we add a couple of extensions for incremental evaluation, namely
  the statement and trigger constructions. This is the output of our
  compilation, and known as M3.
  \item Triggers + statements anatomy (lhs maps, loop variables, bigsum
  variables).
  }
  
  \item \todo{Explain why we never need to sum slices, and only multiply
  slices, thus we only have joins and products and never schemaless unions.}
  
  \item Propagation, and relationship to hash join build and probe. Lack
  of propagation as cross product, briefly mention how we return to this when
  discussing our code generation and calculus expression evaluation in the next
  section.

  \item Bigsum evaluation, and late projection/aggregation. This too can be
  optimized as we discuss in the next section.
\end{itemize}

\comment{
\tinysection{Map Datastructures}

\begin{itemize}
  \item Two-tiered maps
  \item API, secondary indexes for loop variables.
\end{itemize}

\tinysection{Statement Semantics and Evaluation}

\begin{itemize}
  \item Slice access for loop variables, using secondary indexes.
  \item Inlining of non-slice expressions to simple arithmetic expressions.
  Example?
  \item Note that we never need to evaluate sums on \textit{maps} due to
  monomialization, which recursively ensures map sums are expanded out into
  polynomials. Thus any remaining sum operations apply to (sum) aggregates.
  \item Product evaluation as a core evaluation primitive, can be thought of as
  join/cross product evaluation based on presence of propagated variables.
  \item \todo{What is the output schema from product evaluation? How does this
  depend on transitive propagation? We are strict on propagation, but we could
  also ``push'' down RHS$\rightarrow$LHS projection+aggregation (subject to
  vars used in RHS constraints, but not part of the LHS outvars) as part of
  slice access, rather than at the very end once joins have been computed.}
  \item Product evaluation to compute conditionals, 0-1 conditional slices.
  \item Bigsums and product evaluation to loop over the active bigsum variable
  domain. Short-circuiting in light of bigsums to avoid unnecessary
  (potentially complex and expensive) then-clause evaluation.
  \item Projection of rhs expression schema prior to applying update to lhs map.
  \item LHS map merge and update.
}

\tinysection{Trigger program example}

\begin{itemize} 
  \item Vwap evaluation example.
  \begin{itemize}
    \item \todo{Indicate partial map evaluation (slicing), propagation (joins
    vs. cross product), singleton expressions (i.e. total functions)}
  \end{itemize}
  
  \item \todo{Give a simpler example of a trigger program, e.g. self-join on
  customers before this complete example. This could be moved to after the
  compilation trace.}
\end{itemize}

\begin{verbatim}
// initial values are 0 if omitted
on_insert_bids(p, v) {
  // q1     = sum v from bids
  // q2(p2) = sum v from bids where p > p2
  // q3     = sum p*v from bids group by p
  // q4     = sum v from bids group by p

  q[][] += 4*q2[p][]<q2_i1> - q1[][] > 0? p*v : 0
  
  q[][] += sum_d
      if 4*q2[d][]<q2_i2> - q1[][] <= 0 then
      (if 4*(q2[d][]<q2_i2>+(d < p ? v : 0))
          - q1[][]+v > 0
       then q3[][d] else 0) else 0
  
  q[][] += sum_d
      if 4*q2[d][]<q2_i2> - q1[][] > 0 then
      (if 4*(q2[d][]<q2_i2>+(d < p ? v : 0))
          - q1[][]+v <= 0
       then q3[][d]*-1.0 else 0) else 0
  
  q[][] += if 4*q2[p][]<q2_i1> - q1[][] <= 0 then
      (if 4*q2[p][]<q2_i1> - q1[][]+v > 0
       then p*v else 0) else 0
  
  q[][] += if 4*q2[p][]<q2_i1> - q1[][] > 0 then
      (if 4*q2[p][]<q2_i1> - q1[][]+v <= 0 
       then p*v*-1.0 else 0) else 0

  q1[][] += v
  q2[d][]<q2_i2> += p > d ? v : 0 
  q3[][p] += p*v
  q4[][p] += v
  
  q2_i1: q2[p][] := sum_c c > p ? q4[][c] : 0
  q2_i2: q2[d][] := sum_c c > d ? q4[][c] : 0
}
\end{verbatim}

\subsection{Initial value computation}

\begin{itemize}
  \item Initializers are non-zero only in the presence of input variables in a
  map definition expression. Initialization is thus computing a new map entry
  for a bound variable whose value we've not seen before.

  \item In IVC, these in vars must be bound, yielding a specialization of the
  definition query that we must evaluate.
    
  \item This input/bound variable comes from another relation, whose delta
  we've taken at some prior stage of compilation. This can be from multiple
  levels above, since LHS in vars can appear on the RHS, e.g.:
  
  \begin{align*}
  & q[][] \; =
  \calcsum(R(AB)*S(CD)*T(EF)*U(GH) \\
  & \qquad * A * H * B<C * B<E * F<G  * B<H) \\
  \Delta_{R} & \; q[][] = xA*\calcsum(S(CD)*T(EF)*U(GH)*H \\
  & \qquad * xB<C * xB<E * F<G * xB<H)\\
  & = xA * m1[xB][]\\
  \Delta_{S} & \; m1[B][] = \calcsum(T(EF)*U(GH)*H \\
  & \qquad * B<xC * B<E * F<G * B<H)\\
  & = m2[B,xC][]\\
  \Delta_{T} \; & m1[B][] = \calcsum(S(CD)*U(GH)*H\\
  & \qquad * B<C * B<xE * xF<G * B<H)\\
  & = m2.2[B,xE,xF][]\\
  \Delta_{U} \; & m1[B][] = \calcsum(S(CD)*T(EF)\\
  & \qquad * B<C * B<E * F<xG) * B<xH * xH \\
  & = m2.3[B,xG][] * B<xH * xH \\
  \Delta_{T} & \; m2[BC][] = \calcsum(U(GH)*H \\
  & \qquad * B<C * B<xE * xF<G * B<H) \\
  & = \calcsum(U(GH)*H \\
  & \qquad * B<C * B<xE * xF<G * B<H)\\
  & = m3[B,C,xE,xF][]\\
  \Delta_{U} & \; m3[BCEF][] = \\
  & xH * B<C * B<E * F<xG * B<xH
  \end{align*}

  Above, B is an input variable for map $m3$, but is passed through multiple
  levels of deltas due to the join conditions. \note{(Note this query currently
  fails in our implementation, and should be debugged since it is perfectly
  valid.)}
  
  \item The above is using our currently implemented factorization technique,
  which is not quite aggressive enough w.r.t bound variables, otherwise the
  derivation would be:

  \begin{align*}
  & q[][] \; =
  \calcsum(R(AB)*S(CD)*T(EF)*U(GH) \\
  & \qquad * A * H * B<C * B<E * F<G  * B<H) \\
  \Delta_{R} & \; q[][] = xA*\calcsum(S(CD)*T(EF)*U(GH)*H \\
  & \qquad * xB<C * xB<E * F<G * xB<H)\\
  & = xA * \calcsum(S(CD) * xB<C) * \\
  & \qquad \calcsum(T(EF)*U(GH)*H\\
  & \qquad \quad * xB<E * F<G * xB<H)\\
  & = xA * m1[xB][] * m2[xB][]\\
  \Delta_{S} & \; m1[B][] = B<xC\\
  \Delta_{T} & \; m2[B][] \\
  & = \calcsum(U(GH)*H * xF<G * B<H) * B<xE\\ 
  & = m3[B,xF][] * B<xE\\
  \Delta_{U} & \; m2[B][]\\
  & = \calcsum(T(EF) * B<E * F<xG) * xH * B<xH\\
  & = m4[B,xG][] * xH * B<xH\\
  \Delta_{U} & \; m3[BF][] = xH * F<xG * B<xH\\
  \Delta_{T} & \; m4[BG][] = B<xE * xF<G  
  \end{align*}

  \item \note{We've observed that we can do initial value computation by
  exploiting other map entries, i.e. ``neighboring'' in var map entries, but
  never explicitly formalized this, nor factored it into data structure
  construction. For example, we could build an extremely compact data structure
  for neighboring in vars, if the map expression does not change (i.e. no
  difference in tuples satisfying the map definition between different in var
  values.)}
  
  \item Given a map definition expression, recursive compilation will
  create delta queries for each relation in the definition, to maintain the map.
  
  \item Let's consider lookup-based IVC first, i.e. where initializers are
  invoked as part of RHS evaluation. IVC is invoked whenever we encounter
  lookups that use trigger vars as map parameters, and entries corresponding to
  these trigger vars are not yet present in the map. This applies to map
  parameters that are either in or out vars. How does one-shot lookup-based IV
  computation differ in its evaluation plan from maintenance?
  
  \item Consider the above example. Suppose we are maintaining $q$ and we
  invoke IVC for $m1$. Since we are in a trigger for $R$, we have bound
  variables $xB, xC$. Recursive compilation instantiates maps to maintain $m1$
  in the event of deltas to the remaining relations, the question then becomes
  how can we supply the necessary bound variables for these maps to compute an
  initial value? We should be able to aggregate over the active domains of these
  maps.
  
  \item For example, to compute IVs of $m1$ using $m2$, we need trigger vars for
  $S$, specifically $xC$, but only have them for $R$. Suppose we have seen the
  relation $S$:
  \[S = \{\vec{s_1}, \vec{s_2} \ldots \vec{s_n}\}\]
  
  Given $B$ is an in var to the relation $m1$, we can define the current state
  of $m1$ for any value of $B$ as:
  
  \begin{align*}
  m1[B][] := \; & m2[B,\vec{s_1}(C)][] \\
  & + m2[B,\vec{s}_2(C)][]\\
  & + \ldots + m2[B,\vec{s}_n(C)][]
  \end{align*}
  
  Now, we are only interested in the initial value for the bound
  variable $xB$, yielding:
  
  \[m1[xB][] := \sum_C m2[xB,C][]\]

  Thus for IVC, we can simply change the {\tt+}= increment operator into an
  aggregation over the active domain of the dependent map, and ``flip'' in vars,
  i.e. binding $m2$'s loop in vars with the current trigger's vars, and looping
  over the other trigger's vars. Note this will recursively invoke IVC for $m2$,
  since it too has $B$ as an in var, and we will not have seen $xB$ for $m2$
  before either.

  The challenge here is then how do we obtain the values of $C$ that we are
  summing over? We cannot enumerate the active domain from the map $m2$
  directly, since no entries involving $xB$ are present in the map. Here we
  have two options, we can keep an auxiliary map for initialization maintaining
  the domain of $C$, or we can use the domain for a neighboring entry. The
  latter option requires that the active domain for $C$ is complete for a
  neighboring value to $xB$, thus we prefer the former option rather than
  enforcing this requirement for dense maps.
  
  This auxiliary map is essentially a preaggregated map, as we witnessed above
  when considering minimal width calculus expressions. That is rather than
  maintaining the base relation $S$, we can simply keep a count of $C$ values:
  
  \begin{align*}
  m1[xB][] :&= \calcsum(\tuple{C}, S(CD)) * m2[xB,C][]\\
  & = init1[][C] * m2[xB,C][]
  \end{align*}

  In addition to map accesses, the delta expression may contain residual
  constraints involving either input or trigger variables. We impose one
  restriction on the calculus expression defining the preaggregated map due to
  the fact we are performing initialization, namely we omit any terms containing
  input variables, otherwise this auxiliary map itself would require initial
  value computation. Overall, this approach both reuses maps from delta
  computation, in addition to restricting further creating initializers for the
  auxiliary maps (which otherwise could potentially never terminate due to the
  presence of input variables). For example consider the initializer for
  $m2[B][]$ based on the $\Delta_T$ expression. We create an initializer:
  
  \begin{align*}
  m2[xB][] :&= \calcsum(\tuple{EF}, T(EF)) * m3[xB,F][] * xB < E\\
  & = init2[][EF] * m3[xB,F] * xB < E 
  \end{align*}
  
  rather than
  
  \begin{align*}
  m2[xB][] :&= \calcsum(\tuple{EF}, T(EF) * xB<E) * m3[xB,F][]\\
  & = init'[xB][EF] * m3[xB,F]
  \end{align*}
  
  since the second would require initialization itself due to the presence of
  $B$ as an input variable.
  
  We can simply repeat this approach for $m2$, again bearing in mind the trigger
  (and thus bound vars) in which the initializer is invoked:
  
  \[m2[xB,C][] := \sum_{E,F} m3[xB,C,E,F][]\]
  
  
  Notice that here $C$ is bound from the lookup for $m2$ in computing the
  initial value for $m1[xB][]$. Further note we could compute the IV for $m1$
  as:
  
  \[m1[xB][] := \sum_{C,E,F} m3[xB,C,E,F][]\]
  
  but for consistency, we need to physically instantiate the initial value for
  $m2[xB,C]$. This is valid because $BC$ are loop in vars to $m3$, and thus their
  active domains are kept consistent with the active domains of the vars as they
  appear in $m2$. This illustrates another tradeoff in terms of performing
  initial value computation, where just as with incremental maintenance, we can
  trade off materialization overhead and computation time by using higher-order
  deltas and their maps.
  
  \note{Note that pure aggregations corresponding to marginalization, i.e.:
  $m1[A,B] \mbox{{\tt +}=} m2[A,B,C]$
  are very obvious candidates for avoiding materialization to reduce space
  overhead.}
  
  \item \note{It may be worthwhile pointing out this property of consistent
  active domains for loop in vars in the semantics section for the {\tt+}=
  operator.}
  
  \todo{The above technique only works if the map $m2, \ldots$ is maintained for
  the entire active domain of its loop in var $B$. This ensures a dense set of
  $B,C$ entry pairs. Interestingly this conflicts with the suggested loop
  optimization in the previous section. If we apply that, then we must explicitly
  enumerate the complete $C$ domain here as part of IVC.}

  \item Current implementation of constructing initializer for RHS map access:
  initial value computation with bigsums
  \begin{itemize}
    \item Create initializer from ``accessed'' definition: substitute map params in
    definition with params in RHS expr. \note{Note, this is the same as
    considering the context in which the initializer is invoked, as described
    above.}
    \item Create initial maps and ``bigsum'' expression: separate constraints
    from relations and vars in definition, creating an aggregate over the
    relations and vars as the initial map. This is essentially
    preaggregation, to create a minimal width expression of the base
    relation, as needed for the rest of the initialization expression.
    \item \note{This differs from standard extraction and recursive compilation
    applied in our algorithm, the extraction for initialization omits all
    constraints, not just nested ones. Why can't simple constraints be pushed
    into the initial maps? They can, and we do this with the revised approach for
    those terms without in vars (to ensure initial maps don't need complex
    initializers themselves). I believe this a bug with the current
    implementation.}
  \end{itemize}
  
  Constrast the above approach to our current implementation. The key
  difference is that we are constructing initializers that explicity reuse
  deltas, and do not need to perform additional recursive compilation to
  determine initialization expressions. The auxiliary data structure is a
  preaggregated map guaranteed to apply over a single relation, and can
  incorporate residual constraints that do not contain input variables.

  \item We have multiple options/plans for computing initial values
  recursively, based on the order of deltas available. Which one do we choose?
  
  \item \note{IVC and delta order: look at the maintenance of $m1$ for all
  deltas. The statement for $\Delta_{U}$ is the most interesting, it identifies the
  remainder expression that is not kept in a delta map with $B<xH*xH$. We could
  explicitly keep this in a map and then combine with $m2.3$ to compute initial
  values.}
  
  \todo{Why is this a good idea, given we'll have to keep one data structure
  anyway for $m2.3$? If we do IVC with $\Delta_{S}$ or $\Delta_{T}$ we don't
  need any additional datastructures \textit{at this stage}.}
  \note{If we keep the same map anyway for bottoming out, there may be a
  performance argument. The other plan may perform many more iterations since
  it applies to a higher dimensional map, whereas this one has already applied
  some aggregation. I think that in general there is a cost-based optimization
  to be done in terms of which delta expression to be used for initial value
  computation, but the cost model and the optimization algorithm lie outside
  the scope of this work.}
  
  \note{Putting aside the question of ordering and picking the optimal order,
  this indicates that the map we instantiate for bottoming out will be used
  multiple times, for different initializers. This is fairly straightforward,
  since we need initialization expressions for all maps, and through recursive
  initialization, whichever order we use for a high-level map will also use the
  same initialization expressions as the lower-level maps present in the order.
  For example, the remainder expression for initializing $m1$ with $\Delta_U$
  is the same as the map needed when bottoming out from the $\Delta_{S,T,U}$
  order.}

  \comment{
  \item \note{Idea: stitching: using recursive initialization, we must keep one
  additional map for each initializer when bottoming out. This will typically
  be because we do not have active domains for the last set of trigger vars.
  The idea of stitching is to use the active domain of another map, where the
  trigger vars are either input or output vars to supply valuations, avoiding
  the need to instantiate any additional data structures. Note this final
  expression contains no further maps, thus we can simply use the variable's
  domain from the map, and don't actually need the map value.}
  \todo{Which map do we take the domain from, and whether in the presence of
  other variables, is this domain a complete active domain for the variable?
  See the above comment about loop in var optimizations.}
  \note{Stitching sounds a little bit like view matching and determining
  calculus expression containment. I think we should defer this, since this
  could equally be applied at any stage of duplicate map detection and
  extraction, where we could attempt to match any extracted term to existing
  map terms, and perform any compensating computation.}
  }

  \item The following is a simpler example that can be used to illustrate
  initializers and residual expressions, but does not have the example of
  an input variable bound through multiple levels of compilation:
  
  \begin{align*}
  & q[][] = \calcsum(R(AB) * S(CD) * A * D * B<C)\\
  \Delta_{R} & q[][] = \calcsum(S(CD) * D * xB<C) * xA\\
  & = m1[xB][] * xA\\
  \Delta_{S} & q[][] = \calcsum(R(AB) * A * B<xC) * xD\\
  & = m2[xC][] * xD\\
  \Delta_{S} & m1[B][] = B<xC * xD\\
  \Delta_{R} & m2[C][] = xB<C * xA
  \end{align*}

  \note{Do we want to use this instead of the above example for readability?}

  \item The following is a variant of the above query that interestingly
  causes our current implementation to break, that I believe is due to
  propagated errors relating to factorization:
  \begin{align*}
  & q[][] = \calcsum(R(AB) * S(CD) * A * C * B<C)\\
  \Delta_{R} & q[][] = \calcsum(S(CD) * C * xB<C) * xA\\
  & = m1[xB][] * xA\\
  \Delta_{S} & q[][] = \calcsum(R(AB) * A * B<xC * xC)\\
  & = m2[xC][]\\
  \Delta_{S} & m1[B][] = B<xC * xC\\
  \Delta_{R} & m2[C][] = xB<C * C * xA
  \end{align*}


\end{itemize}

\subsection{Compilation Algorithm}

\begin{itemize}
  \item We now piece together all of the transformations presented in the
  previous sections into our query compilation algorithm.

  \item Algorithm overview: recursive compilation, involves taking deltas,
  simplifying and specializing based on deltas, and then extracting the next
  calculus expression for recursion. Additional aspects: initial value
  expressions, map deduplication, code generation, including map typing and map
  evaluation (potentially partial).
  \item \todo{Initializer expression construction}
  \item Duplicate map elimination description
  \item Extraction as selection of next compilation target. Should elaborate on
  this in a bit of depth, i.e. what are the range of options when considering
  extraction? How could we build up a cost-based/heuristic/extensible approach
  to extraction, beyond this outermost aggregate approach?
  \item Algorithm description
\end{itemize}

\def \alg         {{\bf algorithm}}
\def \algbegin    {{\bf begin}}
\def \algend      {{\bf end}}
\def \algforeach  {{\bf for each}}
\def \algdo       {{\bf do}}
\def \algdone     {{\bf done}}
\def \algreturn   {{\bf return}}
\def \algcomment#1{{\tt //} #1}

\def \codeforeach {{\tt for}}
\def \codev#1     {\mbox{{\tt #1}}}

\begin{figure}
\begin{tabular}{c|l}
Symbol   & Description\\
\hline
$\Gamma$ & Map (i.e. view) schemas and defining queries \\
$\pm$ & Event type (i.e. insert/delete) \\
$m[\vec{x}]$ & Map $m$ with keys $\vec{x}$\\
$\vec{t}$ & Trigger arguments \\
$\vec{b}$ & Bound variables (trigger arguments and map keys)\\
$\v{s}_i$ & Map update statement\\ 
triggers[$R$] & Per-relation trigger statement accumulator 
\end{tabular}

\begin{itemize}
\item \todo{Preaggregation} 
\item \todo{Bigsum extraction based on comparing degrees of delta terms}
\item \todo{Break down Simplify steps}
\item \todo{Don't show code for MakeStmt, just lhs map, lhs map args, delta expr}
\item \todo{Change $\Gamma$ as this is used for the variable environment}
\item \todo{Invoke SR code generation from annotated expression}
\end{itemize}

\begin{tabbing}
\alg\ Compile(\q: query, m: map name, $\vec{x}$: map keys) \\
\algcomment{returns triggers (a set of map update statements)}\\
\algcomment{for update events to relations in \q} \\
$\Gamma_{\q} := \Gamma$\\
\algforeach\ base relation $R$ in \q,
               $\pm$ in $\{\v{insert},\v{delete}\}$
\algdo \\
~~\= $\vec{t}$ := fresh variables for columns of $R$
     \algcomment{trigger args}\\
  \> $\vec{b} \; := \; \vec{t} \; \cup \; \vec{x}$
     $\qquad \qquad \qquad \qquad \qquad$ \algcomment{bound vars}\\
  \> $\v{\q}_{init}$ := MakeInitializer(\q)\\
  \> \algforeach\ $\v{q}_{m_i}$ in Monomialize($\Delta\v{q}$) \algdo\\
\>~~\= ($\partial\v{q}_i$, $\Gamma_i$) :=\=\ ExtractSimplerQuery($\vec{b}$,\\
  \>\>\> ~~ SimplifyQuery($\partial\v{q}_{m_i}$, $\vec{b}$))\\
  \>\> $\partial_{init} := $ MakeInitializer($\partial\v{\q} _i$)\\
  \>\> $\v{s}_i$ := \= EliminateLoops(MakeStmt(\\
  \>\>\> ~~\{\codeforeach\ $\vec{x} \in \codev{m} [\vec{x}]:$
  $\codev{m} [\vec{x}]\tuple{\v{\q}_{init}} \pm = $
  $\partial\codev{q} _{i}\tuple{\partial_{init}}$\}))\\
  \>\> triggers[$R$] := triggers[$R$] $\cup$ \todo{Annotate($\v{s}_i$)}\\
  \>\> $\Gamma := \Gamma \bigcup_i \Gamma_i$
  \ \ \ \ \algcomment{eliminates duplicate maps}\\
  \>\algdone\\
\algdone\\
\algforeach\ $(q, m[\vec{x}]) \in \Gamma - \Gamma_{\q}$ \algdo\\
  \> triggers := triggers $\bigcup_{R}$ Compile($q, m, \vec{x}$); \\
\comment{\algdone\\}
\algreturn\ triggers
\end{tabbing}

\caption{The compilation algorithm.}
\label{fig:compilation-algo}
\end{figure}

\begin{figure*}
\begin{itemize}
\item \todo{Annotate operator? \ldots or write w/ annotated calculus?}
\item \todo{Initializers per RHS map}
\end{itemize}
\begin{tabular}{l|l|l}
Operation & Result & Notes \\
\hline 
Initial query, $\v{q}$
    & $\calcsum(B(\v{P0,V0}) * \v{P0} * \v{V0}$
    &
\\
    & $\quad * \calcgt(0.25 * \calcsum(B(\v{P1,V1}) * \v{V1})$
    &
\\
    & $\quad - \calcsum(B(\v{P2,V2}) * \v{V2} * \calcgt(\v{P2-P0})))$
    &
\\[1.5ex]
Preaggregation
    & $\calcsum(
       \calcsum(\tuple{\v{P0}}, B(\v{P0,V0}) * \v{P0} * \v{V0})$
    & 
\\
    & $* \; \calcgt(0.25 * \calcsum(B(\v{P1,V1}) * \v{V1})$
    &
\\
    & $\qquad - \calcsum(B(\v{P2,V2}) * \v{V2} * \calcgt(\v{P2-P0})))$
    & 
\\[1.5ex]
$\Delta_{+B(\v{\tiny{xP}},\v{\tiny{xV}})}$,
    & $\calcsum($
      $\calcsum(\tuple{\v{P0}}, B(\v{P0,V0}) * \v{P0} * \v{V0}) * $
      $\calcgt(\v{Q2}) * \calclte(\v{Q1}))$
    &
\\
Monomialize,
    & $+ \; \calcsum($
      $\calcsum(\tuple{\v{P0}}, (\v{xP} = \v{P0}) * \v{xV}) * $
      $\calcgt(\v{Q3}) * \calclte(\v{Q1}))$
    &
\\
Simplify
    & $ - \; \calcsum($
      $\calcsum(\tuple{\v{P0}}, B(\v{P0,V0}) * \v{P0} * \v{V0}) * $
      $\calclte(\v{Q2}) * \calcgt(\v{Q1}))$
    &
\\
    & $ - \; \calcsum($
      $\calcsum(\tuple{\v{P0}}, (\v{xP} = \v{P0}) * \v{xV}) * $
      $\calclte(\v{Q3}) * \calcgt(\v{Q1}))$
    &
\\
    & $+ \; \calcsum($
      $\calcsum(\tuple{\v{P0}}, (\v{xP} = \v{P0}) * \v{xV}) * $
      $\calcgt(\v{Q1}))$
    &
\\[1ex]
    & where Q1 =
      $0.25 * \calcsum(B(\v{P1,V1}) * \v{V1})$ 
    &
\\
    & $\quad - \; \calcsum(B(\v{P2,V2}) * \v{V2} * \calcgt(\v{P2-P0}))$
    &
\\
    & and Q2 =
      $0.25 * \calcsum(B(\v{P1,V1}) * \v{V1}) + \v{xV}$ 
    &
\\
    & $\quad - \; (\calcsum(B(\v{P2,V2}) * \v{V2} * \calcgt(\v{P2-P0}))$
      $+ (\v{xV} * \calcgt(\v{xP,P0})))$
    &
\\
    & and Q3 =
      $0.25 * \calcsum(B(\v{P1,V1}) * \v{V1}) + \v{xV}$ 
    &
\\
    & $\quad - \; \calcsum(B(\v{P2,V2}) * \v{V2} * \calcgt(\v{P2-P0}))$
    &
\\[1.5ex]
Extract 
    & $\v{q1}(\tuple{\v{P0}}) := $
      $\calcsum(\tuple{\v{P0}}, B(\v{P0,V0}) * \v{P0} * \v{V0})$
    &
\\
    & $\v{q2} := \calcsum(B(\v{P1,V1}) * \v{V1})$
    &
\\
    & $\v{q3}(\tuple{\v{P0}}) := $
      $\calcsum(B(\v{P2,V2}) * \v{V2} * \calcgt(\v{P2-P0}))$
    &
\\[1ex]
    & $\partial \v{q} = $
      $\calcsum( \v{q1}(\tuple{\v{P0}}) * $
      $\calcgt(\v{E2}) * \calclte(\v{E1}))$
    &
\\
    & $+ \; \calcsum( \calcsum(\tuple{\v{P0}}, (\v{xP} = \v{P0}) * \v{xV}) * $
      $\calcgt(\v{E3}) * \calclte(\v{E1}))$
    &
\\
    & $- \; \calcsum( \v{q1}(\tuple{\v{P0}}) * $
      $\calclte(\v{E2}) * \calcgt(\v{E1}))$
    &
\\
    & $- \; \calcsum( \calcsum(\tuple{\v{P0}}, (\v{xP} = \v{P0}) * \v{xV}) * $
      $\calclte(\v{E2}) * \calcgt(\v{E1}))$ &
\\
    & $+ \; \calcsum( \calcsum(\tuple{\v{P0}}, (\v{xP} = \v{P0}) * \v{xV}) * $
      $\calcgt(\v{E1}))$
    &    
\\
    & where E1 = $0.25 * \v{q2} - \v{q3}(\tuple{\v{P0}})$
    &
\\
    & and E2 = $0.25 * \v{q2} + \v{xV} - (\v{q3}(\tuple{\v{P0}}) $
      $+ (\v{xV} * \calcgt(\v{xP,P0})))$
    &
\\
    & and E3 = $0.25 * \v{q2} + \v{xV} - \v{q3}(\tuple{\v{P0}})$
    &
\\[1.5ex]
MakeStmt
    & \texttt{q[][] += sum\_P0 c1 * c2 * (q1[][P0])}
    & MakeInitializer = 0
\\
    & \texttt{q[][] += sum\_P0 c1 * c3 * (xP=P0?xP*xV:0)}
    &
\\
    & \texttt{q[][] += sum\_P0 c1 * c2 * (q1[][P0])}
    &
\\
    & \texttt{q[][] += sum\_P0 c1 * c3 * (xP=P0?xP*xV:0)}
    &
\\
    & \texttt{q[][] += sum\_P0 c1 * (xP=P0?xP*xV:0)}
    &
\\
    & where c1 = \texttt{0.25 * q2[][] - q3[P0][] > 0? 1:0}
    &
\\
    & and c2 = \texttt{0.25 * q2[][]+xV - q3[P0][]+(xP>P0?xV:0) > 0? 1:0}
    &
\\
    & and c3 = \texttt{0.25 * q2[][]+xV - q3[P0][] > 0? 1:0}
    &
\\[1.5ex]
EliminateLoops
    & \texttt{q[][] += sum\_P0 c1 * c2 * (q1[P0])}
    & MakeInitializer = 0
\\
    & \texttt{q[][] += c1x * c3x * xP * xV}
    &
\\
    & \texttt{q[][] += sum\_P0 c1 * c2 * (q1[P0])}
    &
\\
    & \texttt{q[][] += c1x * c3x * xP * xV}
    &
\\
    & \texttt{q[][] += c1x * xP * xV}
    &
\\
    & where c1x = \texttt{0.25 * q2 - q3[xP] > 0? 1:0}
    &
\\
    & and c3x = \texttt{0.25 * q2+xV - q3[xP] > 0? 1:0}
    &
\\[1.5ex]
Recur, $\v{q1}(\tuple{\v{P0}})$
    & $\calcsum(\tuple{\v{P0}}, B(\v{P0,V0}) * \v{P0} * \v{V0})$
    &
\\
M,
$\Delta_{+B(\v{\tiny{xP}},\v{\tiny{xV}})}$,
S
   & $(\v{xP} = \v{P0}) * \v{xP} * \v{xV}$
   &
\\
MakeStmt
    & \texttt{for P0: q1[P0]<0> += xP = P0? xP * xV : 0}
    & MakeInitializer = 0
\\
EliminateLoops
    & \texttt{q1[xP]<0> += xP * xV}
    &
\\[1.5ex]
Recur, $\v{q2}$
    & $\calcsum(B(\v{P1,V1}) * \v{V1})$
    & 
\\
M,
$\Delta_{+B(\v{\tiny{xP}},\v{\tiny{xV}})}$,
S
   & $\v{xV}$
   &
\\
MakeStmt
    & \texttt{q2[]<0> += xV}
    & MakeInitializer = 0, EliminateLoops is no-op
\\[1.5ex]
Recur, $\v{q3}(\tuple{\v{P0}})$
    & $\calcsum(B(\v{P2,V2}) * \v{V2} * \calcgt(\v{P2-P0}))$
    &    
\\
M,
$\Delta_{+B(\v{\tiny{xP}},\v{\tiny{xV}})}$,
S
   & $\v{xV} * \calcgt(\v{xP,P0})$
   &
\\
MakeInitializer
    & \texttt{q3i: q3[P0] := sum\_p P0 < p? m4[p] : 0}
    &
\\
MakeStmt
    & \texttt{for P0: q3[P0]<q3i> += xP-P0 > 0? xV : 0 }
    & EliminateLoops is no-op
\end{tabular}
\caption{Simplified VWAP compilation trace.}
\label{fig:compilevwap}
\end{figure*}