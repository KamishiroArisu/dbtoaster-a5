\section{Related Work}

To the best of our knowledge, there has been limited research literature
addressing the question of how to compile continuously evaluated SQL queries to
a low-level imperative language such as C++.  With this in mind, in this section
we contrast our map calculus, compilation algorithm and generated engine to
several more extensively investigated topics, namely view maintenance,
main-memory databases as well as stream and event processing.

\textbf{Database compilers and generators.}
Database management systems have long realised the benefits of compilation in
their ability to \textit{prepare} and execute SQL queries and stored procedures
since the days of System R~\cite{chamberlin-tods:81}, which investigated
compilation to machine code for computing architectures from that era. To the
best of our knowledge, modern database systems do not generate assembly but
query plans -- a network of operators fundamentally processing with a
Volcano-style iterator-based approach~\cite{graefe-tkde:94}. We view query plans
as interpreted programs, the plan is a data structure maintaining a composition
of efficient operator implementations, and must be inspected, and dispatched by
the query execution engine (i.e the interpreter).
\comment{
Furthermore, in such systems, continuous query processing
is often an afterthought, and added in the form of triggers, views and
incremental view maintenance algorithms.
}
In contrast our compilation generates engines that differ
significantly from plan-based approaches in that our generated code is designed
for per-tuple processing, and thus streaming, and does not incur any of the
overhead of having to inspect plan data structures.
Database compilers have also looked at database generation, for example the work by Batory
et al. on the GENESIS~\cite{batory-tse:88} system. Such work is much more in the
spirit of extensible database systems, such as Starburst and Postgres. We focus
purely on the query processing aspect, in the main-memory context, as opposed to
attempting to define database generation languages, or tools to support modular
database architectures.


\textbf{View maintenance.}
View maintenance algorithms are in abundance in the database literature, with
topics ranging from efficient view maintenance algorithms given base relation
deltas~\cite{colby-sigmod:96}, which may be eagerly or lazily
applied~\cite{yan-vldb:95,zhou-vldb:07}, to the question of which views to
actually materialize and how to use such views during query
optimization~\cite{kotidis-tods:01}.
Zhou et al.~\cite{zhou-vldb:07} describe a lazy view materialization technique,
where their computation of deltas from single tuples starts from a similar point
to ours, however, their approach does not generalize like our map calculus,
particularly in the case of nested aggregations such as the VWAP query seen in
this work.
Griffin and Libkin~\cite{griffin-sigmod:95} study the problem of incremental
view maintenance for relations with duplicates, using a bag algebra to derive
update rules.  
More pertinent to the main-memory database context,
Roussopoulos~\cite{roussopoulos-tods:91} presents a pointer-based approach to
implement views with ViewCache.
Our map calculus differs in that it inherently computes a set of maps to support
incremental maintenance of a query result, rather than a single view. None of
these works consider any form of multi-level view maintenance attained via
recursive compilation.
\comment{
Our approach to selecting maps under a memory constraint differs from
traditional cost models to choose views to materialize, given our top-down approach to
decomposing queries with maps.
Furthermore, given our target application of non-interactive queries and a static
query workload, we need not maintain base relations, unlike standard relational
query processors which do so to provide ad-hoc query capability at the expense of
orders of magnitude performance as seen in our experimental section. This
obviates the need for any pointer or index-based approach to track back to
original base relation tuples, unlike existing main-memory database systems. Most
critically, the notion of compiling these view maintenance algorithms is clearly
a simple and effective technique for generating query executors for unmatched
performance.
}
\comment{
\begin{itemize}
  \item Asymmetric increment techniques \cite{yang-icde:05}.
  \item Group-by optimizations \cite{yan-icde:94}.
\end{itemize}
\noindent \textbf{DataCubes}
\begin{itemize}
  \item Classical literature \cite{gray-icde:96,mumick-sigmod:97}.
  \item TODO: more papers if we head into cubes.
\end{itemize}
}

\noindent \textbf{Stream and event processing.}
Our work can loosely be compared to complex event and stream processing engines
such as Aurora, STREAM, Cayuga,
SASE+~\cite{wu-sigmod:06,agrawal-sigmod:08,white-pods:07,motwani-cidr:03,abadi-vldbj:03}
due to the common goal of handling frequently changing data.
\comment{
SASE+~\cite{agrawal-sigmod:08} and Cayuga~\cite{white-pods:07} are complex event
processors that focus on sequence and pattern query processing on event streams
using NFA-based approaches to process a variety of patterns including Kleene
closures and negation operators. Relational stream processing engines such as
STREAM~\cite{motwani-cidr:03} and Aurora~\cite{abadi-vldbj:03} investigate
continuous query processing architectures that are capable of evaluating
stream-equivalent versions of the standard relation algebra.
}
\comment{From the systems
perspective, several novel techniques were developed including scheduling, load
shedding, and approximate query processing.
}
Many of these systems are simultaneously addressing the issue of efficiency and
the semantic requirements of their target applications, exploiting such semantic
properties to attain performance. In contrast, our approach does not use such
novel data and query models -- we focus on standard relational queries, but
critically address performance issues of arbitrary updates to a database
snapshot.
\comment{
We also argue for an alternative to plan-based execution, and design query
executors to best suit continuous execution in a main-memory environment.
}


The SPADE~\cite{gedik-sigmod:08} language of IBM's System S stream processing
engine, is a declarative language for specifying data stream applications. The
authors discuss their implementation including their compilation process and
compiler optimizations such as operator grouping for execution across multiple
hardware contexts, and operator fusion to reduce internal queueing. Note that
\compiler\ completely eliminates any internal queueing, since we produce a
single-straight line function, and automatically derives appropriate data
structures to support query processing via a single function. This latter issue
is not considered in SPADE. Finally, StreamBase~\cite{streambase}, a
commercialization of the Aurora stream processing engine has also studied the
problem of compiling stream programs, however there is little
documentation~\cite{sb-patent} regarding the technical details of their
techniques. To the best of our knowledge, their compilation process does not
include generating straight-line processing functions in the same aggressive
manner as \compiler.

 \comment{
 TODO:
contrast our work to the above -- we avoid the need for windows or punctuations
by making adopting an insert/delete/update stream model, and assuming that the
base relations always fit in memory (i.e. \#deletes/updates is proportional to
\#inserts). Stream processors still evaluate an operator-centric query plan,
hence they cannot exploit compiler optimizations on a query processing kernel
function. They also do not deal with maintaining precomputed partial results as
we do with our map decompositions. What can we say about complex event processors
and the NFAs they evaluate?


\begin{itemize}
  \item STRIP: rule-based maintenance of derived data, with finance app example \cite{adelberg-sigmod:97}
\end{itemize}
}

\comment{
\noindent \textbf{Compiling high-level languages for embedded systems.}
The embedded systems community has partially studied on compiling high-level
languages into a low-level imperative target language.
Newton et al.~\cite{newton-lctes:08} present WaveScript, a scripting language
for processing windows, or segments, of a data stream, and describe a compiler to
construct stream dataflow graphs that are capable of running on XScale CPUs.
WaveScript programs are transformed into dataflow graphs, which may in turn be
manipulated by algebraic rewrite rules, similar to a query optimizer. However,
WaveScope does not address relational queries.
Toman and Weddell~\cite{toman-dbtel:01} describe the DEMO system which compiles
SQL queries into Java or C code, implementing query plans as navigational plans
that utilize pointer access in place of scans and indexing, and supporting query
functionality over existing data structures for an embedded program. The authors
use integrity constraints in their compiler to describe the relationship between
schemas and the physical data structures used by an embedded program, and in turn
generate navigational plans and iterators, which can be used to produce C code in
a straightforward manner. DEMO has taken a theoretical approach to describe
their compilation, and does not consider systems issues nor have they released
a prototype and provided an effective tool to perform compilation.
}


\noindent \textbf{Main memory databases.}
Early work on main-memory databases studied how to reduce the bottleneck effect
of a variety of I/O tasks including recovery tasks such as checkpointing, as well
as logging and locking structures \cite{bohannon-sigmod:99}. There have been
several recent efforts on this topic given the developments in main memory,
including MonetDB/X100~\cite{boncz-cidr:05}, H-Store~\cite{kallman-pvldb:08},
and Blink~\cite{raman-icde:08}.
\comment{
Boncz et al.~\cite{boncz-cidr:05} present the MonetDB/X100 database engine, a
high-performance column-oriented database, that leverages techniques such as
vectorized processing and loop pipelining on chunks of columns while evaluating
operators in query plans. Kallman et al.~\cite{kallman-pvldb:08} describe
H-Store, a shared-nothing distributed main memory database to provide high
throughput processing on OLTP workloads. H-Store achieves its performance through
careful deployment of horizontally partitioning of relations, and localizing OLTP
transactions to partitions. Raman et al.~\cite{raman-icde:08} describe Blink, a
row-oriented main memory query processor that extensively uses compression on
denormalized relations, and applies aggregates while scanning these relations as
its main query processing functionality.
}
These systems highlight the potential for exploring main-memory database
architectures, but do not address continuous processing and compilation. Having
only started generating our runtime, we can now proceed to the systems level
aspects of our tuple functions, for example investigating cache performance in
much greater depth, and in particular understanding which of the above
techniques and optimizations can be applied in our context.
