
\section{Experiments}
\label{sec:experiments}

We have implemented a prototype of the sliceDBread system, written in approximately 6,000 lines of Ruby (the runtime), and 10,000 lines of Ocaml (the compiler).  We evaluate the scaling properties of our implementation on a database generated by the TPC-H benchmark database generator, using data warehouse construction queries modeled after the TPC-H benchmark queries.  

Unless otherwise specified, queries are evaluated on a data warehouse initialized with a 0.5 TB benchmark database (TPC-H scaling factor 500).  Load is generated by applying the contents of a benchmark update (refresh factor 10, or 0.1\% of the dataset) in a random order.  Unless otherwise noted, trials take place on a cluster of 40 nodes.  When a smaller cluster size is used, the size of the database used is decreased proportionately.  Each node runs on an independent 2.66 GHz core, each with a 4MB cache, and 16 GB of 667 MHz memory.  Nodes are connected over a Gigabit Ethernet link. 

Though the TPC-H benchmark query is geared towards decision support queries, asking specific questions of a database.  Conversely, sliceDBread constructs a materialized view to answer such queries quickly and efficiently.  While we do consider the response time of a queries posed over the materialized views produced by sliceDBread, our analysis focuses on the construction of that materialized view rather than the final query workloads.  Thus, instead of using the TPC-H workload as-is, we consider what materialized views could be used to efficiently answer TPC-H queries.

%TODO: verify the number of queries expressible in this way.
We select two materialized views to be representative of the TPC-H workload.  12 of the 22 queries in the TPC-H query workload can be expressed as OLAP queries over a view corresponding to one of the following natural joins:
\begin{itemize}
\item customer $\bowtie$ orders $\bowtie$ lineitem
\item part $\bowtie$ partsupp $\bowtie$ supplier
\end{itemize}
We study the behavior sliceDBread with respect to these two materialized views, respectively denoted COL and PPsS for their join paths.  Specifically, our experiments are performed with group-by aggregates of each view; The view construction queries are presented in Figure \ref{fig:experimentqueries}.  As we show in Section \ref{sec:exp:complexity}, these results generalize to larger views, where fewer columns are aggregated out.

Unless otherwise noted, we measure query latency by computing the sum of the value computed by the query; As our current implementation does not compute datacubes of the materialized view by default, this query represents a worst case scenario, requiring a full scan of the entire materialized view.

\begin{figure*}
\begin{center}
\begin{minipage}{3.4in}
\begin{verbatim}
SELECT
  COUNT(*), customerkey, nationkey, orderkey, 
  orderpriority, shippingpriority, shipmode,
  case(when shipdate > commitdate then 1 else 0) 
    AS lateship,
  case(when receiptdate > commitdate then 1 else 0) 
    AS latereceipt
FROM
  customer JOIN orders JOIN lineitem
GROUP BY
  customerkey, nationkey, orderkey, 
  orderpriority, shippingpriority, shipmode,
  lateship, latereceipt
\end{verbatim}
\begin{center}
(a)
\end{center}
\end{minipage}
\begin{minipage}{3.4in}
\begin{verbatim}
SELECT
  SUM((retailprice - supplycost) * availqty), 
  partkey, mfgr, type, suppkey, 
  size, container, nationkey
FROM
  part JOIN partsupp JOIN supplier
GROUP BY
  partkey, mfgr, type, suppkey, 
  size, container, nationkey
\end{verbatim}
\begin{center}
(b)
\end{center}
\end{minipage}
\end{center}
\caption{Example queries used in evaluating sliceDBread.  (a) A COL query class example that produces a materialized view for analysis of order fulfilment delays, and (b) A PPsS query class example that produces a materialized view for analysis of potential asset acquisition.}
\label{fig:experimentqueries}
\end{figure*}

\subsection{Performance and Scalability}
\label{sec:exp:performance}
\label{sec:exp:scalability}

\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
((Table 1)) & ((Graph 1)) & ((Graph 2)) \\
(a) & (b) & (c)
\end{tabular}
\caption{Baseline performance characteristics of sliceDBread.  (a) Per-node memory usage for a range of data-sizes and query classes.  (b) Average CPU usage at each node for a fixed-size cluster as data update rate increases.  (c) Network bandwidth usage across the network as data update rate increases.}
\label{fig:performance}
\end{center}
\end{figure*}

We first obtain a performance baseline for sliceDBread by installing our example queries on a 10 node cluster.  Figure \ref{fig:performance}a shows sliceDBread's memory usage on a variety of dataset sizes; the overhead of storing materialized subqueries is only (TODO: Fill in number).  In Figures \ref{fig:performance}b and \ref{fig:performance}c, we measure the processing and bandwidth requirements of incrementally maintaining the example materialized views under varying load levels.

(TODO: Discuss the behavior of Figure \ref{fig:performance}b and c.)

\begin{figure*}
\begin{center}
\begin{tabular}{cc}
((Graph 3)) & ((Graph 4))\\
(a) & (b)
\end{tabular}
\caption{sliceDBread Scalability.  (a) Update and query processing improvements from adding new nodes to a sliceDBread cluster.  (b) Scalability of sliceDBread as cluster size and data volume are both increased proportionately.}
\label{fig:scalability}
\end{center}
\end{figure*}

\begin{figure}
\begin{center}
((Graph 5))
\caption{sliceDBread performance (CPU load and query latency) under variable load.}
\label{fig:variableload}
\end{center}
\end{figure}

We next analyze how sliceDBread scales with additional nodes.  Figure \ref{fig:scalability}a shows the response time of queries posed over a view materialized by the warehouse as a function of the rate at which one of the view's dependent tables is updated.  As the processing capabilities of the data warehouse become saturated processing updates, average query latency spikes dramatically.

We refer to the elbow in the latency vs update rate curve as the breakdown point, at which the processing capabilities of the warehouse have been reached and queuing delays account for most of the query latency.  Note however, that this behavior only occurs for sustained streams of updates at the indicated rate.  Figure \ref{fig:variableload} demonstrates sliceDBread's ability to absorb bursts of updates.

For all further experiments, we use the breakdown point to measure system performance in a given configuration.  Formally, the maximum update rate of the system in a given configuration is the maximum update rate (rounded to the next lower multiple of 50 updates/sec), such that query response time is at most 50\% higher that of an idle system.

Finally, we consider the performance impact of scaling sliceDBread to larger cluster sizes and data volumes.  Figure \ref{fig:scalability} shows the effect of growing the cluster while maintaining a constant volume of data stored on each node.  (TODO: discuss the graph).

\subsection{Query Complexity}
\label{sec:exp:complexity}
Up to this point, we considered sliceDBread's performance on two specific materialized views.  We next analyze how sliceDBread reacts to queries of varying complexity.  

First, Figure \ref{fig:scalinggroupby} shows performance with respect to the level of aggregation performed on the materialized view.  Starting with the example materialized views, we discard a randomly selected set of columns.  We measure both the maximum update rate and query latency at a fixed update rate of (TODO: fill in) while sliceDBread maintains the new materialized view.  The results are further partitioned to indicate whether one or both key columns have been removed from the materialized view.

While non-key columns have a relatively low impact on performance, key columns provide a natural means of partitioning the data.  Having a column common across most maps (especially the one storing the final materialized view) improves message locality and minimizes the number of M3 statements that require data to be sent across the network.

Conversely, any decrease in the size of the materialized view results in a higher degree of pre-aggregation during updates, and reduces the work done at query time.  This suggests that sliceDBread is ideally matched to the needs of incremental datacube maintenance; the computational overhead of maintaining an aggregate does not depend on the level of aggregation required.

\begin{figure}
((Graph 6))
\caption{Maximum update rate for a given number of columns in the materialized view}
\label{fig:scalinggroupby}
\end{figure}

Figure \ref{fig:scalingdatacube} examines sliceDBread performance while maintaining a heirarchy of aggregates.  Each trial maintains a specified number of aggregates, each computed over a randomly selected subset of the columns the example views.  (TODO:discuss)  A substantial amount of work can be shared between the updates; indeed, for our example views, only one additional M3 statement is produced for each additional aggregate.  This work sharing makes it possible to very efficiently produce and maintain datacubes in a sliceDBread data warehouse.

\begin{figure}
\begin{center}
((Graph 7))
\caption{Maximum update rate while maintaining multiple aggregates of the same view.}
\label{fig:scalingdatacube}
\end{center}
\end{figure}


Finally, we consider performance with respect to join complexity.  As the TPC-H schema does not include sufficient levels of complexity, we also examine the behavior of sliceDBread on two synthetic examples.  We incrementally maintain views over all three of the following joins:
\begin{itemize}
\item order $\bowtie$ lineitem $\bowtie$ supp $\bowtie$ nation $\bowtie$ region
\item R(A,B) $\bowtie$ S(B,C) $\bowtie$ T(C,D) $\bowtie$ U(D,E) $\bowtie$ V(E,F)
\item M(A,B,C,D,E) $\bowtie$ R(A,F), S(B,G), T(C,H), U(D,I), V(E,J)
\end{itemize}
For our synthetic datasets, we use input relations initialized to size (TODO: fillin) and for each update perform a random insertion or deletion to/from a random relation in the set.  In the case of the first two joins, we maintain the sum of a product of values taken from the input relations at either end of the linear join.  In the case of the latter join, we maintain the sum of the product of one value from each dimension table.

Figure \ref{fig:scalingjoincomplexity} shows sliceDBread performance as we increase join complexity.  For the linear joins, input relations are added from left to right.  The same holds true for the star join, although the fact table M is present in all trials. (TODO: discuss)

\begin{figure}
((Graph 8))
\caption{Maximum update rate relative to the join complexity; Shown are results for a star join with the indicated number of branch tables, and a linear join over the indicated number of tables.}
\label{fig:scalingjoincomplexity}
\end{figure}



\subsection{Foreign Key Optimization}
\label{sec:exp:fkoptimization}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                 Notes
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%% TPCH Joins %%%%%%%%%%%%%%%%
% 
% Q01: li
% Q02: p, ps, s, n, r
% Q03: c, o, li
% Q04: o, li
% Q05: c, o, l, s, n, r
% Q06: l
% Q07: s, l, o, c, n, n
% Q08: p, s, l, o, c, n, n, r
% Q09: p, s, l, ps, o, n
% Q10: c, o, l, n
% Q11: p, ps, s
% Q12: o, l
% Q13: c, o
% Q14: l, p
% Q15: s, l
% Q16: p, ps, s
% Q17: l, p
% Q18: c, o, l
% Q19: l, p
% Q20: l, p, ps, s, n
% Q21: s, l, o, n
% Q22: c