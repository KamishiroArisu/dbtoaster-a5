\documentclass{sig-semester}

\pdfpageheight=11in
\pdfpagewidth=8.5in

\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\usepackage{xspace}

% These are from candidacy edic.tex
\graphicspath{{./graphics/}}
\DeclareGraphicsExtensions{.pdf,.jpeg,.png}

% Legacy from Christoph's paper:
% \DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

% When converting eps to pdf, suffix is added to filename. 
% Recommended not to be empty string by http://www.tex.ac.uk/tex-archive/macros/latex/contrib/oberdiek/epstopdf.pdf : page 4, suffix paragraph.
\epstopdfsetup{suffix=-generated}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{todo}[theorem]{ToDo}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{metatheorem}{Metatheorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{property}[theorem]{Property}
\newtheorem{corollary}[theorem]{Corollary}
% \newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proviso}[theorem]{Proviso}


\title{Loose consistency under Cumulus}


%\numberofauthors{1}
\author{\alignauthor Aleksandar Vitorovic \\[1ex]
\affaddr{Dept.\ of Computer Science} \\
\affaddr{EPFL} \\
\affaddr{aleksandar.vitorovic@epfl.ch}
}

\def\SQL{SQL\xspace}
\def\OLAP{OLAP\xspace}
\def\OLTP{OLTP\xspace}
\def\M3{M3\xspace}
\def\EXORD{actual OLAP update execution order\xspace}

\begin{document}
\maketitle


\abstract{
This paper presents issues related to a lowered consistency in Cumulus.\\
}


\keywords{Incremental View Maintenance, \OLAP updates, Parallelization}


\section{Introduction}
\vspace{2mm}
Our earlier design was explicit and contained a lot of machinery. The implementation was mostly engineering, and the idea is to provide consistency in more unexpected way. We will trade-off consistency and network traffic for decreased amount of machinery. This project aims to provide distributed transactions optimistically in order to avoid any blocking.

\section{System reconsideration}
\vspace{2mm}
We imply multiple switch system, the destination-computation approach, static partitioning and we introduce some total order among all the updates in the system.

\begin{example} \em
\label{ex:starter}
Consider the following sequence of updates arriving from two different switches:
\begin{verbatim}
-------------
1.1 m2+=m1
1.2 m3+=m2
-------------
2.1 m2+=3
2.2 m3+=m2
-------------
\end{verbatim}
\end{example}
Depending on order of updates, the correct final result for m3 is either 3+m1+m1 or 3+3+m1. Schedules \{2.1, 1.1, 1.2, 2.2\} and \{1.1, 2.1, 2.2, 1.2\} are invalid because final result of m3=3+3+m1+m1 could not be obtained by either update order - \{1, 2\} nor \{2, 1\}. This is an example of ``too-much-data'', or violated R-W dependencies. The remedy for this problem is to send only the necessary data. For instance, in the schedule \{2.1, 1.1, 1.2, 2.2\} in the statement 1.2, the DW Node containing m2 will send only data from TS 1, if introduced total order is \{1, 2\}.

Note that 1.2 is executed after 1.1 in all schedules, since the DW Node containing m2 completes 1.1 before sending the m2 value.

\begin{example} \em
\label{ex:starter2}
This example has similar characteristics as the previous one:
\begin{verbatim}
-------------
1.1 m1+=10
-------------
2.1 m2+=m1
2.2 m3+=m1
-------------
\end{verbatim}
\end{example}
If Schedule is \{2.1, 1.1, 2.2\} and introduced order is \{2, 1\}, in statement 2.2 the m1 value before 1.1 is sent.

\begin{example} \em
\label{ex:atomicity}
There are ``not-enough-data'' situations, where W-R data dependencies are violated:
\begin{verbatim}
-------------
1.1 m1+=m2
-------------
2.1 m2+=m1
-------------
\end{verbatim}
\end{example}
We will assume updates are issued by different switches, and m1 and m2 are stored on different DW Nodes, DW Node1 and DW Node2, respectively. If FETCH messages reach the corresponding DW Nodes at the nearly same time, DW Node1 will send m1 and DW Node2 will send m2 values before any of map updates are executed. No serial execution of updates are equivalent to the final result obtained in this particular scenario. No matter of introduced total order, these situations arise if there are cyclic dependencies between map updates in different updates.

This is a program where the statement graph is cyclic, it is not expected as DBToaster output, and garbage collection must be specified through 2PC between DW Nodes containing m1 and m2.

Still, we would like to continue executing without blocking. The only way to do so is to introduce some corrective update mechanism. By our old approach, the four types of logs are utilized: the WriteLog, the OperationLog, the ReceiveLog, the SendLog. Here, we may coalesce WriteLog and OperationLog into one in the following way. A map is stored in \{TS, rhs\_term\} pairs. If we prune ReceiveLog as well, the value of a particular rhs is appended to a pair. In a case when there are multiple maps in a single rhs, a value for each rhs\_term has to be kept. Avoiding the ReceiveLog incur increased memory consumption, since a rhs is stored redundantly. 

We might also change corrective update mechanism totally. Previously, on each map update, the SendLog was examined in order to check for potential corrective update mechanism, guaranteeing eventual consistency. On each PUSH message, the SendLog structure was updated. In our new design, the SendLog and explicit sending of corrective update messages is not needed anymore. Each PUSH message consists of all the deltas for particular map. Upon arriving on a destination, all previously executed statements are examined in order to check if they missed some deltas. However, our system does not support eventual consistency anymore, but a lower level of consistency, implying that if the system is continiously updating we will have ``second-recent'' data. For instance, in our Example\ref{ex:atomicity}, if there is no further communication between the two DW Nodes, a user will never read the consistent version of m1 and m2 map.

In the old design, on each PUT message we examined the SendLog for each destination. Since the SendLog consists of pairs \{map, destination, TS\}, this will take in total O(N) time, where is N total number of the nodes in the system. Upon arrival of a corrective update message on the destination, all the previously executed statements are glanced over to check if this corrective update influence them. In the new design, we examine the OperationLog on each PUSH message arriving, thus much more frequently. Each PUSH message is potentially a corrective update message for previously executed statements on that node. In order to be more efficient, for each map that a DW Node ever receives, the system may preserve a list of maps on the DW Node depending on it, thereby effectively trading-off the SendLog structure for this new list.

Sending all the deltas in a PUSH message is not expected to increase drastically number of messages in the network, since all the deltas are expected to fit into a single IP package. However, experiments must support this claim.

Note that we do not want to express all the maps through a single map, since this will negate all the incremental effects DBToaster offered us. In addition, that does not rule out the need for corrective update or similar mechanism.

\begin{example} \em
\label{ex:M3Program}
Similar behaviour is anticipated for M3 program containing implicit cycles:
\begin{verbatim}
-------------
1.1 q+=m1
1.2 m2+=c
-------------
2.1 q+=m2
2.2 m1+=c
-------------
\end{verbatim}
\end{example}
If statements 1.1 and 2.1 was executed at the nearly same time, neither of updates will see the effect of the other one.

This program is acyclic on statement level, but cyclic on trigger level, allowing standard garbage collection procedure. It is typical example of DBToaster, implying that there is cycle including all the triggers in the system.

M3 programs do not guarantee the same result after any order of trigger execution, but M3 programs generated by DBToaster do.

\subsection{Ordering} Ortogonal to the problem of machinery is the problem of choosing ordering. PUSH messages imply some order between two TS, and the system might be designed by sticking with that \textit{natural order}. The natural order is built incrementally, so any received partial order is combined with local constraints about order. This will prevent corrective updates when the effect of statement execution is the same as some total order, and the total order do not need to be introduced before execution. In other words, R-W dependecy violations are avoided, but ``not-enough-data'' still remains the problem in Example\ref{ex:atomicity}. In that case, DW Node1 and DW Node2 implies contradictory natural orders and we have to choose one of them and perform corrective updates accordingly. 

Transitions might complicate things further. If the DW Node which perfromed corrective update have not computed the order itself, but rather received from some other node along with some map, it has to inform the nodes on all the path through source of that information, so they also might perform corrective updates. Also, all the children of any node in the path must be notified about the change in the order.

\begin{example} \em
\label{ex:atomicity2}
In the following example DW Nodes imply different natural order and do not communicate:
\begin{verbatim}
-------------
1.1 m1+=m2
1.2 m3+=m4
-------------
2.1 m2+=3
2.2 m4+=5
-------------
\end{verbatim}
\end{example}
If Schedule\{1.1, 2.1, 2.2, 1.2\} appears, maps m1 and m2 agree upon \{1,2\} and m3 and m4 agree upon \{2,1\}. This schedule is possible since in general all the maps may be on separate DW Nodes. There is no clear mechanism to avoid this, since those conflicting orders may be scattered among multiple updates.

This kind of programs is not present in DBToaster.

The only way to prevent this is to force nodes containing a lhs in the same update to communicate upon any of the lhs is updated. Again, the transition can complicate things further.

Introduced order may lead to violated W-R dependencies even when atomicity is preserved, but statements were executed in unexpected order. Natural order do not have this problem, but the mechanism complicates when there are cyclic dependencies among map updates.

\section{Further work} Based on system design we choose, garbage collection may require reconsideration as well. A node must ensure no further deltas will arrive. If we decide to sustain the SendLog the mechanism remains the same. Otherwise, things are becoming more complex.

%{
%\bibliographystyle{ieeetr}
%\bibliography{../../Semester1/refs,../../Semester1/main,../../candidacy/writeup/refs}
%}

\newpage

\end{document}