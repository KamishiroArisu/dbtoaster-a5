\subsection{The Challenges of Monitoring}
Consider the following examples of commonplace automated tasks in three different application domains:
\begin{enumerate}
\item An automated stock market trading system monitors the distribution of buy and sell orders of a particular stock to identify the best time and price for its own orders.
\item A corporate data warehouse monitors the current status of its production facilities, warehoused inventory and active demand for its products in order to preemptively identify supply chain problems.
\item A compute cluster monitors its current status overnight to alert a network administrator when some of its hardware fails, but only if the chance of computations being disrupted by further failures exceeds a given threshold.
\end{enumerate}

The core challenge in automating each of these tasks is the instantiation of a system for {\em monitoring} the state of the world from the application domain's perspective, be it a stock market, data warehouse, or compute cluster, etc\ldots.  This is a fundamentally data-management challenge: As the state of the world changes, the monitoring system must update it's own internal view of that state accordingly -- each order placed for a given stock can potentially affect the automated trading system's behavior.  

In spite of this, the challenges associated with monitoring rapidly changing, but otherwise persistent state have not been sufficiently addressed by the database community. Although active database (i.e., Triggers)\cite{?} and complex stream event processing (CEP)\cite{?} techniques both address the challenges of monitoring persistent state, neither is designed to handle complexity in the monitoring task, nor rapid changes to ``persistent'' state.  In this section we now support this assertion through experimental (and some anecdotal) evidence, which clearly demonstrates that existing data-management systems are incapable of efficiently performing these sorts of monitoring tasks.

We consider the three example scenarios as described above.  Each monitoring task is implemented using triggers in Postgres and two commercial database systems, and using CEP in two commercial stream processing systems\footnote{The names of the commercial databases and stream processors are kept anonymous due to restrictions in their license agreements.}.  Concretely, the scenarios are as follows:

\begin{figure}
\tinysection{Example \ref{ex:dbfail:stock}}
\begin{verbatim}
-- Example 2.1 --
CREATE TABLE bids(volume float, price float);
CREATE TABLE asks(volume float, price float);
\end{verbatim}

\tinysection{Example \ref{ex:dbfail:tpch}}'s schema is identical to the TPC-H schema\cite{tpch}.

\tinysection{Example \ref{ex:dbfail:network}}
\begin{verbatim}
CREATE TABLE Server(ssid int, status int);
CREATE TABLE Task(ttid int, priority int);
CREATE TABLE Assignment(asid int, atid int);
\end{verbatim}

\label{fig:dbfail:schemas}
\caption{Schemas for the three example scenarios}
\end{figure}

\begin{example}
\label{ex:dbfail:stock}
A stock market trading system monitors the spread across significant orders for a particular stock (i.e., orders larger than 0.01\% of the total volume of orders for the stock).  Given the table schemas defined in Figure \ref{fig:dbfail:example:schemas}, we can express the value being monitored as a SQL query:
% test/sql/finance/pricespread.sql
\begin{verbatim}
SELECT SUM(a.price - b.price)
FROM   bids b, asks a
WHERE  b.volume > 0.0001 * (SELECT SUM(b1.volume) 
                            FROM   bids b1)
  AND  a.volume > 0.0001 * (SELECT SUM(a1.volume) 
                            FROM   asks a1);
\end{verbatim}

\todo{Back of napkin calculations/citation for expected update rate}
\end{example}

\begin{example}
\label{ex:dbfail:tpch}
A corporate data warehouse monitors the volume of of parts of each type being shipped between each region based on the locations of the supplier and the client.  Given the standard TPC-H benchmark schema\cite{tpch}, we can express this monitoring task as the following SQL group-by query (based on, but not identical to SSB query 4\cite{ssb}): 
\begin{verbatim}
SELECT   sn.regionkey, cn.regionkey,
         p.type, SUM(li.quantity)
FROM     CUSTOMER c, ORDERS o, LINEITEM li, PART p, 
         SUPPLIER s, NATION cn, NATION sn
WHERE       c.custkey = o.custkey
  AND      o.orderkey = li.orderkey
  AND       p.partkey = li.partkey
  AND       s.suppkey = li.suppkey
  AND    cn.nationkey = c.nationkey
  AND    sn.nationkey = s.nationkey
GROUP BY sn.regionkey, cn.regionkey, PART.type;
\end{verbatim}

\todo{Back of napkin calculations/citation for expected update rate}
\end{example}

\begin{example}
\label{ex:dbfail:network}
A cluster monitoring system keeps monitors servers assigned to work on different compute tasks -- the system issues an alert whenever the number of failed servers assigned to a given task exceeds a given threshold (e.g., 50\%).  These alerts are dispatched based on priority, and number of threatened tasks.  Given the table schemas defined in Figure \ref{fig:dbfail:example:schemas}, we can represent this monitoring task as triggering whenever the following SQL group-by query returns a non-empty result:
\begin{verbatim}
SELECT priority, COUNT(*)
FROM   Task t
WHERE  (SELECT COUNT(*) 
        FROM Assignment a2,Server s2
        WHERE t.ttid = a2.atid 
        AND a2.asid = s2.ssid) * 0.5 > 
       (SELECT COUNT(*) 
        FROM Assignment a3,Server s3
        WHERE t.ttid = a3.atid 
        AND a3.asid = s3.ssid 
        AND s3.status = 1)
GROUP BY t.priority;
\end{verbatim}

\todo{Back of napkin calculations/citation for expected update rate}
\end{example}

\subsection{Monitoring with Active Databases}
The first existing technique for monitoring in the database literature is based around so called Active Databases\cite{?}.  This technique allows user-defined {\em triggers} to intercept updates to one table in the database, and initiate secondary updates as a result.  Trigger functionality has been implemented in most high-end database management systems -- Postgres\cite{?}, MySQL\cite{?}, and virtually all commercial systems, including the two we test.

Closely related to Active Databases are Incremental View Maintenance\cite{?} (IVM) systems.  Rather than allowing arbitrary user-defined operations to trigger after updates to the database, IVM systems allow users to define materialized views.  The result of the view query is materialized, but rather than re-evaluating the entire query from scratch whenever one of the tables in the query changes, the IVM system uses a so-called delta query.  The delta query is a simpler form of the original query, which identify how the original query changes as after an update to one of its tables.  

\begin{example}
For example, consider the query from Example \ref{ex:dbfail:tpch} and an insertion into the {\tt LINEITEM} table:
\begin{verbatim}
INSERT INTO 
LINEITEM(orderkey, partkey, suppkey, quantity)
VALUES  (37,       42,      7,       100);
\end{verbatim}

We can run the following query:
\begin{verbatim}
SELECT   sn.regionkey, cn.regionkey,
         p.type, SUM(100)
FROM     CUSTOMER c, ORDERS o, PART p, 
         SUPPLIER s, NATION cn, NATION sn
WHERE       c.custkey = o.custkey
  AND      o.orderkey = 37
  AND       p.partkey = 42
  AND       s.suppkey = 7
  AND    cn.nationkey = c.nationkey
  AND    sn.nationkey = s.nationkey
GROUP BY sn.regionkey, cn.regionkey, PART.type;
\end{verbatim}

Note that all columns from the {\tt LINEITEM} table have been replaced by the corresponding value from the inserted tuple.  

The result of running this query is the delta to the original table.  The delta is then merged into the original query's materialized results -- group by terms are paired with like values from the original query, and the new sums are added to the corresponding old ones.  

After merging it with the delta, the materialized view correctly represents the results of the original query after the insertion into {\tt LINEITEM}.
\end{example}

Because the delta query is simpler, it can be evaluated more efficiently than the original, conceivably saving time on updates.  IVM functionality has been implemented into most commercial database systems, including the two we test.

\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} \\
(a) & (b) & (c)
\end{tabular}
\end{center}
\label{fig:dbfail:postgres}
\caption{Performance results for Postgres on Examples \ref{ex:dbfail:stock} (a), \ref{ex:dbfail:tpch} (b), and \ref{ex:dbfail:network} (c).}
\end{figure*}
\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} \\
(a) & (b) & (c)
\end{tabular}
\end{center}
\label{fig:dbfail:CD1}
\caption{Performance results for Commercial DBMS 1 on Examples \ref{ex:dbfail:stock} (a), \ref{ex:dbfail:tpch} (b), and \ref{ex:dbfail:network} (c).}
\end{figure*}\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_db_result} \\
(a) & (b) & (c)
\end{tabular}
\end{center}
\label{fig:dbfail:CD2}
\caption{Performance results for Commercial DBMS 2 on Examples \ref{ex:dbfail:stock} (a), \ref{ex:dbfail:tpch} (b), and \ref{ex:dbfail:network} (c).}
\end{figure*}

Figures \ref{fig:dbfail:postgres}, \ref{fig:dbfail:CD1}, and \ref{fig:dbfail:CD2} show how the Postgres and two Commercial DBMSes (respectively) perform in each of the three example scenarios.  We consider three techniques for keeping a live view of the query result:
\begin{itemize}
\item {\bf Variable Refresh:} Periodically re-evaluate the query.  Note that in this implementation this results in the query not being monitored continually; The implementation can not detect trigger conditions which arise and disappear in between re-evaluations.
\item {\bf Full Refresh:} Use triggers to initiate a full refresh of the query results after every update to a base relation.  
\item {\bf Incremental Maintenance:} Use IVM to update the query results after every update to the base relation.  In the case of Postgres where IVM is not natively supported, we compute the delta query by hand and implement it via triggers.
\end{itemize}

\todo{Hardware overview goes here}.  Each graph shows the rate of updates supported by our configuration with respect to the frequency with which the query results are refreshed.  The Full Refresh and Incremental Maintenance implementations refresh the query results after every update, and are unaffected by the desired refresh rate.

\todo{Rewrite this paragraph after we have results}
Even with IVM techniques (which must be implemented by hand in some implementations), these systems can handle a maximum of \todo{fill in} updates per second to the base relations.  If users are willing resort to sampling as infrequently as once per one or ten seconds, it is possible to improve that to as much as \todo{fill in} or \todo{fill in} updates per second respectively.  Even with sampling, this is not sufficient to achieve the sorts of update rates required by our example applications.

\subsection{Monitoring with Stream Processors}
Database Management Systems -- even Active Databases -- are not designed for monitoring of rapidly changing data.  A more closely related area is the field of Stream Processing\cite{?}.  In a Stream Processing system, users define pipelines similar to traditional query plans, except that each edge in the plan is a continuous stream of data.  Selection and projection operators are evaluated immediately.  Window operators\cite{?} {\em temporarily} cache tuples appearing on a stream, and allow joins over the most recent set of tuples.

Recently, most commercial stream processing systems have begun to support Complex Event Processing\cite{?}.  Essentially, the stream processor is extended with support for traditional relational tables, making it possible to implement joins between relational tables and streams.  

\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=2in]{../graphics-tmp/placeholder_stream_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_stream_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_stream_result} \\
(a) & (b) & (c)
\end{tabular}
\end{center}
\label{fig:dbfail:CSP1}
\caption{Performance results for Commercial Stream Processor 1 on Examples \ref{ex:dbfail:stock} (a), \ref{ex:dbfail:tpch} (b), and \ref{ex:dbfail:network} (c).}
\end{figure*}\begin{figure*}
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=2in]{../graphics-tmp/placeholder_stream_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_stream_result} &
\includegraphics[width=2in]{../graphics-tmp/placeholder_stream_result} \\
(a) & (b) & (c)
\end{tabular}
\end{center}
\label{fig:dbfail:CSP2}
\caption{Performance results for Commercial Stream Processor 2 on Examples \ref{ex:dbfail:stock} (a), \ref{ex:dbfail:tpch} (b), and \ref{ex:dbfail:network} (c).}
\end{figure*}

Figures \ref{fig:dbfail:CSP1}, \ref{fig:dbfail:CSP2}, and \ref{fig:dbfail:CD2} show how two Commercial Stream Processing Systems perform in each of the three example scenarios.  We have also considered a third system, but were unable to implement these scenarios using it.

We consider two implementations of each example in each stream processor: one based on a straight translation of the query into the stream processor's language (Full Refresh), and one based on a translation of the delta queries that would have been generated by IVM (Incremental Maintenance).

\todo{Rewrite this paragraph after we have results}
Even with IVM techniques (which must be implemented by hand in some implementations), these systems can handle a maximum of \todo{fill in} updates per second to the base relations.  This is, in part, the result of a need to lock the base relations before applying updates.  The CEP implementation in the stream processing systems we have analyzed is geared towards implementing tables as purely input or output sources -- persistent state used by the query is assumed to be relatively stable.  

\begin{figure}
\begin{center}
\begin{tabular}{|l|c|c|c|}
\hline
{\bf Engine}   & {\bf 2.1} & {\bf 2.2} & {\bf 2.3} \\\hline
{\bf Postgres} & 8 / 30    & 30 / 120  & 15 / 100 \\\hline
{\bf CDBMS 1}  & 8 / 9     & 30 / 31   & 15 / 16  \\\hline
{\bf CDBMS 2}  & 8 / 9     & 30 / 31   & 15 / 16  \\\hline
{\bf CSP 1}    & 40 / 120  & 60 / 200  & 50 / 200 \\\hline
{\bf CSP 2}    & 40 / 120  & 60 / 200  & 50 / 200 \\\hline
\end{tabular}

\todo{Update these numbers to be correct}
\end{center}
\label{fig:dbfail:locBakeoff}
\caption{Lines of code required to implement each of the example scenarios (including schema definitions) in Postgres, the two commercial database systems (CDBMS 1,2), and the two commercial stream processors (CSP 1,2).  For each engine/scenario pair, both the number of lines to implement the query and it's IVM equivalent are shown (respectively).}
\end{figure}

Another metric that must be considered is implementation complexity.  Figure \ref{fig:dbfail:locBakeoff} shows the number of lines of code required to implement each query in each of these engines.  The use of persistent state in the commercial stream processor forces the user to explicitly manage several messy aspects of the computation (e.g., locking the tables before updating/reading them).  Furthermore, when implementing IVM in systems that do not support it (Postgres and the two stream processors), the delta queries must be computed by hand and explicitly defined, causing a further blowup in number of lines.

\subsection{Implementation Challenges}
\todo{Yanif's horror stories}

\subsection{Other Related Work}