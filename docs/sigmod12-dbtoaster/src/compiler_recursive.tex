\section{The Compiler}
\label{sec:compiler}


In this section we present our compiler. We start by sketching its architecture.
%
The compiler accepts an SQL query and schema definition (essentially create table statements for the relations the query accesses) as input. The parser turns the input query into a first intermediate representation (IR) that is an adapted form of relational algebra. The first compilation stage generates trigger programs for performing multilevel incremental view maintenance (IVM). The statements of these trigger programs increment the values of view data structures (called maps) by expressions of this IR. The IR and basics of the transformation are described in Section~\ref{sec:compiler_calc}. Section~\ref{sec:simplification} presents optimizations for simplifying IRs in the first compilation stage.

In Section~\ref{sec:advanced-rewriting} we discuss the creation of triggers for multi-level IVM by extracting and materializing strict subexpressions rather than the full query. This is necessary to be able to deal with nested aggregation which otherwise, if we always materialize the largest query we can, leads to recursion in compilation that does not terminate. In general, the choice of subquery to materialize and incrementally maintain is a degree of freedom in query optimization. In this section we give heuristic rules for making this choice.

The next stage of the compiler, described in Section~\ref{sec:kthree}, turns the statements of the trigger program created in the first stage into an IR for purely functional programming, essentially lambda calculus without general recursion but with special higher-order functions for transforming collections (primitives such as map and reduce). This stage allows us to perform deforestation and fusion optimizations too low-level to have expression in the first, relational algebra-like IR.
Section~\ref{sec:kthree} also describes our infrastructure for code generation and our runtime system.




\subsection{Queries with binding patterns, deltas, and recursive IVM}
\label{sec:compiler_calc}


\def\Sum{\mbox{Sum}}


Next we develop an IR for SQL queries that is suitable for compilation of database queries and to perform the transformations necessary to enable efficient multilevel IVM. This language is a refinement of the query language defined in \cite{koch-pods:10}, but here we aim at better readability and avoid unnecessary formality. The language is based on positive relational algebra with aggregation but makes the following modifications:
%
\begin{itemize}
\addtolength{\topsep}{-0.3ex}
\addtolength{\labelsep}{-0.3ex}
\addtolength{\itemsep}{-1ex}
\item
The data model is that of relations where tuples have {\em integer} multiplicities. This generalizes SQL bag semantics to allow for
negative multiplicities. This way, databases and updates can be treated uniformly. A relation in which all multiplicities are
nonnegative can be either thought of as an insertion or as a database (=an insertion into the empty database) and a relation in which all tuple multiplicities are negative is a deletion.

\item
We replace relational (bag) union by an operation $+$ that adds two relations $R$ and $S$ by assigning to each tuple in the result the integer sum of the multiplicities of that tuple in $R$ and $S$.
%
%The operation $+$ is associative, and this is key to the overall simplicity of the
%approach and allows us to treat insertions and deletions uniformly.

\item
The join operation is the natural join, where the multiplicity of a tuple in the result
is the integer product of the multiplicities of the two tuples from which is formed.

\item
Conditions are queries in their own right; there is no explicit selection operation. Thus, we write a relational algebra selection
$\sigma_{A<B}(R)$ as $R \bowtie (A<B)$.

\item
Sum-aggregates are of the form $\Sum_{\vec{A}} Q$ where $Q$ is a query and $\vec{A}$ is a tuple of group-by columns (which we will also
call variables). The result are the tuples of the projection of $Q$ on $\vec{A}$ and each tuple's multiplicity is the sum of the
multiplicities of the tuples that were projected down to it.
Sum-aggregates can be viewed as multiplicity-respecting projection.

\item
Terms have a scalar rather than relation value. A sum-aggregate without grouping column evaluates to a term.
%
Terms can be used as queries; in that case, they evaluate to the nullary tuple with multiplicity the value of the term.
Thus an SQL query ``select sum(A) from R'' can be written in our IR as $\Sum(R \bowtie A)$; thus we first multiply the value of $A$ into the multiplicity of each tuple and than sum up all these multiplicities.

\item
There are no explicit operations for projection and relational difference. A multiplicity-aware projection is implemented by
sum-aggregation and universal queries can be expressed by counting aggregation (a popular homework exercise in database courses). All SQL aggregate functions can be expressed using sums (counts), ratios of sums (avg),
and nested aggregates (min and max)\footnote{Here, the materialization of the nested subquery
nicely corresponds to the materialization any IVM technique has to carry out to find second-best values when a min or max value is deleted.}.
\end{itemize}

Let us recap: The language departs from relational bag algebra in essentially two ways; by having integer tuple multiplicities
to deal uniformly with insertions and deletions, and by {\em taking aggregate values out of the tuple} and putting them down into
the multiplicity. This has a very desirable consequence: Incremental computation is all about modifying multiplicities.
As we will see later, by keeping aggregate values in the multiplicities, delta processing for aggregates is vastly simplified.
Aggregate queries dominate analytical workloads and can greatly profit from incremental view maintenance. Thus it makes sense to optimize our IR for aggregate processing.




In summary, the abstract syntax of the IR is two-sorted (consisting of queries $q$ and terms $t$):
%
\begin{eqnarray*}
q &\mbox{:--}& R \mid t \theta t \mid t \mid q + q \mid q \bowtie q \mid \Sum_{\vec{A}}(q) \\
t &\mbox{:--}& c \mid A                 \mid t + t \mid t * t       \mid \Sum(q)
\end{eqnarray*}
Thus queries can be formed from relation names, atomic conditions, addition, natural join, and sum-aggregation.
Terms are formed from constants, variables/columns, addition, multiplication, and sum-aggregation without group-by.
Terms are queries returning the nullary relation in which
tuple $\tuple{}$ has as multiplicity the value of the term. We use $Q_1 - Q_2$ as syntactic sugar for $Q_1 + (-1)\bowtie Q_2$.
Apart from this, the semantics was given above.


We will often use SQL syntax for conciseness, which will correspond to our IR in the natural way analogous to how we translate
between classical bag relational algebra and SQL.

Expressions of the IR have binding patterns: There are input variables or parameters
without which we cannot evaluate these expressions, and there are output variables, the columns of the schema of the query result.
%
Each expression $Q$ has input variables or parameters $\vec{x_{in}}$ and a set of output variables $\vec{x_{out}}$, which form the schema of the query result. We denote such an expression as $Q[\vec{x_{in}}][\vec{x_{out}}]$. The input variables are those that are not {\em range-restricted} in a calculus formulation, or equivalently have to be understood as {\em parameters} in a SQL query because their values cannot be computed from the database: They have to be provided so that the query can be evaluated.

Binding patterns represent information flow. In general, this flow is not exclusively bottom-up.
Some of an expression's variables are input variables or parameters which cannot be computed from the query but have to be given to the query so that it can be evaluated. The most interesting case of this is a correlated nested aggregate, viewed in isolation (which must be possible for small-step compositionality). In such an aggregate, the correlation variable from the outside is such an input variable. The aggregate query can only be computed if a value for the input variable is given.


We illustrate this by a few examples in Table~\ref{tab:ir-examples}.
All of the expressions there are valid queries of our IR, typed by indicating the input and output variables.

\begin{table}
\begin{center}
\begin{scriptsize}
\begin{tabular}{|l|l|l|}
\hline
SQL & IR & Bdg.pat. \\
\hline
select * from R   & $R$ & $[][A,B]$ \\
--- & $C < D$ & $[C,D][]$ \\
--- & $C = D$ & $[C][D]$ \\
&& or $[D][C]$
\\[1ex]
select A from R   & $\Sum_A(R \bowtie (B < C))$ & $[C][A]$ \\
where B < C       &&
\\[1ex]
select A, sum(1) from R  & $\Sum_A(R \bowtie (B < C))$ & $[C][A]$ \\
where B < C group by A &&
\\[1ex]
B < (select sum(D) from S & $B < \Sum(S \bowtie D$ & $[A,B][]$ \\
~~where A > C) & ~~$\bowtie (A > C))$ &
\\[1ex]
select * from R where & $\Sum_{A,B}(R \; \bowtie$ & $[][A,B]$ \\
B < (select sum(D) from S & ~~~~$B < \Sum(S \bowtie D$ & \\
where A > C) & ~~~~~~$\bowtie\; (A > C)))$ & \\
\hline
\end{tabular}
\end{scriptsize}
\end{center}

\vspace{-5mm}

\caption{Example queries with their binding patterns (written as [input vars][output vars]). Relation $R$ and S have schema $A,B$ and $C,D$, respectively.}
\label{tab:ir-examples}
\vspace{-3mm}
\end{table}


\medskip



This language specification covers all of the core features of SQL with the exception of
null values and outer joins (which have the undesirably property of being nonassociative, which is a problem for incremental computation).



\subsubsection{Computing the delta of a query.}

This language has the nice property of being {\em closed under taking deltas}.
For each query expression $Q$, there
is an expression $\Delta Q$ of our IR that expresses how the result of $Q$ changes as the database $D$ is changed by update workload $\Delta D$. This works for updates containing an unbounded number of insertions and deletions, but we will subsequently only study single-tuple inserts and deletes.
This is because such updates allow for particularly efficient view refresh code that
can be run to respond online to each tuple change.


Thanks to the strong compositionality of the language, we only have to give delta rules for the 
individual operators. These rules a given and studied in detail in \cite{koch-pods:10}. In short,
\begin{eqnarray*}
\Delta(Q_1 + Q_2)    &:=& (\Delta Q_1) + (\Delta Q_2), \\ 
\Delta(\mbox{Sum } Q)   &:=& \mbox{Sum } (\Delta Q), \\
\Delta(Q_1 \bowtie Q_2) &:=& ((\Delta Q_1) \bowtie Q_2) + (Q_1 \bowtie (\Delta Q_2)) \\
&& +\; ((\Delta Q_1) \bowtie (\Delta Q_2)).
\end{eqnarray*}
$\Delta R$ is the update to $R$. In the case that the update does not change $R$ (but other relation(s)),
$\Delta R$ is empty.
 

The deltas of conditions are 0 if they do not contain nested queries. The delta for a condition with a nested query
is more complicated. For example, our rule for $\Delta (x > Q)$ is
$(x > (Q + \Delta Q)) \bowtie (x \le Q) - (x \le (Q + \Delta Q)) \bowtie (x > Q)$.
We refer to \cite{koch-pods:10} for the general case of conditions.

Let us be precise though about computing delta queries. In general, each expression has input and output variables,
and taking a delta in general {\em adds variables} parameterizing the query with the update. From now on we only consider single-tuple insertions $+R(\vec{x})$ to or deletions $-R(\vec{x})$ from a relation $R$.

\def\dxy{\Delta_{+R(x,y)}}

\begin{example}\em
\label{ex:RS}
Given schema $R(AB), S(CD)$, and query
\begin{verbatim}
select sum(A * D) from R, S where B < C
\end{verbatim}
or, in our IR,
$\Sum(R \bowtie S \bowtie B < C \bowtie A*D)[][]$.

The delta for this query and the insertion of a tuple $\tuple{x,y}$ into $R$ is as follows.
Let $e$ be $S \bowtie B < C \bowtie A*D$.
Then
\[
\dxy \Sum(R \bowtie e) = \Sum \dxy (R \bowtie e)
\]
and by the delta rule for $\bowtie$,
\begin{eqnarray*}
\dxy (R \bowtie e) &=& (\dxy R) \bowtie e \\
&+& R \bowtie \dxy e \\
&+& (\dxy R) \bowtie \dxy e
\end{eqnarray*}
%
and
$\dxy(e) = 0$
since
$\dxy S = 0$, $\dxy B \theta C = 0$, and $\dxy (A * D) = 0$. (This follows again from the delta rule for $\bowtie$, applied
twice.)
Now
$\dxy R$ is the singleton relation $\{\tuple{A:x,B:y}\}$: the actual insertion.
The expression $\{\tuple{A:x,B:y}\} \bowtie e$ can be simplified to
$\{\tuple{A:x}\} \bowtie S \bowtie y \theta C \bowtie x * D$.
%
Thus the delta of the input query is
$\Sum(\{\tuple{A:x}\} \bowtie  S \bowtie y < C \bowtie x*D)[x,y][]$ which can be further simplified to
$\Sum(S \bowtie y < C \bowtie x*D)[x,y][]$.
In SQL notation, this is
\begin{verbatim}
select sum(x * D) from S where y < C
\end{verbatim}
\end{example}




\subsubsection{Recursive incremental view maintenance.}

\def\deg{\mbox{deg}}

Before we come to multi-level IVM, let us recap the special case of recursive IVM of \cite{koch-pods:10, kennedy-ahmad-koch-cidr:11}, where we try to be as greedily incremental as possible.
If we restrict the query language to exclude aggregates nested into conditions (for which the delta query was complicated), the query language fragment has the following nice property \cite{koch-pods:10}.
%
$\Delta Q$ is structurally strictly simpler than $Q$ when query complexity is measured as follows. 
For union(+)-free queries, the {\em degree} $\deg(Q)$ of query $Q$ is the number of relations joined together. We can use distributivity to push unions above joins and so give a degree to queries with unions, as the maximum degree of the union-free subqueries. Queries are
strongly analogous to polynomials, and the degree of queries is defined precisely as it is defined for
polynomials.

\begin{theorem}[\cite{koch-pods:10}]
If $\deg(Q) > 0$, then \[\deg(\Delta Q) = \deg(Q) - 1.\]
\end{theorem}

Recursive IVM makes use of the simple fact that a delta query is a query too. Thus it can be incrementally maintained as well, making use of a delta query to the delta query, which again can be materialized and incrementally maintained, and so on, recursively.
By the above theorem, this recursive query transformation terminates in the $\deg(Q)$-th recursion level,
when the rewritten query becomes a ``constant'' (independent of the database, and only depending on the updates).

The goal of compilation is to create on-insert and on-delete triggers for every relation occurring in the query.
Conceptually, a query $Q[\vec{x}_{in}][\vec{x}_{out}]$ over relations $R_1, \dots, R_k$ is compiled as follows:
We write $M_Q$ for the materialized view of query $Q$. 
Then the trigger program for the event $\pm R_{i_{j+1}}(\vec{y}_{j+1})$ consists of the
statements

\begin{tabbing}
~~foreach $\vec{x}_{out}, \vec{y}_1, \dots, \vec{y}_j$ do \\
~~~~~
   $M_{\Delta_{\pm R_{i_j}} \dots \Delta_{\pm R_{i_1}} Q}
       [\vec{x}_{in}\vec{y}_1 \dots \vec{y}_j][\vec{x}_{out}] \;\pm=$ \\
~~~~~~~~
    $M_{\Delta_{\pm R_{i_{j+1}}} \dots \Delta_{\pm R_{i_1}} Q}
       [\vec{x}_{in}\vec{y}_1 \dots \vec{y}_{j+1}][\vec{x}_{out}]$.
\end{tabbing}
for each $j \in 0, 1, \dots, \deg(Q)$ and $\tuple{i_1 \dots i_j} \in 1, \dots, k^j$.
For correctness, unless we maintain
old and new versions of the maps (which would be costly), these statements have to be 
ordered by increasing $j$.

\begin{example}\em
\label{ex:RS2}
We return to the query $Q$ of Example~\ref{ex:RS}, and
$(\Delta_{\pm R(x,y)} Q)[x,y][]$ which was already computed there.
We compute:
\[ (\Delta_{\pm S(z,u)} Q)[z,u][] = \Sum(R \bowtie B < z \bowtie A*u) \]
and
\begin{eqnarray*}
(\Delta_{\pm S(z,u)} \Delta_{\pm R(x,y)} Q)[x,y,z,u][] &=& \\
(\Delta_{\pm R(x,y)} \Delta_{\pm S(z,u)} Q)[x,y,z,u][] &=&
\Sum(y < z \bowtie x*u) \\
&=& \mbox{$(y<z)$ ? $(x*u)$ : $0$}
\end{eqnarray*}
where $(y<z)$ ? $(x*u)$ : $0$ is the functional if-then-else of C
(if $y<z$ then $x*u$ else 0).
The on-insert into $R$ trigger $+R(x,y)$ following the construction described above is the program
\begin{tabbing}
$M_Q$[][] += $M_{\Delta_{+R(x,y)} Q}[x,y][]$; \\
foreach $z,u$ do $M_{\Delta_{+S(z,u)} Q}[z,u][]$ $+$= $(y<z)$ ? $(x*u)$ : $0$; \\
foreach $z,u$ do $M_{\Delta_{-S(z,u)} Q}[z,u][]$ $+$= $(y<z)$ ? $(x*u)$ : $0$
\end{tabbing}
The remaining triggers are constructed analogously.
The trigger contains an update rule for the (in this case, scalar) data structure $M_Q$ for the overall
query result, which uses the auxiliary view $M_{\Delta_{\pm R(x,y)} Q}$ which is maintained in the update triggers for $S$, plus update rules for the auxiliary views $M_{\Delta_{\pm S(z,u)} Q}$ that are used to update
$M_Q$ in the insertion and deletion triggers on updates to $S$.
\end{example}

We observe that the structure of the work that needs to be done is extremely regular and (conceptually) simple. Moreover,
there are no classical large-granularity operators left, so it does not make sense to give this workload to a classical
query optimizer. There are for-loops over many variables, which have the potential to be very expensive. But the work
is also perfectly data parallel, and there are no data dependencies comparable to those present in joins. All this provides
justification for adopting a compilers approach.


\subsubsection{Initial value computation}


The technique for constructing trigger programs by recursive delta rewriting discussed above assumes
materialized views to be represented as maps $M$ that store, for tuples $\vec{i}\vec{o}$ of input and output variables, a value (an aggregation result) $M[\vec{i}][\vec{o}]$.
So far we have disregarded the question what the {\em domains} of these maps are, that is,
for which $\vec{i}\vec{o}$ the maps are defined and store values.
If we could assume that the domains contain all the tuples that could ever be formed from values occurring
in the database, we could initialize the map values to zero, $M[\vec{i}][\vec{o}] := 0$,
before the first update is fed into the system.
%
However, it is not feasible to assume such large or even infinite domains.
Instead, we want to start with empty maps and extend the domains when the update triggers want to access
values -- for reading or writing -- that are not yet represented in the map.
This raises the question how to compute initialization values; in general, these will not be zero if 
the system has been running for a while and the maps contain data.

%\begin{example}\em
Take for instance the query of Example~\ref{ex:RS2}: Suppose we have only seen inserts into relation $R$,
but $S$ is still empty. Then the auxiliary maps $M_{\Delta_{\pm S(z,u)} Q}[z,u][]$ still have empty domains and
looping over each pair $\tuple{z,u}$ does nothing. When, later, a tuple $\tuple{z,u}$ is inserted into $S$,
$M_Q$ is incremented by $M_{\Delta_{+S(z,u)} Q}[z,u][]$, which has to be computed from scratch as
$\Sum(R \bowtie B < z \bowtie A*u)$ and will generally be nonzero.
%\end{example}

In the current implementation of our compiler, we use the fact that if query $Q$ does not use inequality joins,
a value to be initialized in map $M_Q$
will be zero. Otherwise, the value is computed from scratch by executing $Q$.
In the future, this could be improved upon by further compiling subqueries of $Q$ using
recursive IVM techniques.


\subsection{Query decomposition and factorization}
\label{sec:simplification}


The construction for trigger programs presented above yields 
materializations of high dimensionality, which will be very large and expensive to maintain.
This can be avoided by being slightly less aggressive with materialization.
We next discuss two query simplification techniques that are key to making
recursive IVM useful, join graph decomposition and factorization of query polynomials.


Join graph decomposition makes use of the so-called generalized distributive law [] (which plays
an important role in the context of probabilistic inference with graphical models) to decompose
sum-aggregates over disconnected join graphs. This is an important case, since computing deltas 
eliminates hyperedges of the join graph, disconnecting it more and more in each delta-rewriting, until all
hyperedges are gone. The basic law is simple. If a query is of the form $Q_1 \times Q_2$, i.e., $Q_1$ and
$Q_2$ do not share common columns, then
$\Sum_{\vec{A}\vec{B}}(Q_1 \times Q_2) = \Sum_{\vec{A}}(Q_2) \times \Sum{\vec{B}}(Q_2)$
where $\vec{A}$ are columns of $Q_1$ and $\vec{B}$ are columns of $Q_2.$
Consider for example the query $Q$
{\tt select sum(A*D) from R natural join S natural join T}
of schema $R(AB), S(BC), T(CDE)$. Then
$\Delta_{+S(b,c)} Q$ is
\vspace{-1mm}
\begin{verbatim}
select sum(A*D) from R,T where B=b and C=c
   = (select sum(A) from R where B=b) *
     (select sum(D) from T where C=c)
\end{verbatim}
\vspace{-1mm}
Given such a decomposition, it is better to materialize the two aggregate subexpressions and compute
the product when updating, rather than to materialize the product, that is,
\begin{eqnarray*}
M_Q[][] &+=& M_{\mathrm{select~sum(A)~from~R~where~B=b}}[b][] \\
&*& M_{\mathrm{select~sum(D)~from~T~where~C=C}}[c][]
\end{eqnarray*}
is better than
$M_Q[][] += M_{\Delta_{+S(b,c)} Q}[b,c][]$ because that way we materialize two (small) one-dimensional
maps rather than one two-dimensional map.

Binary sums  (unions) $+$ can be pushed through aggregate sums ($\Sum(Q_1 + Q_2) = \Sum(Q_1) + \Sum(Q_2)$).
In conjunction with join graph decomposition, this often leaves us with expressions that do a fair amount
of adding and multiplying of subqueries. Such expressions can be optimized using the distributive law.
Factorization is often particularly useful to share common subexpressions.
For example, the general delta rule for $Q_1 \bowtie Q_2$ is a sum of three subexpressions
$\Delta(Q_1 \bowtie Q_2) := ((\Delta Q_1) \bowtie Q_2) + (Q_1 \bowtie (\Delta Q_2))
 + ((\Delta Q_1) \bowtie (\Delta Q_2))$ which can be factorized as
$((Q_1 + \Delta Q_1) \bowtie (\Delta Q_2)) + ((\Delta Q_1) \bowtie  Q_2)$
or as
$((\Delta Q_1) \bowtie (Q_2 + \Delta Q_2)) + (Q_1 \bowtie \Delta Q_2)$.
Determining which one is better is a task for a cost-based optimizer.







