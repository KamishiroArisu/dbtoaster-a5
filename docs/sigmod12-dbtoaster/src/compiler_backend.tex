\subsection{Functional compilation and optimization}
\label{sec:kthree}



\def\map{\mbox{\texttt{map}}}
\def\flatten{\mbox{\texttt{flatten}}}
\def\agg{\mbox{\texttt{agg}}}
\def\groupagg{\mbox{\texttt{groupagg}}}
\def\apply{\mbox{\texttt{apply}}}


\comment{
Each step of delta transformation, and extraction and materialization
in our recursive compilation process results in a maintenance query with parameters
(i.e. a function) that updates an auxiliary datastructure. This leads to our
second stage IR, a small functional programming IR.
}
To narrow the gap while translating the SQL IR directly to low-level code, our
second stage IR is a small functional programming language.
\comment{
Functional programs have strong correspondences to relational queries, both are
dataflow programs, and the relationship of their formal expressiveness has been
studied in complex object languages~\cite{buneman-kleisli:95}, and
comprehensions~\cite{jones-haskell:07}.
}
We perform holistic query optimization in a functional IR, exploiting a
breadth of powerful program transformations including monad transformations from
structural recursion~\cite{buneman-kleisli:95},
deforestation~\cite{marlow-fp:92}, supercompilation and fusion.
Other recent works have observed the need for holistic optimization of query
executors with low-level languages~\cite{krikellas-icde:10,neumann-pvldb:11}.
Our functional IR is a work-in-progress that we believe will bring benefits such
as simpler optimizer development, and non-first normal forms for specialized
in-memory or disk layout strategies in future work.


Our functional IR includes tuples and collection types, lambdas and associative
lambdas\footnote{Determining properties such as associativity, commutativity and
idempotence via program analyses is undecidable~\cite{buneman-kleisli:95}},
conditionals, four higher-order collection transformers that we describe below,
and in-place update operations on collections.
\comment{
Collection mutation operations perform in-place updates of entries, implying
that our functional IR is an impure language, although we could use monads as
with Haskell's I/O system.
}
Also, our functions are not recursive, and only our collection transformers use
higher-order functions, that is we never pass a function as an argument to
another function. Our functional IR is already low-level in the spirit of
functional compiler internal IRs (i.e. after defunctionalization).



For our higher-order collection transformers, we extend the \texttt{map} and
\texttt{flatten} constructs from Kleisli~\cite{buneman-kleisli:95} with
aggregations: \texttt{agg} and \texttt{groupagg}. Their type signatures are:


\vspace{1mm}\hspace{-4mm}
\begin{tabular}{p{1cm}l}
\texttt{map}
    & $: (\sigma \rightarrow \tau)
           \rightarrow \sigma \ \mathcal{C}
           \rightarrow \tau \ \mathcal{C}$\\
\texttt{flatten}       
    & $: (\sigma\ \mathcal{C}) \ \mathcal{C}
           \rightarrow \sigma \ \mathcal{C}$ \\
\texttt{agg}
    & $: (\sigma \rightarrow \tau \rightarrow \tau)
           \rightarrow \tau
           \rightarrow \sigma \ \mathcal{C}
           \rightarrow \tau$ \\
\texttt{groupagg}
    & $: (\sigma \rightarrow \tau \rightarrow \tau)
           \rightarrow (\sigma \rightarrow \upsilon)
           \rightarrow \tau
           \rightarrow \sigma \ \mathcal{C}
           \rightarrow \tuple{\upsilon, \tau} \ \mathcal{C}$ \\
\end{tabular}

\comment{
Type signatures for our collection mutators are:

\vspace{1mm}
\begin{tabular}{p{0.8cm}l}
\texttt{merge}
    & $: \tuple{\sigma, \tau} \ \mathcal{C}
           \rightarrow \tuple{\sigma, \tau} \ \mathcal{C}
           \rightarrow \tuple{\sigma, \tau} \ \mathcal{C}$ \\
\texttt{update} 
    & $: \tuple{\sigma, \tau} \ \mathcal{C}
           \rightarrow \tuple{\sigma, \tau}
           \rightarrow \tuple{\sigma, \tau} \ \mathcal{C}$
\end{tabular}

\todo{Describe these operations}
}

\vspace{1mm}
\noindent Above $\sigma \ \mathcal{C}$ indicates the type of a collection
containing elements of type $\sigma$, and $\tuple{}$ is a tuple constructor. The
\texttt{map} transformer applies a function to every element of a collection,
while \texttt{flatten} reduces a two-level nested collection to a flat
collection. The \texttt{agg} and \texttt{groupagg} transformers applies the
accumulator function given as its first argument over a collection. Both are
given an initial value, and \texttt{groupagg} is additionally given a grouping
or partitioning function as its second argument. Our framework is agnostic to
the form of collection, which may be sets, bags, and lists, and
can vary in their underlying implementation (e.g. tree or hash sets, linked lists or
vectors). Intelligent datastructure selection is future work, we currently use
vectors and hashtables.

Our functional IR supports several optimizations that are not captured by the
SQL IR, including:
\comment{
Currently, our optimizer performs static compile-time optimizations that
universally simplify functional programs in its usage of collections and
scalars. We plan to expand this to perform both cost-based static and runtime
adaptive optimizations in future work.
}

\vspace{1mm}
\noindent\textbf{If-lifting.} This is a transformation to a canonical form
for our functional IR that involves lifting every conditional to its minimal
binding point. Consider an expression:

$\apply(\lambda x. \apply(\lambda y.
  \mbox{\texttt{ if }} x < 0 \mbox{\texttt{ then }} y/a
                             \mbox{\texttt{ else }} y*3, a), b)$

\vspace{1mm}
\noindent The conditional above is independent of $y$ and we can lift to:

\vspace{-2mm}
\begin{tabbing}
$\apply(\lambda x.
  \mbox{\texttt{ if }} x < 0$ \= $\mbox{\texttt{ then }}
    \apply(\lambda y. y/a, a)$\\
\> $\mbox{\texttt{ else }} \apply(\lambda y. y*3, a),\ b)$
\end{tabbing}


\vspace{-2mm}  
\comment{
A minimal binding point is the lambda expression that binds the minimal
sufficient set of variables to ensure a well-defined evaluation of the
conditional.
}
\noindent This canonical form admits maximal optimization of the conditional's
then-else blocks by pushing in expressions from outside the conditional
at the expense of code size. However, queries are often small
in code size, and if-lifting enables joint optimization of IVC expressions and
delta expressions.


\vspace{1mm}
\noindent\textbf{Deforestation and fusion.} This transformation converts
functional programs to treeless forms~\cite{marlow-fp:92}, that is, it eliminates
redundant intermediate datastructures. The core subset of our deforestation
rewrite rules are:

\def\xform{\mbox{\texttt{:-}}}
\def\fst{\mbox{\texttt{fst}}}

\vspace{1mm}\hspace{-6mm}
\begin{tabular}{p{3.6cm}l}
$\map(f, \map(f', c))$ 
    & $\xform\ \map(f \circ f', c)$
\\
$\map(f, \flatten(c))$
    & $\xform\ \flatten(\map(\lambda c'.\ \map(f,c'), c))$
\\
$\agg(f^a, i, \map(f', c))$
    & $\xform\ \agg(f^a \circ f', i, c)$
\\
$\agg(f^a, i, \flatten(c))$
    & $\xform\ \agg(f, i, \map(\lambda c'.\ \agg(f, i, c'),c))$
\\
$\groupagg(f^a, g, i, \map(f', c))$
    & $\xform\ \groupagg(f^a \circ f', g \circ f', i, c)$
\\
$\groupagg(f^a, i, \flatten(c))$ & $\xform$
\\
\multicolumn{2}{l}{
$\quad\groupagg(f, \fst, i, \flatten(
    \map(\lambda c'.\ \groupagg(f, g, i, c'),c)))$}
\end{tabular}


\vspace{1mm}
Above $\circ$ is function composition and \texttt{fst} the left projection of a
pair.
\comment{
Note that both the \texttt{agg} and \texttt{groupagg} compose
associative lambdas (above $f^a$, two-argument lambdas), with regular
(single-argument) lambdas in terms of the first argument.
}
Additional deforestation rules for iterative usage of in-place updates are
straightforward to derive.

The first rule for \texttt{map} eliminates the intermediate collection
constructed after applying $f'$, instead pipelining the results through the
application of both functions $f$ and $f'$. Since both $f$ and $f'$ can
themselves contain \texttt{map} invocations, essentially performing joins, this
rule can alter the arity of the join operation, constructing fully pipelined
n-way joins. The second rule above is a form of late tuple
construction~\cite{abadi-icde:07}, where by lifting the \texttt{flatten}
expression, we carry around nested intermediate collection during processing.
Repeating this facilitates deeply nested representations, in the same vein as
non-first normal forms~\cite{schek-infsys:86}. Aggregate deforestation also uses
composition to realize pipelining and simplification, and the variants with
\texttt{flatten} are partial aggregations that push an associative aggregation
inside a nested collection.

\comment{
\vspace{1mm}
\noindent\textbf{Other optimizations.} The other major simplifications that we
perform on our functional IR include common subexpression elimination and
initial value computation deduplication. Duplicate, redundant IVC arises in the
SQL IR since the same view can appear multiple times when taking deltas of sums
and products expressions. We omit full details of these due to space
restrictions.
}

\comment{
\vspace{1mm}
\noindent\textbf{Common subexpression elimination.}
Our CSE algorithm simultaneously extracts candidate common subexpressions and
substitutes their usage, prioritizing large expressions as candidates. Our
approach is conservative, candidates extracted from disjunctions and
conditionals must be candidates in all subexpressions, to avoid unnecessary
evaluation in light of short-circuiting on disjunctions and conditions. Our
substitution process considers expression equivalence under alpha renaming
rather than pure lexical comparison, as with global value numbering strategies.
\comment{
Currently our CSE algorithm operates separately on side-effecting queries, and
we plan to analyze all maintenance work performed to realize multi-query
optimization.
}

\vspace{1mm}
\noindent\textbf{IVC deduplication.} In the SQL IR, the same view can appear
multiple times when taking deltas of sums and products expressions. Subsequently
its initial value computation (IVC) will also appear many times and when
accounting for binding patterns, these initial value computations are idempotent
and redundant. We can eliminate duplicate IVCs by tracking of binding pattern
equivalences and their dependences on variable domains.
}

\comment{
\vspace{1mm}
\noindent\textbf{Datastructure creation.} The final optimization pass creates
auxiliary index datastructures to efficiently evaluate predicates. This stage
occurs last since the prior optimizations, with the exception of if-lifting,
yield a simpler and smaller functional IR. We presently implement hash
datastructures, by extracting any remaining equality predicates from the IR,
and synthesize an expression to construct a hashtable on-the-fly and perform
probes.
}


\noindent\textbf{Code generation} Following functional optimization, we
synthesize an imperative IR for easy code generation as the final step of
compilation.
\comment{
The synthesis process is extensible in that a code generator
developer can define additional materialization points for temporaries given
their target language since different backend languages have varying builtin
primitive operations and datastructures.
}
The imperative IR supports external functions and types to enable the usage
of library code. We implemented a C++ code generator based around the STL and
its iterator-oriented collection APIs (e.g. \texttt{begin}, \texttt{end},
\texttt{find}, \texttt{erase}, \texttt{clear}, etc.). We omit further details
due to space constraints. Our imperative optimizer and its datastructure and
storage layer concerns is a work-in-progress.

